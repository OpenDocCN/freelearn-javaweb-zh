- en: Message-Driven Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already discussed many features around microservice-based architecture
    provided by Spring Cloud. However, we have always been considering synchronous,
    RESTful-based inter-service communication. As you probably remember from [Chapter
    1](33ddbb93-e658-4d91-97f5-06d6167ef89e.xhtml), *Introduction to Microservices*,
    there are some other popular communication styles, such as publish/subscribe or
    asynchronous, event-driven point-to-point messaging. In this chapter, I would
    like to introduce a different approach to microservices than that presented in
    previous chapters. We will talk in more detail about how you can work with Spring
    Cloud Stream in order to build message-driven microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics we will cover in this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: The main terms and concepts related to Spring Cloud Stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using RabbitMQ and Apache Kafka message brokers as binders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spring Cloud Stream programming model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced configurations of binding, producers, and consumers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of scaling, grouping, and partitioning mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple binder support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about Spring Cloud Stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Cloud Stream is built on top of Spring Boot. It allows us to create standalone,
    production-grade Spring applications and uses Spring Integration that helps in
    implementing communication with message brokers. Every application created with
    Spring Cloud Stream integrates with other microservices through input and output
    channels. Those channels are connected to external message brokers via middleware-specific
    binder implementations. There are two built-in binder implementations available—Kafka
    and Rabbit MQ.
  prefs: []
  type: TYPE_NORMAL
- en: Spring Integration extends the Spring programming model to support the well-known
    **Enterprise Integration Patterns** (**EIP**). EIP defines a number of components
    that are typically used for orchestration in distributed systems. You have probably
    heard about patterns such as message channels, routers, aggregators, or endpoints.
    A primary goal of the Spring Integration framework is to provide a simple model
    for building Spring applications based on EIP. If you are interested in more details
    about EIP, please refer to the website at [http://www.enterpriseintegrationpatterns.com/patterns/messaging/toc.html](http://www.enterpriseintegrationpatterns.com/patterns/messaging/toc.html).
  prefs: []
  type: TYPE_NORMAL
- en: Building a messaging system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I think that the most suitable way to introduce main Spring Cloud Stream features
    is through the sample microservices-based system. We will lightly modify an architecture
    of the system that has been discussed in the previous chapters. Let me provide
    a short recall of that architecture. Our system is responsible for processing
    orders. It consists of four independent microservices. The `order-service` microservice
    first communicates with `product-service` in order to collect the details of the
    selected products, and then with `customer-service` to retrieve information about
    the customer and his accounts. Now, the orders sent to `order-service` will be
    processed asynchronously. There is still an exposed RESTful HTTP API endpoint
    for submitting new orders by the clients, but they are not processed by the application.
    It only saves new orders, sends it to a message broker, and then responds to the
    client that the order has been approved for processing. The main goal of the currently
    discussed example is to show a point-to-point communication, so the messages would
    be received by only one application, `account-service`. Here''s a diagram that
    illustrates the sample system architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d470d32-4e38-49ef-a0b8-56cc3408ded5.png)'
  prefs: []
  type: TYPE_IMG
- en: After receiving a new message, `account-service` calls the method exposed by
    `product-service` in order to find out its price. It withdraws money from the
    account and then sends back the response to `order-service` with the current order
    status. That message is also sent through the message broker. The `order-service`
    microservice receives the message and updates the order status. If the external
    client would like to check the current status order, it may call the endpoint
    exposing the `find` method with the order details. The sample application's source
    code is available on GitHub ([https://github.com/piomin/sample-spring-cloud-messaging.git](https://github.com/piomin/sample-spring-cloud-messaging.git)).
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Spring Cloud Stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recommended way to include Spring Cloud Stream in the project is with a
    dependency management system. Spring Cloud Stream has an independent release trains
    management in relation to the whole Spring Cloud framework. However, if we have
    declared `spring-cloud-dependencies` in the `Edgware.RELEASE` version in the `dependencyManagement`
    section, we wouldn''t have to declare anything else in `pom.xml`. If you prefer
    to use only the Spring Cloud Stream project, you should define the following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to add `spring-cloud-stream` to the project dependencies.
    I also recommend you include at least the `spring-cloud-sleuth` library to provide
    sending messaging with the same `traceId` as the source request incoming to `order-service`
    via the Zuul gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To enable connectivity to a message broker for your application, annotate the
    main class with `@EnableBinding`. The `@EnableBinding` annotation takes one or
    more interfaces as parameters. You may choose between three interfaces provided
    by Spring Cloud Stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Sink`: This is used for marking a service that receives messages from the
    inbound channel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Source`: This is used for sending messages to the outbound channel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Processor`: This can be used in case you need both an inbound channel and
    an outbound channel, as it extends the `Source` and `Sink` interfaces. Because
    `order-service` sends messages, as well as receives them, its main class has been
    annotated with `@EnableBinding(Processor.class)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s the main class of `order-service` that enables Spring Cloud Stream
    binding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Declaring and binding channels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thanks to the use of Spring Integration, the application is independent from
    a message broker implementation included in the project. Spring Cloud Stream automatically
    detects and uses a binder found on the classpath. It means we may choose different
    types of middleware, and use it with the same code. All the middleware-specific
    settings can be overridden through external configuration properties in the form
    supported by Spring Boot, such as application arguments, environment variables,
    or just the `application.yml` file. As I have mentioned before, Spring Cloud Stream
    provides binder implementations for Kafka and Rabbit MQ. To include support for
    Kafka, you add the following dependency to the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Personally, I prefer RabbitMQ, but in this chapter, we will create a sample
    for both RabbitMQ and Kafka. Since we have already discussed RabbitMQ''s features,
    I''ll begin with the samples based on RabbitMQ:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After enabling Spring Cloud Stream and including the binder implementation,
    we may create senders and listeners. Let''s begin with the producer responsible
    for sending new order messages to the broker. This is implemented by `OrderSender`
    in `order-service`, which uses the `Output` bean for sending messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'That bean is called by the controller, which exposes the HTTP method that allows
    submitting new orders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The message with information about the order has been sent to message broker.
    Now, it should be received by `account-service`. To make this happen, we have
    to declare the receiver, which is listening for messages incoming to the queue
    created on the message broker. To receive the message with the order data, we
    just have to annotate the method that takes the `Order` object as a parameter
    with `@StreamListener`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now you may launch the sample applications. But, there is one important detail
    that has not yet been mentioned. Both those applications try to connect with RabbitMQ
    running on localhost, and both of them treat the same exchanges as an input or
    output. It is a problem, since `order-service` sends the message to the output
    exchange, while `account-service` listens for messages incoming to its input exchange.
    These are different exchanges, but first things first. Let's begin with running
    a message broker.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing connectivity with the RabbitMQ broker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already started the RabbitMQ broker using its Docker image in the previous
    chapters, so it is worth reminding ourselves of that command. It starts a standalone
    Docker container with RabbitMQ, available under port `5672`, and its UI web console,
    available under port `15672`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The default RabbitMQ address should be overridden with the `spring.rabbit.*`
    properties inside the `application.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, Spring Cloud Stream creates a topic exchange for communication.
    This type of exchange better suits the publish/subscribe interaction model. We
    may override it with the `exchangeType` property, as in the fragment of `application.yml`,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The same configuration settings should be provided for both `order-service`
    and `account-service`. You don''t have to create any exchange manually. If it
    does not exist, it is automatically created by the application during startup.
    Otherwise, the application just binds to that exchange. By default, it creates
    exchanges with names input for the `@Input` channel, and output for the `@Output`
    channel. These names may be overridden with the `spring.cloud.stream.bindings.output.destination`
    and `spring.cloud.stream.bindings.input.destination` properties, where input and
    output are the names of the channels. This configuration option is not just a
    nice addition to the Spring Cloud Stream features, but the key setting used for
    correlating the input and output destinations in inter-service communication.
    The explanation for why that happens is very simple. In our example, `order-service`
    is the message source application, so it sends messages to the output channel.
    Then, on the other hand, `account-service` listens for incoming messages on the
    input channel. If the `order-service` output channel and `account-service` input
    channel do not refer to the same destination on the broker, the communication
    between them would fail. In conclusion, I decided to use a destination with the
    names `orders-out` and `orders-in`, and I have provided the following configuration
    for `order-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The similar configuration settings for `account-service` are reversed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After both applications start up, you may easily check out the list of exchanges
    declared on the RabbitMQ broker using its web management console, available at `http://192.168.99.100:15672`
    (`quest`/`guest`). The following the implicitly created exchanges, and you may
    see our two destinations created for the test purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/972692ab-4ae9-4f45-ab16-7987f69b003a.png)'
  prefs: []
  type: TYPE_IMG
- en: By default, Spring Cloud Stream provides one input and one output message channel.
    We may imagine a situation where our system would need more than one destination
    for each type of message channel. Let's move back to the sample system architecture
    for a moment, and consider the situation where every order is asynchronously processed
    by two other microservices. Until now, only `account-service` has been listening
    for incoming events from `order-service`. In the current sample, `product-service`
    would be the receiver of incoming orders. Its main goal in that scenario is to
    manage the number of available products and decrease them on the basis of order
    details. It requires us to define two input and output message channels inside
    `order-service`, because we still have point-to-point communication based on a
    direct RabbitMQ exchange, where each message may be processed by exactly one consumer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In that case, we should declare two interfaces with `@Input` and `@Output`
    methods. Every method has to return a `channel` object. Spring Cloud Stream provides
    two bindable message components—`MessageChannel` for an outbound communication,
    and its extension, `SubscribableChannel`, for an inbound communication. Here''s
    the interface definition for interaction with `product-service`. The analogous
    interface has been created for messaging with `account-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to activate the declared components for the application by
    annotating its main class with `@EnableBinding(value={AccountOrder.class, ProductOrder.class}`.
    Now, you may refer to these channels in the configuration properties using their
    names, for example, `spring.cloud.stream.bindings.productOrdersOut.destination=product-orders-in`.
    Each channel name may be customized by specifying a channel name when using the
    `@Input` and `@Output` annotations, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the custom interfaces declaration, Spring Cloud Stream will generate
    a bean that implements that interface. However, it still has to be accessed in
    the bean responsible for sending the message. In comparison with the previous
    sample, it would be more comfortable to inject bound channels directly. Here''s
    the current product order sender''s bean implementation. There is also a similar
    implementation of the bean, which sends messages to `account-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Every message-channel custom interface should also be provided for the target
    service. The listener should be bound to the right message channel and the destination
    on the message broker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Integration with other Spring Cloud projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You have probably noticed that the sample system mixes different styles of
    inter-service communication. There are some microservices that use typical RESTful
    HTTP API, and some others that use the message broker. There are also no objections
    to mixing different styles of communication inside a single application. You may,
    for example, include `spring-cloud-starter-feign` to the project with Spring Cloud
    Stream, and enable it with the `@EnableFeignClients` annotation. In our sample
    system, those two different styles of communication combine `account-service`,
    which integrates with `order-service` via the message broker, and with `product-service`
    through the REST API. Here''s the Feign client''s `product-service` implementation
    inside the `account-service` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: There is other good news. Thanks to Spring Cloud Sleuth, all the messages exchanged
    during a single request incoming to the system via a gateway have the same `traceId`.
    Whether it is synchronous REST communication, or asynchronous messaging, you may
    easily track and correlate the logs between microservices using standard log files,
    or log aggregator tools such as Elastic Stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'I think now is a good time to run and test our sample system. First, we have
    to build the whole project with the `mvn clean install` command. To access the
    code sample with two microservices listening for messages on two different exchanges,
    you should switch to the `advanced` branch ([https://github.com/piomin/sample-spring-cloud-messaging/tree/advanced](https://github.com/piomin/sample-spring-cloud-messaging/tree/advanced)).
    You should launch all the applications available there—gateway, discovery, and
    the three microservices (`account-service`, `order-service`, `product-service`).
    The currently discussed case assumes we have also started RabbitMQ, Logstash,
    Elasticsearch, and Kibana using its Docker container. For detailed instructions
    on how to run Elastic Stack locally using Docker images, refer to [Chapter 9](a84b38a5-4a2f-4e4b-a7fe-6396a2864021.xhtml),
    *Distributed Logging and Tracing*. The following diagram shows the architecture
    of the system in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a3ef3e9-bd8e-45fb-8b75-b8050e1e4560.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After running all the required applications and tools, we may proceed to the
    tests. Here''s the sample request, which can be sent to the `order-service` via
    the API gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When I run the test for the first time with the applications configured following
    the description in the previous sections, it doesn''t work. I can understand that
    some of you may be confused a little, because generally it was tested on the default
    settings. To make it run properly, I also have to add the following property in
    `application.yml`: `spring.cloud.stream.rabbit.bindings.output.producer.routingKeyExpression:
    ''"#"''`. It sets the default producer''s routing key to conform with the exchange''s
    routing key automatically created during the application boot. In the following
    screenshot, you may see one of the output exchange definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d568e23a-9c65-4fb4-beb0-483c8755debb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After the modification described previously, the test should be concluded successfully.
    The logs printed by the microservices are correlated with each other by `traceId`.
    I modified the default Sleuth logging format in `logback-spring.xml` a little,
    and that''s how it is configured now—`%d{HH:mm:ss.SSS} %-5level [%X{X-B3-TraceId:-},%X{X-B3-SpanId:-}]
    %msg%n`. After sending the test request `order-service` test request, log the
    following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, `account-service` also uses the same logging format and prints
    the same `traceId` as `order-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'All the logs generated during the single transaction can be aggregated using
    Elastic Stack. You may filter the entries by the `X-B3-TraceId` field, for example,
    `9da1e5c83094390d`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2061976-3bb0-4367-ba21-2b8b76e7e2b4.png)'
  prefs: []
  type: TYPE_IMG
- en: The publish/subscribe model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main motivation for creating a Spring Cloud Stream project is, in fact,
    support for a persistent publish/subscribe model. In the previous sections, we
    have discussed point-to-point communication between microservices, which is just
    an additional feature. However, the programming model is still the same, irrespective
    of whether we decided to use a point-to-point or publish/subscribe model.
  prefs: []
  type: TYPE_NORMAL
- en: In publish/subscribe communication, the data is broadcast through shared topics.
    It reduces the complexity of both the producer and the consumer, and allows new
    applications to be easily added to the existing topology without any changes in
    flow. This can be clearly seen in the last-presented sample of the system, where
    we decided to add the second application that has consumed events produced by
    the source microservice. In comparison to the initial architecture, we had to
    define custom message channels dedicated for each of the target applications.
    With direct communication through queues, the message can be consumed by only
    one application instance, so as such, the solution was necessary. The uses of
    the publish/subscribe model simplify that architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Running a sample system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The development of the sample application is simpler for the publish/subscribe
    model than for point-to-point communication. We don''t have to override any default
    message channels to enable interaction with more than one receiver. In comparison
    with the initial sample that has illustrated messaging to a single target application
    (`account-service`), we only need to modify configuration settings a little. Because
    Spring Cloud Stream, by default, binds to the topic, we don''t have to override
    `exchangeType` for the input message channel. As you may see in the configuration
    fragment that follows, we still use point-to-point communication when sending
    the response to `order-service`. If we really think about it, that makes sense.
    The  `order-service` microservice sends the message that has to be received by
    both `account-service` and `product-service`, while the response from them is
    addressed only to `order-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The logic of the main processing method of `product-service` is really simple.
    It just has to find all the `productIds` from the received order, change the number
    of stored products for every one of them, and then send the response to `order-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: To access the current sample, you just have to switch to the  `publish_subscribe` branch,
    available at [https://github.com/piomin/sample-spring-cloud-messaging/tree/publish_subscribe](https://github.com/piomin/sample-spring-cloud-messaging/tree/publish_subscribe).
    Then, you should build the parent project and run all the services as for the
    previous sample. If you would like to test it all works fine until you have only
    one running instance of `account-service` and `product-service`. Let's discuss
    that problem.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling and grouping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When talking about microservice-based architecture, scalability is always presented
    as one of its main advantages. The ability to scale up the system by creating
    multiple instances of a given application is very important. When doing this,
    different instances of an application are placed in a competing consumer relationship,
    where only one of the instances is expected to handle a given message. For point-to-point
    communication, it is not a problem, but in a publish-subscribe model, where the
    message is consumed by all the receivers, it may be a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Running multiple instances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Availability for scaling up the number of microservice's instances is one of
    the main concepts around Spring Cloud Stream. However, there is no magic behind
    this idea. Running multiple instances of an application is very easy with Spring
    Cloud Stream. One of the reasons for this is native support from message brokers,
    which is designed to handle many consumers and huge amounts of traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, all the messaging microservices also expose the RESTful HTTP API,
    so first, we have to customize the server port per instance. We have performed
    such operations before. We may also consider setting two Spring Cloud Stream properties,
    `spring.cloud.stream.instanceCount` and `spring.cloud.stream.instanceIndex`. Thanks
    to them, every instance of the microservice is able to receive information about
    how many other examples of the same application are started and what is its own
    instance index. The correct configuration of these properties is required only
    if you would like to enable the partitioning feature. I''ll talk about this mechanism
    more in a moment. Now, let''s take a look at the configuration settings of the
    scaled-up applications. Both `account-service` and `product-service` define two
    profiles for the purpose of running multiple instances of the application. We
    have customized there an HTTP port of the server, number, and an index of the
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: After building the parent project, you may run two instances of the application.
    Each of them is initialized with properties assigned to the right profile passed
    during startup, for example, `java -jar --spring.profiles.active=instance1 target/account-service-1.0-SNAPSHOT.jar`.
    If you send a test request to the `order-service` endpoint `POST /`, the new order
    would be forwarded to the RabbitMQ topic exchange in order to be received by both
    the `account-service` and `product-service`, which are connected to that exchange.
    The problem is that the message is received by all the instances of each service,
    which is not exactly what we wanted to achieve. Here, a grouping mechanism comes
    with help.
  prefs: []
  type: TYPE_NORMAL
- en: Consumer groups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our purpose is clear. We have many microservices that consume messages from
    the same topic. Different instances of an application are placed in a competing
    consumer relationship, but only one of them should handle a given message. Spring
    Cloud Stream introduces the concept of a consumer group that models this behavior.
    To activate such a behavior, we should set a property called `spring.cloud.stream.bindings.<channelName>.group`,
    with a group name. After setting it, all groups that subscribe to a given destination
    receive a copy of the published data, but only one member of each group receives
    and handles a message from that destination. In our case, there are two groups.
    First, for all the `account-service` instances with a name account, and second,
    for a `product-service` with a name product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the current binding configuration for `account-service`. The `orders-in`
    destination is a queue created for direct communication with `order-service`,
    so only `orders-out` is grouped by service name. An analogous configuration has
    been prepared for `product-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The first difference is visible in the names of queues automatically created
    for the RabbitMQ exchange. Now, it is not a randomly generated name, such as `orders-in.anonymous.qNxjzDq5Qra-yqHLUv50PQ`,
    but a determined string consisting of the destination and group name. The following
    screenshot shows all the queues currently existing on RabbitMQ:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea66ddab-9c35-40be-9bb6-9b858aa41305.png)'
  prefs: []
  type: TYPE_IMG
- en: You may perform the retest by yourself to verify if the message is received
    by only one application in the same group. However, you have no confidence which
    instance would handle the incoming message. In order to determine this, you can
    use a partitioning mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Cloud Stream provides support for partitioning data between multiple
    instances of an application. In the typical use case, the destination is viewed
    as being divided into different partitions. Each producer, when sending messages
    received by multiple consumer instances, ensures that data is identified by configured
    fields to force processing by the same consumer instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable the partitioning feature for your application, you have to define
    the `partitionKeyExpression` or `partitionKeyExtractorClass` properties, and `partitionCount`
    in the producer configuration settings. Here''s the sample configuration that
    may be provided for your application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Partitioning mechanisms also require setting of the `spring.cloud.stream.instanceCount`
    and `spring.cloud.stream.instanceIndex` properties on the consumer side. It also
    has to be explicitly enabled with the `spring.cloud.stream.bindings.input.consumer.partitioned`
    property set to `true`. The instance index is responsible for identifying the
    unique partition from which a particular instance receives data. Generally, `partitionCount`
    on the producer side and `instanceCount` on the consumer side should be equal.
  prefs: []
  type: TYPE_NORMAL
- en: Let me familiarize you with the partitioning mechanism provided by Spring Cloud
    Stream. First, it calculates a partition key based on `partitionKeyExpression`,
    which is evaluated against the outbound message or implementation of the `PartitionKeyExtractorStrategy`
    interface, which defines the algorithm for extracting the key for the message.
    Once the message key is calculated, the target partition is determined as a value
    between zero and `partitionCount - 1`. The default calculation formula is `key.hashCode()
    % partitionCount`. It can be customized with the `partitionSelectorExpression`
    property, or by creating an implementation of the `org.springframework.cloud.stream.binder.PartitionSelectorStrategy`
    interface. The calculated key is matched with `instanceIndex` on the consumer
    side.
  prefs: []
  type: TYPE_NORMAL
- en: 'I think that the main concept around partitioning has been explained. Let''s
    proceed to the sample. Here''s the current configuration of the input channel
    for `product-service` (the same as with the account group name set for `account-service`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We have two running instances of each microservice that consumes data from the
    topic exchange. There are also two partitions set for the producer within `order-service`.
    The message key is calculated based on the `customerId` field from the `Order`
    object. The partition with index `0` is dedicated for orders having an even number
    in the `customerId` field, while the partition with index `1` is for odd numbers
    in the `customerId` field.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, RabbitMQ does not have native support for partitioning. It is interesting
    how Spring Cloud Stream implements the partitioning process with RabbitMQ. Here''s
    a screenshot that illustrates the list of bindings for exchanges created in RabbitMQ.
    As you may see, there are two routing keys that have been defined for the exchange—`orders-out-0`
    and `orders-out-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e699f2a7-dbb7-46fd-bb9d-3b1952b257c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you send an order with `customerId` equal to 1 in a JSON message, for example,
    `{"customerId": 1,"productIds": [4],"status": "NEW"}`, it would always be processed
    by an instance with `instanceIndex=1`. It may be checked out in the application
    logs or by using the RabbitMQ web console. Here''s a diagram with the message
    rates for each queue, where the message with `customerId=1` has been sent several
    times:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/683097cd-7210-4c3b-973c-bea138878ac4.png)'
  prefs: []
  type: TYPE_IMG
- en: Configuration options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Cloud Stream configuration settings may be overridden using any mechanism
    supported by Spring Boot, such as application arguments, environment variables,
    and YAML or property files. It defines a number of generic configuration options that
    may be applied to all binders. However, there are also some additional properties specific
    for a particular message broker used by the application.
  prefs: []
  type: TYPE_NORMAL
- en: Spring Cloud Stream properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The current group of properties applies to the whole Spring Cloud Stream application.
    All the following properties are prefixed with  `spring.cloud.stream`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Default value | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `instanceCount` | `1` | The number of running instances of an application.
    For more details, refer to the *Scaling and grouping* section. |'
  prefs: []
  type: TYPE_TB
- en: '| `instanceIndex` | `0` | The index of the instance of the application. For
    more details, also refer to the *Scaling and grouping* section. |'
  prefs: []
  type: TYPE_TB
- en: '| `dynamicDestinations` | - | A list of destinations that can be bound dynamically.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `defaultBinder` | - | The default binder in case there are multiple binders
    defined. For more details, also refer to the *Multiple binders* section. |'
  prefs: []
  type: TYPE_TB
- en: '| `overrideCloudConnectors` | `false` | This is used only if the cloud is active
    and Spring Cloud Connectors is found on the classpath. When it is set to `true`, binders
    completely ignore the bound services and rely on the `spring.rabbitmq.*` or `spring.kafka.*` Spring
    Boot properties. |'
  prefs: []
  type: TYPE_TB
- en: Binding properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next group of properties is related to a message channel. In Spring Cloud
    nomenclature, these are binding properties. They may be assigned only to a consumer,
    a producer, or to both simultaneously. Here is a list of the properties, along
    with their default value and a description:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Default value | Description |'
  prefs: []
  type: TYPE_TB
- en: '| `destination` | - | The target destination name on the broker configured
    for the message channel. It can be specified as a comma-separated list of destinations
    if the channel is used by only one consumer. |'
  prefs: []
  type: TYPE_TB
- en: '| `group` | `null` | The consumer group of the channel. See the *Scaling and
    grouping* section for more details. |'
  prefs: []
  type: TYPE_TB
- en: '| `contentType` | `null` | The content type of messages exchanged via a given
    channel. We may set it, for example, to `application/json`. Then all the objects
    sent from that application would be automatically converted to a JSON string.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `binder` | `null` | The default binder used by the channel. See the *Multiple
    binders* section for more details. |'
  prefs: []
  type: TYPE_TB
- en: The consumer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following list of properties is available for input bindings only, and
    must be prefixed with `spring.cloud.stream.bindings.<channelName>.consumer`. I''ll
    indicate just the most important of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Default value** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `concurrency` | `1` | Number of consumers per single input channel |'
  prefs: []
  type: TYPE_TB
- en: '| `partitioned` | `false` | It enables receiving data from a partitioned producer
    |'
  prefs: []
  type: TYPE_TB
- en: '| `headerMode` | `embeddedHeaders` | If it is set to `raw`, header parsing
    on input is disabled |'
  prefs: []
  type: TYPE_TB
- en: '| `maxAttempts` | `3` | Number of retries if message processing fails. Setting
    this option to `1` disables the retry mechanism |'
  prefs: []
  type: TYPE_TB
- en: The producer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following binding properties are available for output bindings only, and
    must be prefixed with `spring.cloud.stream.bindings.<channelName>.producer`. I''ll
    also indicate only the most important of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Default value** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `requiredGroups` | - | A comma-separated list of groups that must be created
    on the message broker |'
  prefs: []
  type: TYPE_TB
- en: '| `headerMode` | `embeddedHeaders` | If it is set to `raw`, header parsing
    on input is disabled |'
  prefs: []
  type: TYPE_TB
- en: '| `useNativeEncoding` | `false` | If it is set to `true`, the outbound message
    is serialized directly by the client library |'
  prefs: []
  type: TYPE_TB
- en: '| `errorChannelEnabled` | `false` | If it is set to `true`, failure messages
    are sent to the error channel for the destination |'
  prefs: []
  type: TYPE_TB
- en: The advanced programming model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basics around the Spring Cloud Stream programming model have been presented
    together with samples of point-to-point and publish/subscribe communication. Let's
    discuss some more advanced example features.
  prefs: []
  type: TYPE_NORMAL
- en: Producing messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In all the samples presented in this chapter, we have sent orders through RESTful
    API for testing purposes. However, we may easily create some test data by defining
    the message source inside the application. Here''s a bean that generates one message
    per second using `@Poller` and sends it to the output channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you probably remember, `account-service` and `product-service` have been
    receiving events from `order-service` and then sending back the response message.
    We have created the `OrderSender` bean, which was responsible for preparing the
    response payload and sending it to the output channel. It turns out that the implementation
    may be simpler if we return the response object in method and annotate it with
    `@SentTo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can even imagine such an implementation, such as the following, without
    using `@StreamListener`. The transformer pattern is responsible for changing the
    object''s form. In that case, it modifies two `order` fields—`status` and `price`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Consuming messages conditionally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assuming we would like to treat messages incoming to the same message channel
    differently, we may use conditional dispatching. Spring Cloud Stream supports
    dispatching messages to multiple `@StreamListener` methods registered on an input
    channel, based on a condition. That condition is a **Spring Expression Language**
    (**SpEL**) expression defined in the `condition` attribute of the `@StreamListener`
    annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the sample implementation that defines two methods annotated with `@StreamListener`
    that listen on the same topic. One of them is dedicated only for messages incoming
    from `account-service`, while the second is dedicated only for `product-service`.
    The incoming message is dispatched, based on its header with the `processor` name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Using Apache Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have mentioned Apache Kafka a couple of times when discussing Spring Cloud
    integration with message brokers. However, until now, we haven't run any samples
    based on that platform. The fact is that RabbitMQ tends to be the preferred choice
    when working with Spring Cloud projects, but Kafka is also worthy of our attention.
    One of its advantages over RabbitMQ is native support for partitioning, which
    is one of the most important features of Spring Cloud Stream.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka is not a typical message broker. It is rather a distributed streaming
    platform. Its main feature is to allow you to publish and subscribe to streams
    of records. It is especially useful for real-time streaming applications that
    transform or react to streams of data. It is usually run as a cluster consisting
    of one or more servers, and stores streams of records in topics.
  prefs: []
  type: TYPE_NORMAL
- en: Running Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unfortunately, there is no official Docker image with Apache Kafka. However,
    we may use one that is unofficial, for example, that shared by Spotify. In comparison
    to other available Kafka docker images, this one runs both Zookeeper and Kafka
    in the same container. Here''s the Docker command that launches Kafka and exposes
    it on port `9092`. Zookeeper is also available outside on port `2181`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Customizing application settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To enable Apache Kafka for the application, include the `spring-cloud-starter-stream-kafka`
    starter to the dependencies. Our current sample is very similar to to the sample
    of publish/subscribe using with RabbitMQ publish/subscribe with grouping and partitioning
    presented in *The publish/subscribe model*, section. The only difference is in
    the dependencies and configuration settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spring Cloud Stream automatically detects and uses a binder found on the classpath.
    The connection settings may be overridden with `spring.kafka.*` properties. In
    our case, we just need to change the auto-configured Kafka client address to the
    Docker machine address `192.168.99.100`. The same modification should be performed
    for Zookeeper, which is used by the Kafka client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'After starting discovery, gateway, and all the required instances of microservices,
    you can perform the same tests as for the previous samples. If everything is configured
    correctly, you should see the following fragment in the logs during your application
    boot. The result of the tests is exactly the same as for the sample based on RabbitMQ:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Kafka Streams API support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spring Cloud Stream Kafka provides a binder specially designed for Kafka Streams
    binding. With this binder, the application can leverage the Kafka Streams API.
    To enable such a feature for your application, include the following dependency
    to your project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The Kafka Streams API provides high-level stream DSL. It may be accessed by
    declaring the `@StreamListener` method that takes the `KStream` interface as a
    parameter. KStream provides some useful methods for stream manipulation, well-known
    from other streaming APIs such as `map`, `flatMap`, `join`, or `filter`. There
    are also some other methods specific to Kafka Stream, such as `to(...)` (for sending
    streams to a topic) or `through(...)` (same as `to`, but also creates a new instance
    of `KStream` from the topic):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Configuration properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some of the Spring Cloud configuration settings for Kafka have been presented
    before when discussing the implementation of the sample application. Here''s a
    table with the most important properties, which can be set for customizing the
    Apache Kafka binder. All these properties are prefixed by `spring.cloud.stream.kafka.binder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Default value | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `brokers` | `localhost` | A comma-separated list of brokers with or without
    port information. |'
  prefs: []
  type: TYPE_TB
- en: '| `defaultBrokerPort` | `9092` | It sets the default port if no port is defined
    using the `brokers` property. |'
  prefs: []
  type: TYPE_TB
- en: '| `zkNodes` | `localhost` | A comma-separated list of ZooKeeper nodes with
    or without port information. |'
  prefs: []
  type: TYPE_TB
- en: '| `defaultZkPort` | `2181` | It sets the default ZooKeeper port if no port
    is defined using the `zkNodes` property. |'
  prefs: []
  type: TYPE_TB
- en: '| `configuration` | - | A Key/Value map of Kafka client properties. It applies
    to all the clients created by the binder. |'
  prefs: []
  type: TYPE_TB
- en: '| `headers` | - | The list of custom headers that will be forwarded by the
    binder. |'
  prefs: []
  type: TYPE_TB
- en: '| `autoCreateTopics` | `true` | If set to `true`, the binder creates new topics
    automatically. |'
  prefs: []
  type: TYPE_TB
- en: '| `autoAddPartitions` | `false` | If set to `true`, the binder creates new
    partitions automatically. |'
  prefs: []
  type: TYPE_TB
- en: Multiple binders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Spring Cloud Stream nomenclature, the interface that may be implemented to
    provide connection to physical destinations at the external middleware is called
    **binder**. Currently, there are two available built-in binder implementations—Kafka
    and RabbitMQ. In case you would like to provide a custom binder library, the key
    interface that is an abstraction for a strategy for connecting inputs and outputs
    to external middleware is `Binder`, having two methods—`bindConsumer` and `bindProducer`.
    For more details, you may refer to the Spring Cloud Stream specifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important thing for us is an ability to use multiple binders in a single
    application. You can even mix different implementations, for example, RabbitMQ
    with Kafka. Spring Cloud Stream relies on Spring Boot''s auto-configuration in
    the binding process. The implementation available on the classpath is used automatically.
    In case you would like to use both the default Binders, include the following
    dependencies to the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: If more than one binder has been found in the classpath, the application must
    detect which of them should be used for the particular channel binding. We may
    configure the default binder globally with the `spring.cloud.stream.defaultBinder`
    property, or individually per each channel with the `spring.cloud.stream.bindings.<channelName>.binder`
    property. Now, we go back for a moment to our sample to configure multiple binders
    there. We define RabbitMQ for direct communication between `account-service` and
    `order-service`, and Kafka for the publish/subscribe model between `order-service`
    and other microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the equivalent configuration to that provided for `account-service`
    in the `publish_subscribe` branch ([https://github.com/piomin/sample-spring-cloud-messaging/tree/publish_subscribe](https://github.com/piomin/sample-spring-cloud-messaging/tree/publish_subscribe)),
    but based on two different binders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Cloud Stream can be treated as a separate category in comparison to all
    the other Spring Cloud projects. It is often being associated with other projects,
    and which are currently strongly promoted by Pivotal Spring Cloud Data Flow. That
    is a toolkit for building data integration and real-time data processing pipelines.
    However, it is a huge subject and rather a topic of discussion for a separate
    book.
  prefs: []
  type: TYPE_NORMAL
- en: More to the point, Spring Cloud Stream provides support for asynchronous messaging,
    which may be easily implemented using a Spring annotation style. I think that
    for some of you, that style of inter-service communication is not as obvious as
    the RESTful API model. Therefore, I have focused on showing you the examples of
    point-to-point and publish/subscribe communication using Spring Cloud Stream.
    I have also described the differences between those two styles of messaging.
  prefs: []
  type: TYPE_NORMAL
- en: The publish/subscribe model is nothing new, but thanks to Spring Cloud Stream,
    it may be easily included to the microservice-based system. Some of the key concepts,
    such as consumer groups or partitioning, have also been described in this chapter.
    After reading it, you should be able to implement microservices based on the messaging
    model, and integrate them with other Spring Cloud libraries in order to provide
    logging, tracing, or just deploying them as part of the existing, REST-based microservices
    system.
  prefs: []
  type: TYPE_NORMAL
