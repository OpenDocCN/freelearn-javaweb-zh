- en: Deploying Our Microservices to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will deploy the microservices in this book to Kubernetes.
    We will also learn about some of the core features of Kubernetes, such as using
    **Kustomize** to configure deployments for different runtime environments and
    using Kubernetes deployments object for rolling upgrades. Before we do that, we
    need to review how we use service discovery. Since Kubernetes comes with built-in
    support for service discovery, it seems unnecessary to deploy our own since we
    have been using Netflix Eureka up to this point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Replacing Netflix Eureka with Kubernetes `Service` objects and `kube-proxy`
    for service discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kustomize to prepare the microservices to be deployed in different environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the deployments with a version of the test script, `test-em-all.bash`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing rolling upgrades
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to roll back a failed upgrade
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the commands that are described in this book are run on a MacBook Pro using
    macOS Mojave but should be straightforward to modify if you want to run them on
    another platform such as Linux or Windows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only new tool that''s required for this chapter is the `siege` command-line
    tool, which is used for HTTP-based load testing and benchmarking. We will use
    `siege` to put some load on the Kubernetes cluster while performing rolling upgrades.
    The tool can be installed using Homebrew with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The source code for this chapter can be found in this book's GitHub repository: [https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter16](https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter16).
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to run the commands that are described in this book, you need to
    download the source code to a folder and set up an environment variable, `$BOOK_HOME`,
    that points to that folder. Some sample commands are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: All the source code examples in this chapter come from the source code in `$BOOK_HOME/Chapter16` and have
    been tested using Kubernetes 1.15.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to see the changes that were applied to the source code in this
    chapter, that is, see the changes that are required to be able to deploy the microservices
    on Kubernetes, you can compare it with the source code for [Chapter 15](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml),
    *Introduction to Kubernetes*. You can use your favorite `diff` tool and compare
    the two folders, `$BOOK_HOME/Chapter15` and `$BOOK_HOME/Chapter16`.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing Netflix Eureka with Kubernetes services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As shown in the previous chapter, [Chapter 15](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml),
    *Introduction to Kubernetes*, Kubernetes comes with a built-in discovery service
    based on Kubernetes `Service` objects and the `kube-proxy` runtime component.
    This makes it unnecessary to deploy a separate discovery service such as Netflix
    Eureka, which we used in the previous chapters. An advantage of using Kubernetes
    discovery service is that it doesn't require a client library such as Netflix
    Ribbon, which we have been using together with Netflix Eureka. This makes the
    Kubernetes discovery service easy to use, independent of which language or framework
    a microservice is based on. A drawback of using the Kubernetes discovery service
    is that it only works in a Kubernetes environment. However, since the discovery
    service is based on `kube-proxy`, which accepts requests to the DNS name or IP
    address of a service object, it should be fairly simple to replace it with a similar
    discovery service, for example, one that comes bundled with another container
    orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize this, we will remove the discovery server based on Netflix Eureka
    from our microservice landscape, as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b8eed5f-e4b6-4d3a-85a1-e0df126462ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To replace the discovery server based on Netflix Eureka with the Kubernetes
    built-in discovery service, the following changes have been applied to the source
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: Netflix Eureka and the Ribbon-specific configuration (client and server) have
    been removed from the configuration repository, `config-repo`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Routing rules in the gateway service to the Eureka server have been removed
    from the `config-repo/gateway.yml` file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've removed the Eureka server project, that is, we've removed the `spring-cloud/eureka-server` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've removed the Eureka server from the Docker Compose files and the `settings.gradle` Gradle
    file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've removed the dependency to `spring-cloud-starter-netflix-eureka-client`
    in all of Eureka's client build files, that is, `build.gradle`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've removed the no-longer-required `eureka.client.enabled=false` property
    setting from all of Eureka's client integration tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gateway service no longer uses routing based on the client-side load balancer
    in Spring Cloud using the `lb` protocol. For example,  the `lb://product-composite` routing
    destination has been replaced by the `http://product-composite` in the `config-repo/gateway.yml` file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The HTTP port used by the microservices and the authorization server has been
    changed from port the `8080` port (`9999` in the case of the authorization server)
    to the default HTTP port `80`. This has been configured in `config-repo` for each
    affected service like so:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'None of the HTTP addresses that we are using are affected by the replacement
    of Netflix Eureka with Kubernetes services. For example, addresses used by the
    composite service are unaffected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is achieved by changing the HTTP port used by the microservices and the
    authorization server to the default HTTP port, `80`, as described previously.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker Compose still works, even though Netflix Eureka has been removed.
    This can be used for running functional tests of the microservices without deploying
    them to Kubernetes, for example, running `test-em-all.bash` together with Docker
    for macOS in the same way as in the previous chapters. Removing Netflix Eureka,
    however, means that we no longer have a discovery service in place when using
    plain Docker and Docker Compose. Therefore, scaling microservices will only work
    when deploying to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've familiarized ourselves with Kubernetes services, let's move on
    to Kustomize, a tool that's used for customizing Kubernetes objects.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Kustomize
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kustomize** is a tool that''s used for creating environment-specific customizations
    of the Kubernetes definitions files, that is, the YAML files, for example, for development,
    test, staging, and production environments. Common definition files are stored
    in a `base` folder, while environment-specific additions are kept in environment-specific
    `overlay` folders. Environment-specific information can, for example, be any of
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What version of the Docker images to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of replicas to run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource quotas in terms of CPU and memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each folder contains a `kustomization.yml` file that describes its content for
    Kustomize. When deploying to a specific environment, Kustomize will take the content
    from the `base` folder and the environment-specific `overlay` folder and send
    the combined result to `kubectl`. Properties from the files in the `overlay` folder
    will override the corresponding properties in the `base` folder, if any.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will set up customizations for two sample environments:
    development and production.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The folder structure under `$BOOK_HOME/Chapter16` looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a62901e6-d94d-49e1-be8f-22ba37d810c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Since Kubernetes 1.14, `kubectl` comes with built-in support for Kustomize using
    the `-k` flag. As we will see as we proceed, deploying to the development environment
    using Kustomize will be done with the `kubectl apply -k kubernetes/services/overlays/dev` command.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up common definitions in the base folder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the `base` folder, we will have one definition file for each microservice,
    but none for the resource managers (MongoDB, MySQL, and RabbitMQ). The resource
    managers will only be deployed in Kubernetes in the development environment and
    are expected to run outside of Kubernetes in the production environment—for example,
    in an existing database and queue manager service on premises or as a managed
    service in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition files in the `base` folder contain a deployment object and a
    service object for each microservice. Let''s go through a typical deployment object
    in `kubernetes/services/base/product.yml`. It is geared toward what is required
    in a development environment. It starts with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This part looks exactly the same as it does for the NGINX deployment we used
    in the previous chapter, [Chapter 15](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml),
    *Introduction to Kubernetes*, in the *Trying out a sample deployment* section,
    so we don't need to go through it again.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part looks a bit different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding source code in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The Docker image specified, `hands-on/product-service`, will be created underneath
    where we build our microservices. See the *Building Docker images* section for
    more information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `imagePullPolicy: Never` declaration tells Kubernetes to not try to download
    the Docker image from a Docker registry. See the *Building Docker images* section
    for further information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SPRING_PROFILES_ACTIVE` environment variable is defined to tell the Spring
    application to use the `docker` Spring profile in the configuration repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A secret, `config-client-credentials`, is used to provide the container with
    credentials for accessing the configuration server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HTTP port that's used is the default HTTP port `80`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A resource limit is defined to maximize the available memory to 350 MB, that
    is, in the same way as when we used Docker Compose in the previous chapters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The last part of the declaration of the deployment object contains liveness
    and readiness probes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding source code in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The **liveness** **probe** is based on an HTTP request that's sent to the Spring
    Boot Actuator `info` endpoint. This means that if the microservice instance is
    in such bad shape that it is not capable of responding 200 (OK) to a request that's
    sent to the lightweight `info` endpoint, it is time for Kubernetes to restart
    the microservice instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **readiness probe** is based on an HTTP request that's sent to the Spring
    Boot Actuator `health` endpoint. Kubernetes will only send requests to the microservice
    instance if its `health` endpoint responds with the HTTP status 200 (OK). Not
    responding with 200 (OK) typically means that the microservice instance has problems
    with reaching some of the resources it depends on, and so it makes sense to not
    send any requests to a microservice instance when it does not respond with 200
    (OK) on the `health` endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The liveness and the readiness probes can be configured using the following
    properties:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initialDelaySeconds` specifies how long Kubernetes waits to probe a container
    after it''s started up.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`periodSeconds` specifies the time between probe requests sent by Kubernetes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeoutSeconds` specifies how long Kubernetes waits on a response before it
    treats the probe as failed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`failureThreshold` specifies how many failed attempts Kubernetes makes before
    giving up. In the case of a liveness probe, this means restarting the pod. In
    the case of a readiness probe, it means that Kubernetes will not send any more
    requests to the container.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`successThreshold` specifies the number of successful attempts that are required
    for a probe to be considered successful again after a failure. This only applies
    to readiness probes since they must be set to `1` if specified for liveness probes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding optimal settings for the probes can be challenging, that is, finding
    a proper balance between getting a swift reaction from Kubernetes when the availability
    of a pod changes and not overloading the pods with probe requests. Specifically
    configuring a liveness probe with values that are too low can result in Kubernetes
    restarting pods that just take some time to start, that is, that don't need to
    be restarted. Starting a large number of pods with values that have been set too
    low on the liveness probes can result in a lot of unnecessary restarts. Setting
    the configuration values too high on the probes (except for the `successThreshold`
    value) makes Kubernetes react slower, which can be annoying in a development environment.
    Proper values also depend on the available hardware, which affects the startup
    times for the pods. For the scope of this book, `failureThreshold` for the liveness
    probes is set to a high value, `20`, to avoid unnecessary restarts on computers
    with limited hardware resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The service object in `kubernetes/services/base/product.yml` looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The service object looks similar to the NGINX service object we used in the
    previous chapter, [Chapter 15](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml), *Introduction
    to Kubernetes*, in the *Trying out a sample deployment* section. One difference
    is that the service type is `ClusterIP` (which is the default type and therefore
    not specified). The service object will receive internal requests on port `80`
    and forward them to the target port, `80`, on the selected pod. The only exception
    to this is the gateway microservice that is exposed externally using a `NodePort`
    service on the host''s port, that is, `31443`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have the Kustomize file that binds everything together in the `base`
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It simply lists the YAML definition files that Kustomize shall use in the `base`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will see how we can use these base definitions with the definitions
    in the `overlay` folders, and see how they are applied using the `-k` switch with
    the `kubectl apply` command.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Kubernetes for development and test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will deploy the microservices in an environment to be used
    for development and test activities, for example, system integration tests. This
    type of environment is used primarily for functional tests and is therefore configured
    to use minimal system resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the deployment objects in the `base` folder are configured for a development
    environment, they don''t need any further refinement in the overlay for development.
    We only have to add deployment and service objects for the three resource managers
    for RabbitMQ, MySQL, and MongoDB in the same way as when using Docker Compose.
    We will deploy the resource managers in the same Kubernetes namespace as the microservices.
    This is illustrated by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5ebe638-fb4c-4344-ab1d-2cd3b1698480.png)'
  prefs: []
  type: TYPE_IMG
- en: The definition files for the resource managers can be found in the `kubernetes/services/overlays/dev` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kustomization.yml` file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It defines that the `base` folder shall be used as the base and adds the three
    resources we mentioned previously.
  prefs: []
  type: TYPE_NORMAL
- en: Building Docker images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normally, we have to push images to a Docker registry and configure Kubernetes
    to pull images from the registry. In our case, where we have a local single node
    cluster, we can shortcut this process by pointing our Docker client to the Docker
    engine in Minikube and then run the `docker-compose build` command. This will
    result in the Docker images being immediately available to Kubernetes. For development,
    we will be using `latest` as the Docker image version for the microservices.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering how we can update a pod that uses the `latest` Docker
    image.
  prefs: []
  type: TYPE_NORMAL
- en: From Kubernetes 1.15, this is very simple. Just change the code and rebuild
    the Docker image, for example, using the `build` command that's described here.
    Then, update a pod with the `kubectl rollout restart` command.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the `product` service has been updated, run the `kubectl rollout
    restart deploy product` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can build Docker images from source as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `eval $(minikube docker-env)` command directs the local Docker client to
    communicate with the Docker engine in Minikube, for example, when building the
    Docker images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `docker-compose.yml` file has been updated to specify a name for the Docker
    images it builds. For example, for the `product` service, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`latest` is the default tag for a Docker image name, so it is not specified.'
  prefs: []
  type: TYPE_NORMAL
- en: With the Docker images built, we can start creating the Kubernetes resource
    objects!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can deploy the microservices to Kubernetes, we need to create a namespace,
    the required config maps, and secrets. After the deployment is performed, we will
    wait for the deployments to be up and running, and also verify that we got the
    expected result in terms of deployed pods and Docker images that were used per
    pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a namespace, `hands-on`, and set it as the default namespace for `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: All application configuration is kept in the configuration repository that's
    managed by the configuration server. The only configuration information that needs
    to be stored outside of the configuration repository is the credentials for connecting
    to the configuration server and an encryption key. The encryption key is used
    by the configuration server to keep sensitive information in the configuration
    repository encrypted at rest, that is, on disk.
  prefs: []
  type: TYPE_NORMAL
- en: We will store the configuration repository in a config map with all the sensitive
    information encrypted; see [Chapter 12](a250774a-03a1-41b1-b935-cbeb9624b6e3.xhtml),
    *Centralized Configuration*, for details. The credentials for connecting to the
    configuration server and the encryption key will be stored in two secrets, one
    for the configuration server and one for its clients.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check this, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the config map for the configuration repository based on the files in
    the `config-repo` folder with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the secret for the configuration server with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the secret for the clients of the configuration server with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Since we have just entered commands that contain sensitive information in clear
    text, for example, passwords and an encryption key, it is a good idea to clear
    the `history` command. To clear the `history` command both in memory and on disk,
    run the `history -c; history -w` command.
  prefs: []
  type: TYPE_NORMAL
- en: See the discussion at [https://unix.stackexchange.com/a/416831](https://unix.stackexchange.com/a/416831)
    for details on the `history` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid a slow deployment due to Kubernetes downloading Docker images (potentially
    causing the liveness probes we described previously to restart our pods), run
    the following `docker pull` commands to download the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the microservices for the development environment, based on the `dev`
    overlay, using the `-k` switch to activate Kustomize, as described previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Wait for the deployments and their pods to be up and running by running the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Expect each command to respond with `deployment.extensions/... condition met`.
    `...` will be replaced with the name of the actual deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the Docker images that are used for development, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The response should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42300572-7769-4a8b-9b4d-5efd9d361dab.png)'
  prefs: []
  type: TYPE_IMG
- en: We are now ready to test our deployment!
  prefs: []
  type: TYPE_NORMAL
- en: But before we can do that, we need to go through changes that are required in
    the test script for use with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the test script for use with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test the deployment we will, as usual, run the test script, that is, `test-em-all.bash`.
    To work with Kubernetes, the circuit breaker tests have been slightly modified.
    Take a look at the `testCircuitBreaker()` function for more details. The circuit
    breaker tests call the `actuator` endpoints on the `product-composite` service
    to check their health state and get access to circuit breaker events. The `actuator`
    endpoints are not exposed externally, so the test script needs to use different
    techniques to access the internal endpoints when using Docker Compose and Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: When using Docker Compose, the test script will launch a Docker container using
    a plain `docker run` command that calls the `actuator` endpoints from the inside
    of the network created by Docker Compose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using Kubernetes, the test script will launch a Kubernetes pod that it
    can use to run the corresponding commands inside Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see how this is done when using Docker Compose and Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Reaching the internal actuator endpoint using Docker Compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The base command that''s defined for Docker Compose is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that the container will be killed using the `--rm` switch after each execution
    of a test command.
  prefs: []
  type: TYPE_NORMAL
- en: Reaching the internal actuator endpoint using Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since launching a pod in Kubernetes is slower than starting a container, the
    test script will launch a single pod, `alpine-client`. The pod will be launched at
    the start of the `testCircuitBreaker()` function, and the tests will use the `kubectl
    exec` command to run the test commands in this pod. This will be much faster than
    creating and deleting a pod for each test command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Launching the single pod is handled at the beginning of the `testCircuitBreaker()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the circuit breaker tests, the pod is deleted by using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Choosing between Docker Compose and Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make the test script work with both Docker Compose and Kubernetes, it assumes
    that Docker Compose will be used if the `HOST` environment variable is set to
    `localhost`; otherwise, it assumes that Kubernetes will be used. See the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The default value for the `HOST` environment variable in the test script is `localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the `EXEC` variable has been set up, depending on whether the tests are
    running on Docker Compose or on Kubernetes, it is used in the `testCircuitBreaker()` test
    function. The test starts by verifying that the circuit breaker is closed with
    the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: A final change in the test script occurs because our services are now reachable
    on the `80` port inside the cluster; that is, they are no longer on the `8080`
    port.
  prefs: []
  type: TYPE_NORMAL
- en: If the various ports that we've used seem confusing, review the definitions
    of the services in the *Setting up common definitions in the base folder* section.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When launching the test script, we have to give it the address of the host that
    runs Kubernetes, that is, our Minikube instance, and the external port where our
    gateway service listens for external requests. The `minikube ip` command can be
    used to find the IP address of the Minikube instance and, as mentioned in the
    *Setting up common definitions in the base folder* section, we have assigned the
    external `NodePort 31443` to the gateway service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the tests with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the output from the script we will see how the IP address of the Minikube
    instance is used and also how the `alpine-client` pod is created and destroyed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97c12d92-177b-46e9-9f35-57cef2157850.png)'
  prefs: []
  type: TYPE_IMG
- en: Before we move on and look at how to set up a corresponding environment for
    staging and production use, let's clean up what we have installed in the development
    environment to preserve resources in the Kubernetes cluster. We can do this by
    simply deleting the namespace. Deleting the namespace will recursively delete
    the resources that exist in the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete the namespace with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: With the development environment removed, we can move on and set up an environment
    targeting staging and production.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Kubernetes for staging and production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will deploy the microservices in an environment for staging
    and production usage. A staging environment is used for performing **quality**
    **assurance** (**QA**) and **user acceptance tests** (**UAT**) as the last step
    before taking a new release into production. To be able to verify that the new
    release not only meets functional requirements but also non-functional requirements,
    for example, in terms of performance, robustness, scalability, and resilience,
    a staging environment is configured to be as similar as possible to the production
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'When deploying to an environment for staging or production, there are a number
    of changes required compared to when deploying for development or tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource managers should run outside of the Kubernetes cluster**: It is technically
    feasible to run databases and queue managers for production use on Kubernetes
    as stateful containers using `StatefulSets` and `PersistentVolumes`. At the time
    of writing this chapter, I recommend against it, mainly because the support for
    stateful containers is relatively new and unproven in Kubernetes. Instead, I recommend using
    the existing database and queue manager services on premises or managed services
    in the cloud, leaving Kubernetes to do what it is best for, that is, running stateless
    containers. For the scope of this book, to simulate a production environment,
    we will run MySQL, MongoDB, and RabbitMQ as plain Docker containers outside of
    Kubernetes using the already existing Docker Compose files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lockdown:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For security reasons, things like `actuator` endpoints and log levels need to
    be constrained in a production environment.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Externally exposed endpoints should also be reviewed from a security perspective.
    For example, access to the configuration server should most probably be locked
    down in a production environment, but we will keep it exposed in this book for
    convenience.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker image tags must be specified to be able to track which versions of the
    microservices have been deployed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale up available resources**: To meet the requirements of both high availability
    and higher load, we need to run at least two pods per deployment. We might also
    need to increase the amount of memory and CPU that are allowed to be used per
    pod. To avoid running out of memory in the Minikube instance, we will keep one
    pod per deployment but increase the maximum memory allowed in the production environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Set up a production-ready Kubernetes cluster**:This is outside the scope
    of this book, but, if feasible, I recommend using one of the managed Kubernetes
    services provided by the leading cloud providers. For the scope of this book,
    we will deploy to our local Minikube instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is not meant to be an exhaustive list of things that have to be considered
    when setting up an environment for production, but it's a good start.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our simulated production environment will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ddfcf4d-89dd-4007-a19c-4568e6cb2220.png)'
  prefs: []
  type: TYPE_IMG
- en: Changes in the source code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following changes have been applied to the source code to prepare for deployment
    in an environment that''s used for production:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Spring profile named `prod`  has been added to the configuration files in
    the `config-repo` configuration repository:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `prod` profiles, the following has been added:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URLs to the resource managers that run as plain Docker containers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We are using the `172.17.0.1` IP address to address the Docker engine in the
    Minikube instance. This is the default IP address for the Docker engine when creating
    it with Minikube, at least for Minikube up to version 1.2.
  prefs: []
  type: TYPE_NORMAL
- en: There is work ongoing for establishing a standard DNS name for containers to
    use if they need to access the Docker host they are running on, but at the time
    of writing this chapter, this work effort hasn't been completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Log levels have been set to warning or higher, that is, error or fatal. For
    example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The only `actuator` endpoints that are exposed over HTTP are the `info` and `health` endpoints
    that are used by the liveness and readiness probes in Kubernetes, as well as the `circuitbreakerevents` endpoint
    that''s used by the test script, `test-em-all.bash`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In the production `overlay` folder, `kubernetes/services/overlays/prod`, one
    deployment object for each microservice has been added with the following content
    so that it can be merged with the base definition:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For all microservices, `v1` is specified as the Docker `image` tag, and the
    `prod` profile is added to the active Spring profiles. For example, we have the
    following for the `product` service:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For the Zipkin and configuration server, which don''t keep their configuration
    in the configuration repository, environment variables have been added in their
    deployment definitions with the corresponding configuration:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, a `kustomization.yml` file defines that the files in the `prod overlay`
    folder shall be merged by specifying the `patchesStrategicMerge` patch mechanism
    with the corresponding definition in the `base` folder:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In a real-world production environment, we should have also changed the `imagePullPolicy:
    Never` setting to `IfNotPresent`, that is, to download Docker images from a Docker
    registry. But since we will be deploying the production setup to the Minikube
    instance where we manually build and tag the Docker images, we will not update
    this setting.'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To simulate production-grade resource managers, MySQL, MongoDB, and RabbitMQ will
    run outside of Kubernetes using Docker Compose. We start them up as we did in
    the previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to tag the existing Docker images with `v1` using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: From here, the commands are very similar to how we deployed to the development
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use another Kustomize overlay and use different credentials for the
    configuration server, but, otherwise, it will be the same (which, of course, is
    a good thing!). We will use the same configuration repository but configure the
    pods to use the `prod` Spring profile, as described previously. Follow these steps
    to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a namespace, `hands-on`, and set this as the default namespace for `kubectl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the config map for the configuration repository based on the files in
    the `config-repo` folder with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the secret for the configuration server with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the secret for the clients of the configuration server with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the clear text encryption key and passwords from the command history:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the microservices for the development environment, based on the `prod` overlay,
    using the `-k` switch to activate Kustomize, as described previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Wait for the deployments to be up and running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the Docker images that are currently being used for production, run
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The response should look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f3b0b67-78f2-4815-8c42-3f67a8931783.png)'
  prefs: []
  type: TYPE_IMG
- en: Note the `v1` version of the Docker images!
  prefs: []
  type: TYPE_NORMAL
- en: Also note that the resource manager pods for MySQL, MongoDB, and RabbitMQ are
    gone; these can be found with the `docker-compose ps` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the test script, `thest-em-all.bash`, to verify the simulated production
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Expect the same type of output that we got when the test script was run against
    the development environment.
  prefs: []
  type: TYPE_NORMAL
- en: Performing a rolling upgrade
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Historically, updates often result in some downtime of the component that is
    updated. In a system landscape with an increasing number of autonomous microservices
    that are updated independently of each other, recurring downtimes due to frequent
    updates of the microservices is not acceptable. Being able to deploy an update
    without downtime becomes crucial.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will see how we can perform a rolling upgrade, updating
    a microservice to a new version of its Docker image without requiring any downtime.
    Performing a rolling upgrade means that Kubernetes first starts the new version
    of the microservice in a new pod, and when it reports as being healthy, Kubernetes
    will terminate the old one. This ensures that there is always a pod up and running,
    ready to serve incoming requests during the upgrade. A prerequisite for a rolling
    upgrade to work is that the upgrade is backward compatible, both in terms of APIs
    and message formats that are used to communicate with other services and database
    structures. If the new version of the microservice requires changes to either
    the external APIs, message formats, or database structures that the old version
    can't handle, a rolling upgrade can't be applied. A deployment object is configured
    to perform any updates as a rolling upgrade by default.
  prefs: []
  type: TYPE_NORMAL
- en: To try this out, we will create a v2 version of the Docker image for the `product`
    service and then start up a test client, `siege`, that will submit one request
    per second during the rolling upgrade. The assumption is that the test client
    will report 200 (OK) for all the requests that it sends during the upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the rolling upgrade
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To prepare for the rolling upgrade, first, verify that we have the `v1` version
    of the product pod deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output should reveal that `v1` of the Docker image is in use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ba120f8-86f6-4b8e-9f5c-88e9e66e05cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a `v2` tag on the Docker image for the `product` service with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: To try out a rolling upgrade from a Kubernetes perspective, we don't need to
    change any code in the `product` service. Deploying a Docker image with another
    tag than the existing one will start up a rolling upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to observe whether any downtime occurs during the upgrade, we will
    start a low volume load test using `siege`. The following command starts a load
    test that simulates one user (`-c1`) that submits one request per second on average
    (`-d1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Since the test calls the gateways health endpoint, it verifies that all the
    services are healthy.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should receive an output that looks similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a9f905e-809b-4146-9b64-208c924649cb.png)'
  prefs: []
  type: TYPE_IMG
- en: The interesting part in the response is the HTTP status code, which we expect
    to be `200` at all times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, monitor changes to the state of the product pods with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Upgrading the product service from v1 to v2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To upgrade the `product` service, edit the `kubernetes/services/overlays/prod/product-prod.yml` file
    and change `image: hands-on/product-service:v1` to `image: hands-on/product-service:v2`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the update with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Expect a response from the command that reports that most of the objects are
    left unchanged, except for the product deployment that should be reported to be
    updated to `deployment.apps/product configured`.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes comes with some shorthand commands. For example, `kubectl set image
    deployment/product pro=hands-on/product-service:v2` can be used to perform the
    same update that we did by updating the definitions file and running the `kubectl
    apply` command. A major benefit of using the `kubectl apply` command is that we
    can keep track of the changes by pushing the changes in the source code to a version
    control system such as Git. This is very important if we want to be able to handle
    our infrastructure as code. When playing around with a Kubernetes cluster, only
    use it to test shorthand commands, as this can be very useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the output from the `kubectl get pod -l app=product -w` command we launched in
    the *Preparing the rolling upgrade* section, we will see some action occurring.
    Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e395b5c0-fc40-43f1-b8ec-f85a9ce86628.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see how the existing pod (`ffrdh`) initially reported that it was
    up and running and also reported to be healthy when a new pod was launched (`t8mcl`).
    After a while (`16s`, in my case), it is reported as up and running as well. During
    a certain time period, both pods will be up and running and processing requests.
    After a while, the first pod is terminated (2 minutes, in my case).
  prefs: []
  type: TYPE_NORMAL
- en: 'When looking at the `siege` output, we can sometimes find a few errors being
    reported in terms of the `503` service unavailable errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3caab39b-4d02-4f93-ace5-5402b72aac09.png)'
  prefs: []
  type: TYPE_IMG
- en: This typically happens when the old pod is terminated. Before the old pod is
    reported unhealthy by the readiness probe, it can receive a few requests during
    its termination, that is, when it is no longer capable of serving any requests.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using a Service
    Mesh to Improve Observability and Management*, we will see how we can set up routing
    rules that move traffic in a smoother way from an old pod to a newer one without
    causing 503 errors. We will also see how we can apply retry mechanisms to stop
    temporary failures from reaching an end user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wrap this up by verifying that the pod is using the new `v2` version of the
    Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output reveals that `v2` of the Docker image is in use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd38e1cd-b695-49b8-991b-63e395ee577d.png)'
  prefs: []
  type: TYPE_IMG
- en: After performing this upgrade, we can move on to learning what happens when
    things fail. In the next section, we will see how we can roll back a failed deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back a failed deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From time to time, things don't go according to plan, for example, an upgrade
    of deployments and pods can fail for various reasons. To demonstrate how to roll
    back a failed upgrade, let's try to upgrade to `v3` without creating a `v3` tag
    on the Docker image!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try out the following shorthand command to perform the update:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Expect to see the following changes reported by the `kubectl get pod -l app=product
    -w` command we launched in the *Preparing the rolling upgrade*section:'
  prefs: []
  type: TYPE_NORMAL
- en: <q>![](img/290f3cd4-de45-4a31-8abb-495b8f20c15e.png)</q>
  prefs: []
  type: TYPE_NORMAL
- en: We can clearly see that the new pod (ending with `m2dtn`, in my case) has failed
    to start because of a problem finding its Docker image (as expected). If we look
    at the output from the `siege` test tool, no errors are reported, only 200 (OK)!
    Here, the deployment hangs since it can't find the requested Docker image, but
    no errors are affecting end users since the new pod couldn't even start.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what history Kubernetes has regarding the product''s deployment.
    Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'You will receive output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5e3c16a-efab-43df-a5a5-04dda9c5c4c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can guess that revision 2 is the one with the latest successful deployment,
    that is, `v2` of the Docker image. Let''s check this with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In the response, we can see that `revision #2` is the one with Docker image
    `v2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e3d11c8-a112-40c0-be6b-29631e196c19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s roll back our deployment to `revision=2` with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Expect a response that confirms the rollback, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/936308fe-f3a2-45aa-949b-2fe8fe52cbe5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `kubectl get pod -l app=product -w` command we launched in the *Preparing
    the rolling upgrade* section will report that the new (not working) pod has been
    removed by the `rollback` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f0d45ea-cb69-4f32-a2fa-55edca821532.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can wrap this up by verifying that the current image version is still `v2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Cleaning up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To delete the resources that we used, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: Stop the watch command, `kubectl get pod -l app=product -w`, and the load test
    program, `siege`, with *Ctrl* *+* *C*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Delete the namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Shut down the resource managers that run outside of Kubernetes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The `kubectl delete namespace` command will recursively delete all Kubernetes
    resources that existed in the namespace, and the `docker-compose down` command
    will stop MySQL, MongoDB, and RabbitMQ. With the production environment removed,
    we have reached the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to deploy the microservices in this book on
    Kubernetes. We also introduced some core features in Kubernetes, such as using Kustomize to
    configure deployments for different runtime environments, using Kubernetes deployment
    objects for rolling upgrades, and how to roll back a failed update if required. To
    help Kubernetes understand when the microservices need to be restarted and if
    they are ready to accept requests, we implemented liveness and readiness probes.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to be able to deploy our microservices, we had to replace Netflix Eureka
    with the built-in discovery service in Kubernetes. Changing the discovery service
    was done without any code changes – all we had to do was apply changes to the
    build dependencies and some of the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how we can further utilize Kubernetes to reduce
    the number of supporting services we need to deploy in Kubernetes. Head over to
    the next chapter to see how we can eliminate the need for the configuration server
    and how our edge server can be replaced by a Kubernetes ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why did we remove the Eureka server from the microservices landscape when deploying
    it on Kubernetes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What did we replace the Eureka server with and how was the source code of the
    microservices affected by this change?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How are base and overlay folders used with Kustomize?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we get a running pod updated with changes in a config map or secret?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we are using the latest tag on a Docker image, how can we get running pods
    using a new build of the Docker image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What commands can we use to roll back a failed deployment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the purpose of liveness and readiness probes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different ports that are being used in the following service definition?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
