- en: Distributed Logging and Tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When breaking down a monolith into microservices, we usually spend a lot of
    time thinking about business boundaries or the partitioning of our application
    logic, but we forget about the logs. From my own experience as a developer and
    software architect, I can say that developers do not usually pay much attention
    to logging. On the other hand, operation teams, which are responsible for application
    maintenance, are mainly dependent on logs. Regardless of one's area of expertise,
    it is indisputable that logging is something that all applications have to do,
    whether they have monolithic or microservices architecture. However, microservices
    force adding a whole new dimension to design and arrangement of application logs. There
    are many small, independent, horizontally scaled, intercommunicating services
    that are running on multiple machines. Requests are often processed by multiple
    services. We have to correlate these requests and store all the logs in a single,
    central place in order to make it easier to view them. Spring Cloud introduces
    a dedicated library that implements a distributed tracing solution, Spring Cloud
    Sleuth.
  prefs: []
  type: TYPE_NORMAL
- en: There is also one thing that should be discussed here. Logging is not the same
    as tracing! It is worth pointing out the differences between them. Tracing is
    following your program's data flow. It is typically used by technical support
    teams to diagnose where a problem occurs. You have to trace your system flow to
    discover performance bottlenecks or times when the error occurs. Logging is used
    for error reporting and detecting. It should always be enabled, in contrast to
    tracing. When you design a large system and you would like to have good and flexible
    error reporting across machines, you should definitely think about collecting
    log data in a centralized way. The recommended and most popular solution for this
    is the **ELK** stack (**Elasticsearch** + **Logstash** + **Kibana**). There is
    no dedicated library for this stack in Spring Cloud, but the integration may be
    realized with Java logging frameworks, such as Logback or Log4j. There is another
    tool that will be discussed in this chapter, Zipkin. It is a typical tracing tool
    that helps gather timing data that can be used to troubleshoot latency problems
    in microservice architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we will cover in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The best practices for logging in microservices-based systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Spring Cloud Sleuth to append tracing information to messages and correlating
    events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating the Spring Boot application with Logstash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying and filtering log entries using Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Zipkin as a distributed tracing tool and integrating it with the application
    through Spring Cloud Sleuth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best logging practices for microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most important best practices for dealing with logging is to trace
    all the incoming requests and outgoing responses. Maybe it seems obvious to you,
    but I have seen a couple of applications that did not comply with that requirement.
    If you meet this demand, there is one consequence that occurs with microservices-based
    architecture. The overall number of logs in your system increases compared to
    monolithic applications, where there is no messaging. This, in turn, requires
    us to pay even more attention to logging than before. We should do our best to
    generate as little information as possible, even though this information can tell
    us much about the situation. How do we achieve this? First of all, it is good
    to have the same log message format across all the microservices. For example,
    let''s consider how to print variables in the application logs. I suggest you
    use the JSON notation in view of the fact that, usually, messages exchanged between
    microservices are formatted with JSON. This format has a very straightforward
    standard, which makes your logs easily readable and parseable, as shown in the
    following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding format is much easier to analyze than something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'But generally, the most important thing here is standardization. No matter
    which format you choose, it is crucial to use it everywhere. You should also be
    careful to ensure that your logs are meaningful. Try to avoid sentences that do
    not contain any information. For example, from the following format, it is not
    clear which order is being processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if you really want this kind of log entry format, try to assign it
    to different log levels. It is really a bad practice to log everything with the
    same level of `INFO`. Some kinds of information are more important than others,
    so the one difficulty here is to decide what level the log entry should be logged
    at. Here are some suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TRACE`: This is for very detailed information, intended only for development.
    You might keep it for a short period of time, just after deployment to a production
    environment, but treat it as a temporary file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DEBUG`: At this level, log anything that happens in the program. This is mostly
    used for debugging or troubleshooting by developers. The distinction between `DEBUG`
    and `TRACE` is probably the most difficult.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INFO`: At this level, you should log the most important information during
    the operation. These messages have to be easily understandable, not just for developers,
    but also for administrators or advanced users, to let them quickly find out what
    the application is doing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WARN`: At this level, log all events that could potentially become errors.
    Such a process may be continued, but you should take extra caution with it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERROR`: Usually, you print exceptions at this level. The important thing here
    is not to throw exceptions everywhere if, for example, only one business logic
    execution has not succeeded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FATAL`: This Java logging level designates very severe error events that will
    probably cause the application to stop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other good logging practices, but I have mentioned the most important
    ones for use in microservices-based systems. It is also worth mentioning one more
    aspect of logging, normalization. If you would like to easily understand and interpret
    your logs, you should definitely know how and when they were collected, what they
    contain, and why they were emitted. There are some especially important characteristics
    that should be normalized across all microservices, such as `Time` (when), `Hostname`
    (where), and `AppName` (who). As you will see in the next part of this chapter,
    this kind of normalization is very useful when a centralized method of collecting
    logs is implemented in your system.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with Spring Boot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Boot uses Apache Commons Logging for internal logging, but if you are
    including dependencies with starters, Logback will be used by default in your
    application. It doesn't inhibit the possibility of using other logging frameworks
    in any way. The default configurations are also provided for Java Util Logging,
    Log4J2, and SLF4J. Logging settings may be configured in the `application.yml`
    file with `logging.*` properties. The default log output contains the date and
    time in milliseconds, log level, process ID, thread name, the full name of the
    class that has emitted the entry, and the message. It may be overridden by using
    the `logging.pattern.console` and `logging.pattern.file` properties respectively
    for the console and file appenders.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Spring Boot only logs on to a console. In order to allow the writing
    of log files in addition to a console output, you should set a `logging.file`
    or `logging.path` property. If you specify the `logging.file` property, the logs
    would be written to the file at an exact location or a location relative to the
    current directory. If you set `logging.path`, it creates a `spring.log` file in
    the specified directory. Log files will be rotated after reaching 10 MB.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last thing that can be customized in the `application.yml` settings file
    is the log levels. By default, Spring Boot writes messages with `ERROR`, `WARN`,
    and `INFO` levels. We may override this setting for every single package or class
    with `logging.level.*` properties. The root logger can also be configured using
    `logging.level.root`. Here''s an example configuration in the `application.yml`
    file, which changes the default pattern format, as well as a few log levels, and
    sets the location of the logging file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the preceding example, such a configuration is pretty simple,
    but, in some cases, it is not enough. If you would like to define additional appenders
    or filters, you should definitely include the configuration for one of the available
    logging systems—Logback (`logback-spring.xml`), Log4j2 (`log4j2-spring.xml`),
    or Java Util Logging (`logging.properties`). As I have mentioned earlier, by default, Spring
    Boot uses Logback for the application logs. If you provide the `logback-spring.xml`
    file in the root of the classpath, it will override all settings defined in `application.yml`.
    For example, you may create file appenders that rotate logs daily and retain a
    maximum history of 10 days. This feature is very commonly used in applications.
    In the next section of this chapter, you will also learn that a custom appender
    is required to integrate your microservice with Logstash. Here''s an example Logback
    configuration file''s fragment that sets a daily rolling policy for the `logs/order.log`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also worth mentioning that Spring recommends using `logback-spring.xml`
    for Logback instead of the default `logback.xml`. Spring Boot includes a couple
    of extensions to Logback that may be helpful for an advanced configuration. They
    cannot be used in the standard `logback.xml`, but only with `logback-spring.xml`.
    We have listed some of these extensions that will allow you to define profile-specific
    configurations or surface properties from the Spring Environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Centralizing logs with ELK Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ELK is the acronym for three open source tools—Elasticsearch, Logstash, and
    Kibana. It is also called **Elastic Stack**. The heart of this system is **Elasticsearch**, a
    search engine based on another open source project written in Java, Apache Lucene. This
    library is especially suitable for applications that require full-text searches
    in cross-platform environments. The main reason for the popularity of Elasticsearch
    is its performance. Of course, it has some other advantages, such as scalability,
    flexibility, and easy integration by providing a RESTful, JSON-based API for searching
    stored data. It has a large community and many use cases, but the most interesting
    one for us is its ability to store and search logs generated by applications.
    Logging is the main reason for including Logstash in ELK Stack. This open source
    data-processing pipeline allows us to collect, process, and input data into Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: '**Logstash** supports many inputs that pull events from external sources. What
    is interesting is that it has many outputs, and Elasticsearch is only one of them.
    For example, it can write events to Apache Kafka, RabbitMQ, or MongoDB, and it
    can write metrics to InfluxDB or Graphite. It not only receives and forwards data
    to their destinations, but can also parse and transform it on the fly.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kibana** is the last element of ELK Stack. It is an open source, data-visualization
    plugin for Elasticsearch. It allows you to visualize, explore, and discover data
    from Elasticsearch. We may easily display and filter all the logs collected from
    our application by creating search queries. On this basis, we can export data
    to PDF or CSV formats to provide reports.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up ELK Stack on the machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we try to send any logs from our application to Logstash, we have to
    configure ELK Stack on the local machine. The most suitable way to run it is through
    Docker containers. All the products in the stack are available as Docker images.
    There is a dedicated Docker registry hosted by Elastic Stack's vendor. A full
    list of published images and tags can be found at [www.docker.elastic.co](http://www.docker.elastic.co).
    All of them use `centos:7` as the base image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin from the Elasticsearch instance. Its development can be started
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Running Elasticsearch in development mode is the most convenient way of running
    it because we don''t have to provide any additional configuration. If you would
    like to launch it in production mode, the `vm.max_map_count` Linux kernel setting
    needs to be set to at least `262144`. The procedure for modifying it is different
    depending on the OS platform. For Windows with Docker Toolbox, it must be set
    via `docker-machine`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to run a container with Logstash. In addition to launching
    a container with Logstash, we should also define an input and output. The output
    is obvious—Elasticsearch, which is now available under the default Docker machine
    address, `192.168.99.100`. As an input, we define the simple TCP plugin `logstash-input-tcp`,
    which is compatible with `LogstashTcpSocketAppender` used as a logging appender
    in our sample application. All the logs from our microservices will be sent in
    JSON format. For now, it is important to set the `json` codec for that plugin. Each
    microservice will be indexed in Elasticsearch with its name and `micro` prefix.
    Here''s the Logstash configuration file, `logstash.conf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the command that runs Logstash and exposes it on port `5000`. It also
    copies the file with  the preceding settings to the container and overrides the
    default location of the Logstash configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can run the last element of the stack, Kibana. By default, this
    is exposed on port `5601` and connects to the Elasticsearch API available on port `9200` in
    order to be able to load data from there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If you would like to run all Elastic Stack products on your Docker machine on
    Windows, you would probably have to increase the default RAM memory for your Linux
    virtual image to a minimum of 2 GB. After launching all containers, you may finally
    access the Kibana dashboard available under `http://192.168.99.100:5601` and then
    proceed to integrate your application with Logstash.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating an application with ELK Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many ways of integrating Java applications with ELK Stack via Logstash.
    One of the methods involves using Filebeat, which is a log data shipper for local
    files. This approach requires a beats (`logstash-input-beats`) input configured
    for the instance of Logstash, which is, in fact, the default option. You should
    also install and launch a Filebeat daemon on the server machine. It is responsible
    for the delivery of the logs to Logstash.
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I prefer a configuration based on Logback and dedicated appenders.
    It seems to be simpler than using a Filebeat agent. Besides having to deploy an
    additional service, Filebeat requires us to play with a parsing expression, such
    as the Grok filter. When using a Logback appender, you don't require any log shippers.
    This appender is available within the project Logstash JSON encoder. You may enable
    it for your application by declaring the `net.logstash.logback.appender.LogstashSocketAppender`
    appender inside the `logback-spring.xml` file.
  prefs: []
  type: TYPE_NORMAL
- en: We will also discuss an alternative approach for sending data to Logstash, using
    a message broker. In the example that we will shortly examine, I'm going to show
    you how to use Spring `AMQPAppender` to publish logging events to a RabbitMQ exchange.
    In this case, Logstash subscribes to the exchange and consumes published messages.
  prefs: []
  type: TYPE_NORMAL
- en: Using LogstashTCPAppender
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The library `logstash-logback-encoder` provides three types of appenders—UDP,
    TCP, and async. The TCP appender is most commonly used. What is worth mentioning
    is that TCP appenders are asynchronous, and all the encoding and communication
    is delegated to a single thread. In addition to appenders, the library also provides
    some encoders and layouts to enable you to log in the JSON format. Because Spring
    Boot includes a Logback library by default, as well as `spring-boot-starter-web`,
    we only have to add one dependency to Maven `pom.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to define the appender with the `LogstashTCPAppender` class in
    the Logback configuration file. Every TCP appender requires you to configure an
    encoder. You may choose between `LogstashEncoder` and `LoggingEventCompositeJsonEncoder`.
    `LoggingEventCompositeJsonEncoder` gives you more flexibility. It is composed
    of one or more JSON providers that are mapped to the JSON output. By default,
    there are no providers configured. It doesn''t work that way with `LogstashTCPAppender`.
    By default, it includes several standard fields, such as timestamp, version, logger
    name, and stack trace. It also adds all entries from the **m****apped diagnostic
    context** (**MDC**) and the context, unless you disable it by setting one of the
    `includeMdc` or `includeContext` properties to `false`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, I would like to come back for a moment to our sample system. We are still
    in the same Git repository ([https://github.com/piomin/sample-spring-cloud-comm.git](https://github.com/piomin/sample-spring-cloud-comm.git))
    and `feign_with_discovery` branch ([https://github.com/piomin/sample-spring-cloud-comm/tree/feign_with_discovery](https://github.com/piomin/sample-spring-cloud-comm/tree/feign_with_discovery)).
    I have added some logging entries in the source code in accordance with the recommendations
    described in the *Best logging practices for microservices* section. Here''s the
    current version of the `POST` method inside `order-service`. I have used Logback
    over SLF4J as a logger by calling the `getLogger` method from `org.slf4j.LoggerFactory`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the Kibana dashboard. It is available at `http://192.168.99.100:5601`.
    The application logs may be easily discovered and analyzed there. You can select
    the required index name in the menu on the left side of the page (labeled **1**
    in the following screenshot). Log statistics are presented on the timeline graph
    (**2**). You can narrow down the time taken as search parameter by clicking a
    concrete bar or choosing a group of bars. All logs for a given period of time
    are displayed on the panel present below the graph (**3**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e56b172-f8c5-4a8a-84c3-0bece490c7a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each entry can be expanded to look at its details. In the detailed table view,
    we can see, for example, the name of the Elasticsearch index (`_index`) and the
    level or name of the microservice (`appName`). Most of those fields have been
    set by `LoggingEventCompositeJsonEncoder`. I have only defined one application-specific
    field, `appName`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/032646e3-4508-404b-b1cc-9ad63012db02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Kibana gives us a great ability to search for particular entries. We may define
    filters just by clicking on the selected entries in order to define a set of search
    criteria. In the preceding screenshot, you can see how I filtered out all the
    entries with incoming HTTP requests. As you probably remember, the `org.springframework.web.filter.CommonsRequestLoggingFilter`
    class is responsible for logging them. I have just defined the filter whose name
    is equal to a fully-qualified logger class name. Here''s the screen from my Kibana
    dashboard, which displays the logs generated only by `CommonsRequestLoggingFilter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1b536fc-7f08-4f9a-b12a-cc92d8fe5ebc.png)'
  prefs: []
  type: TYPE_IMG
- en: Using AMQP appender and a message broker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The configuration with the Spring AMQP appender and message broker is a little
    bit more complicated than the method that uses the simple TCP appender. First,
    you need to launch a message broker on your local machine. I have described this
    process in [Chapter 5](37142825-02d0-48a0-99df-1a1a88a1bbd4.xhtml), *Distributed
    Configuration with Spring Cloud Config*, where I introduced RabbitMQ for dynamic
    configuration reloading with Spring Cloud Bus. Assuming you have started an instance
    of RabbitMQ locally or as a Docker container, you can proceed to configuration.
    We have to create a queue for publishing incoming events and then bind it to the
    exchange. To achieve this, you should log in to the Rabbit management console
    and then go to the Queues section. I have created the queue with the name `q_logstash`.
    I defined the new exchange with the name `ex_logstash`, which is visible in the
    following screenshot. The queue has been bound to the exchange with routing keys
    for all the example microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6af1e4a9-b02d-4c4f-9e24-dea67478f24b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After we have launched and configured the instance of RabbitMQ, we may start
    integrating on the application side. First, you have to include `spring-boot-starter-amqp` in
    the project dependencies to provide implementations of the AMQP client and AMQP
    appender:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the only thing you have to do is to define the appender with the `org.springframework.amqp.rabbit.logback.AmqpAppender` class in
    the Logback configuration file. The most important properties that need to be
    set are the RabbitMQ network address (`host`, `port`), the name of the declared
    exchange (`exchangeName`), and the routing key (`routingKeyPattern`), which has
    to match one of the keys declared for the exchange bindings. In comparison with
    the TCP appender, a disadvantage of this approach is the need to prepare a JSON
    message sent to Logstash by yourself. Here''s a fragment of the Logback configuration
    for `order-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Logstash may be easily integrated with RabbitMQ by declaring the `rabbitmq`
    (`logstash-input-rabbitmq`) input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Spring Cloud Sleuth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Cloud Sleuth is a rather small, simple project, which nevertheless provides
    some useful features for logging and tracing. If you refer to the example discussed
    in the *Using LogstashTCPAppender* section, you can easily see that there is no
    possibility to filter all the logs related to single request. In a microservices-based
    environment, it is also very important to correlate messages exchanged by the
    applications when handling requests that are coming into the system. This is the
    main motivation in creating the Spring Cloud Sleuth project.
  prefs: []
  type: TYPE_NORMAL
- en: If Spring Cloud Sleuth is enabled for the application, it adds some HTTP headers
    to the requests, which allows you to link requests with the responses and the
    messages exchanged by independent applications, for example, through RESTful API.
    It defines two basic units of work—span and trace. Each of these is identified
    by a unique 64 bit ID. The value of the trace ID is equal to the initial value
    of the span ID. Span refers to a single exchange, where the response is sent as
    a reaction to the request. Trace is something that is usually called **correlation
    IT**, and it helps us to link all the logs from different applications generated
    during the processing of requests coming into the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every trace and span ID is added to the Slf4J **MDC** (**mapped diagnostic
    context**), so you will able to extract all the logs with a given trace or span
    in a log aggregator. MDC is just a map that stores the context data of the current
    thread. Every client request coming to the server is handled by a different thread.
    Thanks to this, each thread can have access to the values of its MDC within the
    thread lifecycle. As well as `spanId` and `traceId`, Spring Cloud Sleuth also
    adds the following two spans to the MDC:'
  prefs: []
  type: TYPE_NORMAL
- en: '`appName`: The name of the application that has generated the log entry'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exportable`: This specifies whether the log should be exported to Zipkin or
    not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to the preceding features, Spring Cloud Sleuth also provides:'
  prefs: []
  type: TYPE_NORMAL
- en: An abstraction over common distributed tracing data models, which allows integrating
    with Zipkin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Records timing information in order to aid it in latency analysis. It also includes
    different sampling policies to manage the volume of data exported to Zipkin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrates with common Spring components taking part in communication like servlet
    filter, asynchronous endpoints, RestTemplate, message channels, Zuul filters and
    Feign client.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating Sleuth with an application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to enable Spring Cloud Sleuth features for the application, just add
    the `spring-cloud-starter-sleuth` starter to the dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After including this dependency, the format of the log entries generated by
    the application has been changed. You can see this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Searching events using Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spring Cloud Sleuth automatically adds HTTP headers `X-B3-SpanId` and `X-B3-TraceId`
    to all the requests and responses. These fields are also included to the MDC as
    `spanId` and `traceId`. But before moving to the Kibana dashboard, I would like
    you to take a look at the following figure. This is a sequence diagram that illustrates
    the communication flow between sample microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8eb077a-8417-4960-9e12-f67cb2cf2070.png)'
  prefs: []
  type: TYPE_IMG
- en: There are two available methods that are exposed by `order-service`. The first
    is for creating a new order and the second is for confirming it. The first `POST
    /` method, in fact, calls endpoints from all other services directly from `customer-service`,
    `product-service`, and `account-service` through `customer-service`. The second
    `PUT /{id}` method integrates with only one endpoint from `account-service`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow described previously may now be mapped by the log entries stored in
    ELK Stack. When using Kibana as a log aggregator, together with fields generated
    by Spring Cloud Sleuth, we may easily find entries by filtering them using trace
    or span IDs. Here''s an example, where we have discovered all the events related
    to a call of the `POST /` endpoint from `order-service` with the `X-B3-TraceId` field equal
    to `103ec949877519c2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c108158a-5db0-419a-88dd-a10b8f87d796.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here''s an example similar to the previous one, but where all events stored
    during the processing request are sent to the `PUT /{id}` endpoint. These entries
    have been also filtered out by the `X-B3-TraceId` field, the value of which is
    equal to `7070b90bfb36c961`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4587b4f2-9b08-4ba3-8f56-662bdf8ee49a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, you can see the full list of fields, which has been sent to Logstash
    by the microservice application. The fields with the `X-` prefix have been included
    in the message by the Spring Cloud Sleuth library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dda5a4be-8e5e-4cc0-aa93-eb13883dbcbb.png)'
  prefs: []
  type: TYPE_IMG
- en: Integrating Sleuth with Zipkin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zipkin is a popular, open source, distributed tracing system, which helps in
    gathering timing data needed to analyze latency problems in microservices-based
    architecture. It is able to collect, look up, and visualize data using a UI web
    console. The Zipkin UI provides a dependency diagram showing how many traced requests
    were processed by all applications within the system. Zipkin consists of four
    elements. I have already mentioned one of them, Web UI. The second one is Zipkin
    collector, which is responsible for validating, storing, and indexing all incoming
    trace data. Zipkin uses Cassandra as a default backend store. It also natively
    supports Elasticsearch and MySQL. The last element is query service, which provides
    a simple JSON API for finding and retrieving traces. It is mostly consumed by
    Web UI.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Zipkin server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We may run the Zipkin server locally in several ways. One of these ways involves
    using a Docker container. The following command launches an in-memory server instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the Docker container, the Zipkin API is available at `http://192.168.99.100:9411`.
    Alternatively, you can start it using Java libraries and the Spring Boot application.
    To enable Zipkin for your application, you should include the following dependencies
    to your Maven `pom.xml` file, as shown in the following code fragment. The default
    versions are managed by `spring-cloud-dependencies`. For our example application,
    I have used `Edgware.RELEASE` Spring Cloud Release Train:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'I have added a new `zipkin-service` module to our example system. It is really
    simple. The only thing that has to be implemented is the application main class,
    which is annotated with `@EnableZipkinServer`. Thanks to this, the Zipkin instance
    is embedded in the Spring Boot application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to launch the Zipkin instance on its default port, we have to override
    the default server port in the `application.yml` file. After launching the application,
    the Zipkin API is available at `http://localhost:9411`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Building the client application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you would like to use both Spring Cloud Sleuth and Zipkin in your project,
    just add starter `spring-cloud-starter-zipkin` to the dependencies. It enables
    integration with Zipkin via the HTTP API. If you have started the Zipkin server
    as an embedded instance inside the Spring Boot application, you don''t have to
    provide any additional configuration containing connection address. If you use
    the Docker container, you should override the default URL in `application.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can always take advantage of integration with service discovery. If you
    have the discovery client enabled through `@EnableDiscoveryClient` for your application
    with the embedded Zipkin server, you may just set the property `spring.zipkin.locator.discovery.enabled`
    to `true`. In that case, even if it is not available under the default port, all
    applications will be able to localize it through the registered name. You should
    also override the default Zipkin application name with the `spring.zipkin.baseUrl` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, Spring Cloud Sleuth sends only a few selected incoming requests.
    It is determined by the property `spring.sleuth.sampler.percentage`, the value
    of which needs to be a double between 0.0 and 1.0\. The sampling solution has
    been implemented because data volumes exchanged between distributed systems can
    be sometimes very high. Spring Cloud Sleuth provides sampler interface that can
    be implemented to take control over the sampling algorithm. The default implementation
    is available in class `PercentageBasedSampler`. If you would like to trace all
    the requests exchanged by your applications, just declare `AlwaysSampler` bean.
    It may be useful for the test purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Analyze data with the Zipkin UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's go back for a moment to our example system. As I have mentioned before,
    the new `zipkin-service` module has been added. I have also enabled Zipkin tracing
    for all the microservices, including `gateway-service`.  By default, Sleuth takes
    the value `spring.application.name` as a span's service name. You may override
    that name with the `spring.zipkin.service.name` property.
  prefs: []
  type: TYPE_NORMAL
- en: To successfully test our system with Zipkin, we have to start the microservices,
    gateway, discovery, and Zipkin servers. To generate and send some test data, you
    could just run the JUnit test implemented by the `pl.piomin.services.gateway.GatewayControllerTest` class.
    It sends 100 messages to `order-service` via `gateway-service`, available at `http://localhost:8080/api/order/**`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s analyze the data collected from all the services by Zipkin. You may
    easily check it out using its UI web console. All the traces are tagged with the
    service''s name spans. If there are five spans for the entry, it means that the
    request coming into the system has been processed by five different services.
    You can see this in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22f22dfd-2f85-4951-a595-58f9d9f2e542.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You may filter the entries with different criteria, such as the service name,
    span name, trace ID, request time, or duration. Zipkin also visualizes failed
    requests and sorts them by duration, in descending or ascending order:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/838af6f7-c7cb-4894-8a9e-1f13a49baa51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can take a look at the details of every entry. Zipkin visualizes the flow
    between all the microservices taking part in communication. It is considering
    timing the data of every incoming request. You may uncover the reasons for latency
    in your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8eef8c1-15f9-4bd9-8ea3-41cc319da73a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Zipkin provides some additional interesting features. One of these is the ability
    to visualize dependencies between applications. The following screenshot illustrates
    the communication flow of our sample system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ac3b51e-bbae-409f-8fd2-515abf55c6ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You may check out how many messages have been exchanged between services just
    by clicking on the relevant element:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d479575-78db-4869-8be4-77bc371c36e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Integration via message broker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Integration with Zipkin via HTTP is not the only option. As is usual with Spring
    Cloud, we may use a message broker as a proxy. There are two available brokers—RabbitMQ
    and Kafka. The first of these can be included in the project by using the `spring-rabbit`
    dependency, while the second can be included with `spring-kafka`. The default
    destination name for both of these brokers is `zipkin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This feature also requires changes on the Zipkin server side. We have configured
    a consumer that is listening for the data coming into the RabbitMQ or Kafka queue.
    To achieve this, just include the following dependencies in your project. You
    still need to have the `zipkin-server` and `zipkin-autoconfigure-ui` artifacts
    in the classpath:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You should annotate the main application class with `@EnableZipkinStreamServer`
    instead of `@EnableZipkinServer`. Fortunately, `@EnableZipkinStreamServer` is
    also annotated with `@EnableZipkinServer`, which means that you may also use the
    standard Zipkin server endpoints for collecting spans over HTTP, and for searching
    them with the UI web console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logging and tracing are usually not very important during development, but these
    are the key features that are used in the maintenance of the system. In this chapter,
    I have placed emphasis on the fields of development and operations. I have shown
    you how to integrate a Spring Boot microservice application with Logstash and
    Zipkin in several ways. I have also shown you some examples to illustrate how
    to enable Spring Cloud Sleuth features for an application in order to make it
    easier to monitor calls between many microservices. After reading this chapter,
    you should also be able to effectively use Kibana as a log aggregator tool and
    Zipkin as a tracing tool for discovering bottlenecks in communication inside your
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Spring Cloud Sleuth, in conjunction with Elastic Stack and Zipkin, seems to
    be a very powerful ecosystem, which removes any doubts you might have about problems
    with monitoring systems that consist of many independent microservices.
  prefs: []
  type: TYPE_NORMAL
