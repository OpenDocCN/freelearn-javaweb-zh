- en: Docker Support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already discussed the basics of microservices architecture and Spring
    Cloud projects in the first part of this book. In the second part, we looked at
    the most common elements of that architecture and we discussed how to implement
    them using Spring Cloud. So far, we have talked about some important topics related
    to microservice migration, such as centralized logging, distributed tracing, security,
    and automated testing. Now, as we are armed with that knowledge, we may proceed
    to the final part of the book, where we will discuss the real power of microservices
    as a cloud-native development approach. The ability to isolate applications from
    each other using containerization tools, implementing continuous deployment in
    the software delivery process and the ability to easily scale an application are
    things that all contribute to the rapidly growing popularity of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you will probably remember from earlier chapters, we have used Docker images
    for running third-party tools and solutions on the local machine. With that in
    mind, I would like to introduce you to the main concepts of Docker, such as its
    basic commands and use cases. This information will help you to run the samples
    presented in previous chapters. We will then discuss how to build images with
    our example Spring Boot application, as well as how to run them inside the containers
    on the local machine. We will use simple Docker commands for that, as well as
    more advanced tools such as the Jenkins server, which helps you to perform full,
    continuous delivery and enables a Continuous Integration process in your organization.
    Finally, we will introduce one of the most popular tools used for the automation
    of deploying, scaling, and managing containerized applications: Kubernetes. All
    of our examples will be run locally on a single-node Kubernetes cluster via Minikube.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we will cover in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Most useful Docker commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building Docker containers with Spring Boot microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running Spring Cloud components on Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous Integration/Continuous Delivery with Jenkins and Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and running microservices on Minikube
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker is a tool that helps you to create, deploy, and run applications by using
    containers. It was designed with the view to benefit both developers and system
    administrators in accordance with the DevOps philosophy. Docker helps to improve
    the software delivery process by solving some important concerns related with
    it. One of those concerns is the idea of immutable delivery, which is related
    to something called **it works for me**. It is especially important that a developer
    uses the same image for their tests as the one that is used in production when
    working in Docker. The only difference that should be seen is during configuration.
    Software delivery in an immutable delivery pattern seems to be particularly important
    for a microservices-based system as there are many applications deployed independently.
    Thanks to Docker, developers can now focus on writing code without worrying about
    the target OS (where the application would be launched). The operation can, therefore,
    use the same interface for deploying, starting, and maintaining all the applications.
  prefs: []
  type: TYPE_NORMAL
- en: There are also many other reasons for Docker's growing popularity. After all,
    the containerization idea is nothing new in the Information Technology world.
    Linux containers were introduced many years ago and have been a part of the kernel
    since 2008\. However, Docker has introduced several new things and solutions that
    other technologies haven't. Firstly, it provides a simple interface that allows
    you to easily package an application with dependencies to a single container before
    running it across different versions and implementations of Linux kernel. The
    container may be run locally or remotely on any Docker-enabled server, and every
    container starts in seconds. We can also easily run every command on it without
    going inside a container. In addition, the sharing and distribution mechanisms
    of Docker images allows developers to commit their changes and push and pull images
    in the same way they share source code, for example, using Git. Currently, almost
    all of the most popular software tools are published on Docker Hub as an image,
    some we have successfully used for running the tools required for our sample applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some essential definitions and elements that Docker architecture
    is composed of; the most important is a container. Containers run on a single
    machine and share the OS kernel with that machine. They contain everything you
    need to run specific software on your machine code: runtime, system tools, system
    libraries, and settings. Containers are created from the instructions found within
    a Docker image. Images are like a kind of recipe or template that defines the
    steps for installing and running necessary software on a container. Containers
    can also be compared to virtual machines as they have similar resource isolation
    and allocation benefits. However, they virtualize the operating system instead
    of the hardware, making them more portable and efficient than VMs. The following
    diagram illustrates the architectural differences between a Docker container and
    a virtual machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c4496c4-a46a-4400-a77d-9e5fff6d99ca.png)'
  prefs: []
  type: TYPE_IMG
- en: All containers are launched on a physical or virtual machine that is called
    a **Docker host**. Docker hosts, in turn, run a Docker daemon, which listens for
    the commands sent by the Docker client through a Docker API. Docker clients may
    be command-line tools or other software such as Kinematic. Besides running a daemon,
    a Docker host is responsible for storing cached images and containers created
    from those images. Every image is built from a set of layers. Each layer contains
    only the incremental differences from the parent layer. Such an image is not small
    and needs to be stored elsewhere. This place is called the **Docker registry**.
    You may create your own private repository or use the existing public repository
    available on the web. The most popular repository is Docker Hub, which contains
    almost all of the required images.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker installation instructions for Linux are specific to each distribution
    ([https://docs.docker.com/install/#supported-platforms](https://docs.docker.com/install/#supported-platforms)).
    However, sometimes you have to run a Docker daemon after installation, which you
    can do by calling the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we will focus on instructions for the Windows platform. Generally,
    you have two available options when installing Docker Community Edition (CE) on
    Windows or Mac. The fastest and easiest way is by using Docker for Windows, which
    is available at [https://www.docker.com/docker-windows](https://www.docker.com/docker-windows).
    This is a native Windows application that provides an easy-to-use development
    environment for building, shipping, and running containerized applications. This
    is definitely the best option to utilize, because it uses Windows-native Hyper-V
    virtualization and networking. There is, however, one disadvantage—it is available
    only for Microsoft Windows 10 Professional or Enterprise 64-bit. Earlier versions
    of Windows should use Docker Toolbox, which can be downloaded here at, [https://docs.docker.com/toolbox/toolbox_install_windows/](https://docs.docker.com/toolbox/toolbox_install_windows/).
    This includes the Docker platform, the command-line with Docker Machine, Docker
    Compose, Kitematic, and VirtualBox. Note that you can’t run Docker Engine natively
    on Windows using Docker Toolbox because it uses Linux-specific kernel features.
    Instead, you must use the Docker Machine command (`docker-machine`), which creates
    a Linux VM on the local machine and runs it using Virtual Box. This VM may be
    accessed by your machine using a virtual address that is, by default, `192.168.99.100`.
    All previously discussed examples were integrating with the Docker tools available
    at that IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Commonly used Docker commands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After installing Docker Toolbox on Windows you should run Docker Quickstart
    Terminal. It does everything that is needed, including creating and starting Docker
    Machine and providing the command line interface. If you type a Docker command
    without any parameters, you should now be able to see the full list of available
    Docker client commands with descriptions. These are the types of commands we will
    look at:'
  prefs: []
  type: TYPE_NORMAL
- en: Running and stopping a container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List and remove container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pull and push images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running and stopping a container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first Docker command that is usually run after installation is `docker
    run`. As you may remember, this command is one of the most commonly used in previous
    examples. This command does two things: it pulls and downloads the image definition
    from the registry, in case it is not cached locally, and starts the container.
    There are many options that can be set for this command, which you can easily
    check by running `docker run --help`. Some options have one-letter shortcuts,
    which are often the most commonly used options. Option `–d` runs a container in
    the background, while `–i` keeps `stdin` open even if it is not attached. If your
    container has to expose any ports outside, you can use the activate option `–p`
    with the definition `<port_outside_container>:<port_inside_container>`. Some images
    need additional configurations that are usually done through environment variables
    that can be overridden with the `–e` option. It is also often useful to set a
    friendly name for the container using the `--name` option in order to run other
    commands on it with ease. Take a look at the example Docker command visible here.
    It starts the container with Postgres, creates a database user with a password,
    and exposes it on port `55432`. Now, the Postgres database is available at the
    address `192.168.99.100:55432`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The container with Postgres persists data. The recommended mechanism for containers
    that store data accessed by outside applications is via volumes. A volume may
    be passed to the container with the `–v` option, where the value consists of fields
    separated by a colon, `:`. The first field is the name of the volume, while the
    second is the path where the file or directory is mounted in the container. The
    next interesting option is the ability to limit the maximum RAM allocated for
    the container using the `–m` option. The following are the commands that create
    new volumes and mount them to the launched container. The maximum amount of RAM
    is set to 500 MB. The container is automatically removed after stopping using
    the activated option `--rm`, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Every running container can be stopped using the `docker stop` command. We
    have already set a name for our container so we can easily use it as a label,
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The entire state of the container is written to the disk, so we may run it
    again with exactly the same set of data as we did before stopping, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you only want to restart a container, you can use the following command
    instead of stopping/starting container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Listing and removing containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you have started some containers, you may want to consider displaying a
    list of all the running containers on your Docker machine. The `docker ps` command
    should be used for that. This command displays some basic information about the
    container, such as a list of exposed ports and the name of the source image. This
    command prints only the currently started containers. If you would like to see
    containers that have been stopped or are inactive, use option `-a` on the Docker
    command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cce5a0a7-6103-4def-af56-718b3cd98812.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If a container is no longer needed, it can be removed using the `docker rm`
    command. Sometimes it is necessary that you remove a running container, which
    is not allowed by default. To force this option, set the `-f` option on Docker
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You should remember that the `docker ps` command removes only the container.
    The image from which it has been created is still cached locally. Such images
    can take up a significant amount of space, ranging from a megabyte to several
    hundred megabytes. You may remove every image by using the `docker rmi` command
    with the image ID or name as a parameter, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We haven’t created any Docker images yet, but it''s not unusual to generate
    a large amount of unwanted or unnamed images during image creation. These images
    can be easily recognized, as they are denoted with a name of `<none>`. In Docker
    nomenclature, these are called **dangling images** and can be easily removed with
    the following command. The list of all currently cached images can be displayed
    with the `docker images` command, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Pulling and pushing images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve already discussed Docker Hub. It is the biggest and most popular Docker
    repository available on the web. It is available at [https://hub.docker.com](https://hub.docker.com).
    The Docker client, by default, tries to pull all the images for that repository.
    There are many certified official images for common software such as Redis, Java,
    Nginx, or Mongo, but you may also find hundreds of thousands of images created
    by other people as well. If you use the command `docker run` , the image is pulled
    from the repository in case it is not cached locally. You may also run the following
    command `docker pull`, which is only responsible for downloading an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command downloads the newest version of an  image (with the latest
    tag's name). If you would like to use an older version of a Postgres Docker image,
    you should append the tag with the specific version's number. The full list of
    available versions is usually published on the image's site, and is no different
    in this case. Visit [https://hub.docker.com/r/library/postgres/tags/](https://clicktime.symantec.com/a/1/Im1LdWl8NQ4ddISjfwL_OxcUojdkW-H3fP-oquj1vZs=?d=zKV7R9H5uhYC7J5kAN4WlSdYuV7w56mec0MwOxbVt-onFGmsM6Sx37HIaVHJUb3QiEeB2UoRmfzGJLL2nbKFa0anD4Lnn9-ximh393HGo36BjpeP0FoTIe_ikOi5QeJ1AeoMYVgQp_eESUZZNBRlDtcfYxSSkGpgZ_sGge1ts1DBD0AiZXddlCKygZL3ttJma9imoX-dIYGhyIi7l13N-8Y_5N5OYuthQeHXR4cE3e6ZicVVMyrnPGOm4nPLOHZiFzLZsTnDT0QQgFKRuqd4dsZekUaglgG9Y90wlN16gLc1DewmmCqRs_KiE1hwsBfCnFIku3QSPBvVa8e7YWJmMEGwuCxlybf2ywMx81HkC4uMHvQfq1EiVA0PYg5arA%3D%3D&u=https%3A%2F%2Fhub.docker.com%2Fr%2Flibrary%2Fpostgres%2Ftags%2F) for
    a list of the available tags.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have run and validated your image, you should think about saving it
    remotely. The most appropriate place for it is, of course, Docker Hub. However,
    sometimes you might want to store images in alternative storage, such as a private
    repository. Before pushing an image, you have to tag it with your registry username,
    image name, and its version number. The following command creates a new image
    from a Postgres source image with the name `piomin/postgres` and the `1.0` version
    tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if you run the `docker images` command you will see two images with the
    same ID. The first has the name Postgres and the latest tag, while the second
    has the name `piomin/postgres` and the tag `1.0`. What is important is that `piomin`
    is my username on Docker Hub. So, before proceeding any further we should first
    register the image there. After, we should also log in to our Docker client using
    the `docker login` command. Here, you will be prompted for a username, password,
    and the email address you used for registration. Finally, you can push a tagged
    image with the following `docker push` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now all that''s left to do is log in to your Docker Hub account using a web
    browser to check if the pushed image has appeared. If everything worked correctly,
    you will see a new public repository with your image on-site. The following screenshot
    shows the image currently pushed to my Docker Hub account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ab7815a-c6bc-47e4-bc66-9c939ad920e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Building an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we pushed the copy of the Postgres Docker image to
    a Docker Hub registry. Usually, we push our own images created from the file  `Dockerfile`,
    which defines all the instructions required when installing and configuring software
    on the container. The details related to the structure of `Dockerfile` will be
    discussed later. What is important for now, though, is the command used for building
    a Docker image,  `docker build`. This command should be run in the same directory
    where `Dockerfile` is located. When building a new image it is recommended to
    set its name and tag using the `-t` option. The following command creates the
    image `piomin/order-service` , tagged with a `1.0` version. The image may be pushed
    to your Docker Hub account in the same way as the previous image was with Postgres,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Networking is an important aspect of Docker architecture since we often have
    to provide communication between applications running on different containers.
    A common use case may be a web application that needs access to a database. We''re
    now going to refer to another example that has already been introduced in [Chapter
    11](554c4049-1dc9-430d-8fe7-19f3b9ac99a3.xhtml), *Message Driven Microservices*.
    It is communication between Apache Kafka and ZooKeeper. Kafka requires ZooKeeper
    because it stores a variety of configuration as a key/value pair in the ZK data
    tree and uses it across the cluster. As you may remember, we first had to create
    a custom network and run those two containers there. The following command is
    used to create a user-defined network on a Docker host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After the previous command has finished running, you can check out the list
    of available networks using the following command. By default, Docker creates
    three networks for you, so you should see four networks with the names bridge,
    host, none, and `kafka-network`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to pass the network name to the container created with the `docker
    run` command. It can be achieved through the `--network` parameter, as you can
    see in the following example. If you set the same network''s name for two different
    containers, they will be started on the same network. Let''s analyze what this
    means in practice. If you were inside one container, you could call it its name
    instead of using its IP address, which is why we could have set the environment
    variable `ZOOKEEPER_IP` to ZooKeeper when starting a container with Apache Kafka.
    Kafka, which starts inside this container, connects the ZooKeeper instance on
    the default port as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Creating a Docker image with microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already discussed the basic Docker commands that are available for running,
    creating, and managing containers. It's now time to create and build our first
    Docker image that starts the sample microservice that we introduced in the previous
    chapter. For that, we should move back to the repository available at the address
    [https://github.com/piomin/sample-spring-cloud-comm.git](https://github.com/piomin/sample-spring-cloud-comm.git)
    and then switch to the branch `feign_with_discovery`  on [https://github.com/piomin/sample-spring-cloud-comm/tree/feign_with_discovery](https://github.com/piomin/sample-spring-cloud-comm/tree/feign_with_discovery).
    There, you will find a `Dockerfile` for every single microservice, gateway, and
    discovery. Before discussing these examples however we should refer to the `Dockerfile`
    reference to understand the basic commands that we can place there. In fact, `Dockerfile`
    is not the only way to build Docker images; we're also going to show you how to
    create an image with a microservice using the Maven plugin.
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfiles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker can build images automatically by reading the instructions provided
    in a `Dockerfile`,  a document that contains all the commands that are invoked
    on the command line to assemble an image. All of those commands have to be preceded
    by the keywords defined in the `Dockerfile` specification. The following is a
    list of the most commonly used instructions. They are executed in the order in
    which they are found in the `Dockerfile`. Here, we can also append some comments
    that have to be followed by the `#` character:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Instruction** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `FROM` | This  initializes a new build stage and sets the base image for
    subsequent instructions. In fact, every valid `Dockerfile` has to start with a
    `FROM` instruction. |'
  prefs: []
  type: TYPE_TB
- en: '| `MAINTAINER` | This sets author identities of the generated images. This
    instruction is deprecated, so you may find it in many older images. We should
    use the `LABEL` instruction instead of `MAINTAINER` , as follows:`LABEL maintainer="piotr.minkowski@gmail.com"`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `RUN` | This executes Linux commands for configuring and installing the required
    software in a new layer on top of the current image and then commits the results.
    It can have two forms:`RUN <command>` or `RUN ["executable", "param1", "param2"]`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ENTRYPOINT` | This configures a final script that is used when bootstrapping
    the container that will run as an executable. It overrides all elements specified
    using `CMD` and has two forms: `ENTRYPOINT ["executable", "param1", "param2"]`
    and `ENTRYPOINT` the command `param1 param2`. It is worth noticing that only the
    last `ENTRYPOINT` instruction in the `Dockerfile` will have an affect. |'
  prefs: []
  type: TYPE_TB
- en: '| `CMD` | `Dockerfile` can contain only one `CMD` instruction. This instruction
    provides the default arguments to  `ENTRYPOINT` using a JSON array format. |'
  prefs: []
  type: TYPE_TB
- en: '| `ENV` | This sets the environment variable for a container in key/value form.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `COPY` | This copies new files or directories from a given source path to
    the filesystem inside the container at the path defined by the target path. It
    has the following form: `COPY [--chown=<user>:<group>] <src>... <dest>`. |'
  prefs: []
  type: TYPE_TB
- en: '| `ADD` | This is an alternative to a `COPY` instruction. It is allowed to
    do a little more than `COPY`, for example, it allows `<src>` to be a URL address.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `WORKDIR` | This sets the working directory for `RUN`, `CMD`, `ENTRYPOINT`,
    `COPY`, and `ADD.` |'
  prefs: []
  type: TYPE_TB
- en: '| `EXPOSE` | This is responsible for informing Docker that the container listens
    on the specified network ports at runtime. It does not actually publish the port.
    The ports are published through the `-p` option on the `docker run` command. |'
  prefs: []
  type: TYPE_TB
- en: '| `VOLUME` | This creates a mount point with the specified name. Volumes are
    the preferred mechanism for persisting data inside Docker containers. |'
  prefs: []
  type: TYPE_TB
- en: '| `USER` | This sets the username and, optionally, the user group used when
    running the image, as well as for the `RUN`, `CMD`, and `ENTRYPOINT` instructions.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s take a look how this works in practice. We should define a `Dockerfile`
    for every microservice and place it in the root directory of its Git project.
    The following is a `Dockerfile` created for `account-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding example is not very complicated. It only adds the microservice-generated
    fat JAR file to the Docker container and uses the `java -jar` command as `ENTRYPOINT`.
    Even so, let''s analyze it step-by-step. Our example `Dockerfile` performs the
    following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: The image extends an existing OpenJDK image that is an official, open-source
    implementation of the Java Platform Standard Edition. OpenJDK images come in many
    flavors. The main difference between available images' variants is in their size.
    The image tagged with `8u151-jdk-slim-stretch` provides JDK 8 and includes all
    the libraries needed to run the Spring Boot microservice. It is also much smaller
    than a basic image with this version of Java (`8u151-jdk`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we defined two environment variables that can be overridden during runtime
    that have the `-e` option of the `docker run` command. The first is the active
    Spring profile name, which is by default initialized with a `zone1` value. The
    second is the discovery server's address, which is by default equal to [http://localhost:8761/eureka/](http://localhost:8761/eureka/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fat JAR file contains all the required dependencies together with an application's
    binaries. So, we have to put a generated JAR file inside the container using the
    `ADD` instruction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We configure our container to run as an executable Java application. The defined
    `ENTRYPOINT` is equivalent to running the following command on a local machine:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Using the `EXPOSE` instruction we have informed Docker that it may expose our
    application's HTTP API, which is available inside the container on port `8091`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running containerized microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assuming we have prepared a valid `Dockerfile` for each service, the next step
    is to build the whole Maven project with the `mvn clean install` command, before
    building a Docker image for every service.
  prefs: []
  type: TYPE_NORMAL
- en: 'When building a Docker image, you should always be in the `root` directory
    of every microservice source code. The first container that has to be run in our
    microservices-based system is a discovery server. Its Docker image has been named
    `piomin/discovery-service`. Before running Docker''s `build` command, go to the
    module `discovery-service`. This `Dockerfile` is a little simpler than other microservices,
    because there is no environment variables to set inside the container, shown as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'There are only five steps to perform here, which you can see in the logs generated
    during the target image''s build, just after running the `docker build` command.
    If everything works correctly, you should see the progress of all five steps as
    defined in `Dockerfile` and the following final messages telling you that the
    image has been successfully built and tagged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have successfully built an image, we should run it. We recommend creating
    a network where all the containers with our microservices will be launched. To
    launch a container inside a newly created network, we have to pass its name to
    the `docker run` command using the `--network` parameter. In order to check if
    a container has been successfully started, run the `docker logs` command. This
    command prints all the lines logged by the application to the console, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to build and run the containers with our four microservices—`account-service`,
    `customer-service`, `order-service`, and `product-service`. The procedure is the
    same for each service. For example, if you would like to build `account-service`,
    first go to that directory within the example project''s source code. The `build`
    command is the same here as it is for the discovery service; the only difference
    is in the image name, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The command to run the Docker image is a little more complicated for `discovery-service`.
    In this case, we have to pass the address of the Eureka server to the starting
    container. Because this container is running in the same network as the discovery
    service container, we may use its name instead of its IP address or any other
    identifier. Optionally, we can also set the container''s memory limit by using
    the `-m` parameter, for example, to 256 MB. Finally, we can see the logs generated
    by the application running on the container by using the `docker logs` command
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The same steps as described previously should be repeated for all other microservices.
    The final result is the five running containers that can be displayed using the `docker
    ps` command, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2638f3f5-f1ee-446e-8090-25e5c1622119.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All the microservices are registered in the Eureka server. The Eureka dashboard
    is available at the address `http://192.168.99.100:8761/`, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da00480d-f780-4d73-a76d-7f1055e5e176.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There is one more interesting Docker command that we mention here: `docker
    stats`. This command prints some statistics related to the started container,
    such as memory or CPU usage. If you use the  `--format` parameter of that command
    you can customize the way it prints the statistics; for example, you can print
    the container name rather than its ID. Before running that command you may perform
    some tests in order to check that everything is working as it should. It''s worth
    checking whether the communication between microservices that was started on the
    containers has finished successfully. You may also want to try to call the endpoint
    `GET /withAccounts/{id}` from `customer-service` , which calls an endpoint exposed
    by `account-service`. We run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is visible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4eb480b3-3149-4c73-bdc3-7885ad46bf71.png)'
  prefs: []
  type: TYPE_IMG
- en: Building an image using the Maven plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we''ve mentioned previously, `Dockerfile` is not the only way of creating
    and building containers. There are some other approaches available, for example,
    by using Maven plugin. We have many available plugins dedicated to building images,
    which are used with  `mvn` commands. One of the more popular among them is `com.spotify:docker-maven-plugin`.
    This has the equivalent tags in its configuration that can be used instead of
    `Dockerfile` instructions. The configuration of the plugin inside `pom.xml` for
    `account-service `is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This plugin can be invoked during Maven''s `build` command. If you would like
    to build a Docker image just after building the application, use the following
    Maven command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can also set the `dockerDirectory` tag in order to perform
    a build based on `Dockerfile`. No matter which method you choose, the effect is
    the same. Any new image that is built with an application will be available on
    your Docker machine. When using `docker-maven-plugin`, you can force the automated
    image to push to the repository by setting `pushImage` to `true`, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Advanced Docker images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until now, we have built rather simple Docker images. However, it is sometimes
    necessary to create a more advanced image. We will need such an image for the
    purpose of Continuous Delivery presentation. This Docker image will be run as
    a Jenkins slave and would be connected to the Jenkins master, which is started
    as a Docker container. We have not found such an image on Docker Hub, so we created
    in by ourselves. Here, the image has to contain Git, Maven, JDK8, and Docker.
    These are all the tools required for building our example microservices using
    the Jenkins slave. I will give you a brief summary of the basics related to Continuous
    Delivery using the Jenkins server in a later section of this chapter. For now,
    we will focus on just building the required image. The following is the full definition
    of the image provided inside `Dockerfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s analyze what''s happened. Here, we have extended the Docker base image.
    This is a pretty smart solution, because that image now provides Docker inside
    Docker. Although running Docker inside Docker is generally not recommended, there
    are some desirable use cases, such as Continuous Delivery with Docker. Besides
    Docker, there is other software installed on the image using the `RUN` instruction,
    such as Git, JDK, Maven, or Curl. We have also added an OS user, which has `sudoers`
    permission in the `dockerd` script, which is responsible for running the Docker
    daemon on the machine. This is not the only process that has to be started in
    the running container; launching JAR with the Jenkins slave is also required.
    Those two commands are executed inside `entrypoint.sh`, which is set as an `ENTRYPOINT`
    of the image. The full source code of this Docker image is available on GitHub
    at [https://github.com/piomin/jenkins-slave-dind-jnlp.git](https://github.com/piomin/jenkins-slave-dind-jnlp.git).
    You can omit building it from source code and just download a ready image from
    my Docker Hub account by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the script `entrypoint.sh` inside Docker image that starts Docker deamon
    and Jenkins slave:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Continuous Delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key benefits of migrating to microservice-based architecture is the
    ability to deliver software quickly. This should be the main motivation for implementing
    continuous delivery or a continuous deployment process in your organization. In
    short, the continuous delivery process is an approach that tries to automate all
    the stages of software delivery such as building, testing a code, and releasing
    an application. There are many tools that empower that process. One of them is
    Jenkins, an open source automation server written in Java. Docker is something
    that can take your **Continuous Integration** (**CI**) or **Continuous Delivery** (**CD**) processes
    to a higher level. Immutable delivery, for example, is one of the most important
    advantages of Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Jenkins with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main goal here is to design and run the continuous delivery process locally using
    Jenkins and Docker. There are four elements that have a part in this process.
    The first of them is already prepared: the source code repository of our microservices,
    which is available on GitHub. The second element, Jenkins, needs to be run and
    configured. Jenkins is a key element of our continuous delivery system. It has
    to download the application''s source code from the GitHub repository, build it,
    and then place the resulting JAR file in Docker image, push that image to Docker
    Hub, and finally run the container with a microservice. All of the tasks within
    this process are directly performed on a Jenkins master but on its slave node.
    Both Jenkins and its slave are launched as Docker containers. The architecture
    of this solution is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f069db09-864d-4eb4-b61a-e56760ded97d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s worth mentioning that Jenkins is built on the basis of the concept of
    plugins. The core is too simple an engine for automated builds. The real power
    of Jenkins is in its plugins, and there are hundreds of them in the Update Center.
    For now, we will only discuss a few opportunities available to us thanks to the
    Jenkins server. We will need the following plugins installed to be able to build
    and run our microservices in Docker containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline**: This is a suite of plugins that lets you create automation using
    Groovy scripts following the idea **Pipeline as code** ([https://wiki.jenkins.io/display/JENKINS/Pipeline+Plugin](https://clicktime.symantec.com/a/1/4g9YbrLxE43FYJrIE5v0J-RjoqlfXZm5h2piohXV60o=?d=GiSMteljxw-3ox0rf3cMazK9IOHzeSrn0vm9sus4y_n0hehkoAHvPijqT9dNXanC2Z3KtWbAm0BF-YDyp2HFvxXpFa6IkS_tvoddqdWrcb2R6vx-7YEpFHbt4IzErozigZnPecmyLha58i_mX_GOqw8nGcIkFmptcNTdFqB6DA-shedWhYxMv5VpzsTWPmDZA52S7fjMHuYvrTP5MOqqgejXYWvZr4d9OaWe0jeXJ-MEIccIx-UiD_tYy9OK2eYpd4eiaegTQb9XhbUR0ZNPGlpo4vSShb3yAI2Kf9JPcQ4hOSXoj5JpZSvnKhm1C9Yn68IsYCIBmwjYZZYyuS3y9uUI9zHbgSpVOx8ehvCmMWx0MAwCJ5gDR1ZIXXNcnw%3D%3D&u=https%3A%2F%2Fwiki.jenkins.io%2Fdisplay%2FJENKINS%2FPipeline%2BPlugin))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Pipeline**: This allows you to build Docker containers in pipelines
    ([https://wiki.jenkins.io/display/JENKINS/Docker+Pipeline+Plugin](https://clicktime.symantec.com/a/1/3BcsCubSP1UZ0ssSZFCe2iSCQQ_b1asMBhlt_0nQFKI=?d=GiSMteljxw-3ox0rf3cMazK9IOHzeSrn0vm9sus4y_n0hehkoAHvPijqT9dNXanC2Z3KtWbAm0BF-YDyp2HFvxXpFa6IkS_tvoddqdWrcb2R6vx-7YEpFHbt4IzErozigZnPecmyLha58i_mX_GOqw8nGcIkFmptcNTdFqB6DA-shedWhYxMv5VpzsTWPmDZA52S7fjMHuYvrTP5MOqqgejXYWvZr4d9OaWe0jeXJ-MEIccIx-UiD_tYy9OK2eYpd4eiaegTQb9XhbUR0ZNPGlpo4vSShb3yAI2Kf9JPcQ4hOSXoj5JpZSvnKhm1C9Yn68IsYCIBmwjYZZYyuS3y9uUI9zHbgSpVOx8ehvCmMWx0MAwCJ5gDR1ZIXXNcnw%3D%3D&u=https%3A%2F%2Fwiki.jenkins.io%2Fdisplay%2FJENKINS%2FDocker%2BPipeline%2BPlugin))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Git**: This integrates Git with Jenkins ([https://wiki.jenkins.io/display/JENKINS/Git+Plugin](https://clicktime.symantec.com/a/1/Zbv8hM_2L26s_PMbntThO-9W_A4uUxsqo7UyU5nbae8=?d=GiSMteljxw-3ox0rf3cMazK9IOHzeSrn0vm9sus4y_n0hehkoAHvPijqT9dNXanC2Z3KtWbAm0BF-YDyp2HFvxXpFa6IkS_tvoddqdWrcb2R6vx-7YEpFHbt4IzErozigZnPecmyLha58i_mX_GOqw8nGcIkFmptcNTdFqB6DA-shedWhYxMv5VpzsTWPmDZA52S7fjMHuYvrTP5MOqqgejXYWvZr4d9OaWe0jeXJ-MEIccIx-UiD_tYy9OK2eYpd4eiaegTQb9XhbUR0ZNPGlpo4vSShb3yAI2Kf9JPcQ4hOSXoj5JpZSvnKhm1C9Yn68IsYCIBmwjYZZYyuS3y9uUI9zHbgSpVOx8ehvCmMWx0MAwCJ5gDR1ZIXXNcnw%3D%3D&u=https%3A%2F%2Fwiki.jenkins.io%2Fdisplay%2FJENKINS%2FGit%2BPlugin))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maven integration**: This provides some useful commands when building an
    application with Maven and Jenkins ([https://plugins.jenkins.io/maven-plugin](https://clicktime.symantec.com/a/1/jmIwLdZZ-wtodkRm1Goje_nuKFV98VcZYPHn5cWj1KM=?d=GiSMteljxw-3ox0rf3cMazK9IOHzeSrn0vm9sus4y_n0hehkoAHvPijqT9dNXanC2Z3KtWbAm0BF-YDyp2HFvxXpFa6IkS_tvoddqdWrcb2R6vx-7YEpFHbt4IzErozigZnPecmyLha58i_mX_GOqw8nGcIkFmptcNTdFqB6DA-shedWhYxMv5VpzsTWPmDZA52S7fjMHuYvrTP5MOqqgejXYWvZr4d9OaWe0jeXJ-MEIccIx-UiD_tYy9OK2eYpd4eiaegTQb9XhbUR0ZNPGlpo4vSShb3yAI2Kf9JPcQ4hOSXoj5JpZSvnKhm1C9Yn68IsYCIBmwjYZZYyuS3y9uUI9zHbgSpVOx8ehvCmMWx0MAwCJ5gDR1ZIXXNcnw%3D%3D&u=https%3A%2F%2Fplugins.jenkins.io%2Fmaven-plugin))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The required plugins can be configured using the UI dashboard, either after
    startup or via Manage Jenkins *|* Manage Plugins. To run Jenkins locally, we will
    use its Docker image. The following commands create the network called `jenkins`
    and start the Jenkins master container, exposing the UI dashboard on port `38080`.
    Notice that when you start the Jenkins container and use its web console for the
    first time you need to set it up using the initial generated password. You can
    easily retrieve this password from Jenkins logs by invoking the `docker logs jenkins`
    command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have successfully configured the Jenkins master with its required plugins,
    we need to add new slaves'' nodes. To do this, you should go to the section Manage
    Jenkins *|* Manage Nodes and then select New Node. In the displayed form, you
    have to set `/home/jenkins` as a remote root directory, and the launch agent via
    Java Web Start as the launch method. Now you may start the Docker container with
    a Jenkins slave, as previously discussed. Note that you will have to override
    two environment variables indicating the slave''s name and secret. The `name`
    parameter is set during node creation, while the secret is automatically generated
    by the server. You can take a look at the node''s details page for more information,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a171112c-7bfe-4bd0-a491-1746adc35d68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the Docker command that starts a container with the Jenkins
    slave with Docker in Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This short introduction to the configuration of Jenkins should help you to repeat
    the discussed continuous delivery process on your own machine. Remember that we
    have only looked at a few aspects related to Jenkins, including settings, which
    will allow you to set up a CI or CD environment for your own microservices-based
    system. If you are interested in pursuing this topic in greater depth, you should
    refer to the documentation available at [https://jenkins.io/doc](https://jenkins.io/doc).
  prefs: []
  type: TYPE_NORMAL
- en: Building pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In older versions of Jenkins server, the basic unit of work was a job. Currently,
    its main feature is the ability to define pipelines as code. This change is related
    to more modern trends in IT architecture that consider application delivery as
    critical as the application that's being delivered. Since all the components of
    the application stack are already automated and represented as code in the version
    control system, the same benefits can be leveraged for CI or CD pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Jenkins Pipeline provides a set of tools designed for modeling simple and
    more advanced delivery pipelines as code. The definition of such a pipeline is
    typically written into a text file called a `Jenkinsfile`. It supports the domain-specific
    language with additional, specific steps available through the *Shared Libraries* feature. Pipeline
    supports two syntaxes: Declarative (introduced in Pipeline 2.5) and Scripted Pipeline.
    No matter which syntax is used, it will be logically divided into stages and steps.
    Steps are the most fundamental part of a pipeline as they tell Jenkins what to
    do. Stages logically group a couple of steps, which are then displayed on the
    pipeline''s result screen. The following code is an example of a scripted pipeline
    and defines a build process for `account-service`. Similar definitions have to
    be created for other microservices. All of these definitions are located in the
    `root` directory of every application''s source code as `Jenkinsfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The previous definition is divided into four stages. In the first, `Checkout`,
    we clone the Git repository with the source code of all the example applications.
    In the second stage, `Build` , we build an application from the `account-service`
    module and then read the whole Maven project's version number from `root`'s `pom.xml`.
    In the `Image` stage we build an image from `Dockerfile` and push it to the Docker
    repository. Finally, we run a container with the `account-service` application
    inside the `Run` stage. All the described stages are executed on `dind-node-1`
    following the definition of a node element, which is a root for all the other
    elements in the pipeline definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can proceed to defining the pipeline in Jenkins'' web console. Select
    New Item, then check the Pipeline item type and enter its name. After confirmation
    you should be redirected to the pipeline''s configuration page. The only thing
    you have to do once there is to provide the location of `Jenkinsfile` in the Git
    repository and then set the SCM authentication credentials as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de3445e9-d691-4ca5-8344-ac9a2cd9cc06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After saving the changes, the configuration of the pipeline is ready. In order
    to start the build, click the Build Now button. There are two things that should
    be clarified at this stage. In the production mode you can use the `webhook` mechanism,
    which is provided by the most popular Git host vendors, including GitHub, BitBucket,
    and GitLab. This mechanism automatically triggers your build on Jenkins after
    pushing the changes to the repository. In order to demonstrate this, we would
    have to run the version control system locally with Docker, for example using
    GitLab. There is also another simplified way of testing. The containerized application
    is run directly on Jenkins'' Docker in Docker slave; under normal circumstances,
    we would launch on the separated remote machine dedicated only to the deployment
    of applications. The following screenshot is Jenkins'' web console illustrating
    the build process, divided into different stages, for `product-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/389420f0-9f82-4ff8-94f3-50dc772ae4c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We should now create one pipeline per microservice. The list of all the created
    pipelines is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f01811b-9091-425b-90cf-e19bdbffef91.png)'
  prefs: []
  type: TYPE_IMG
- en: Working with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already launched our example microservices on Docker containers. We
    have even used CI and CD automated pipelines in order to run them on the local
    machine. You may, however, be asking an important question. How can we organize
    our environment on a larger scale and in production mode where we have to run
    multiple containers across multiple machines? Well, this is exactly what we have
    to do when implementing microservices in accordance with the idea of cloud native
    development. It turns out that many challenges still remain in this instance.
    Assuming that we have many microservices launched in multiple instances, there
    will be plenty of containers to manage. Doing things such as starting the correct
    containers at the correct time, handling storage considerations, scaling up or
    down, and dealing with failures manually would be a nightmare. Fortunately, there
    are some platforms available that help in clustering and orchestrating Docker
    containers at scale. Currently, the leader in this field is Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is an open-source platform for managing containerized workloads and
    services. It can act as a container platform, as a microservices platform, as
    a cloud platform, and a lot more. It automates such actions as running containers
    across different machines, scaling up and down, distributing load between containers,
    and keeping storage consistency between multiple instances of an application.
    It also has a number of additional features, including service discovery, load
    balancing, configuration management, service naming, and rolling updates. Not
    all of these features would be useful for us however as many similar features
    are provided by Spring Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that Kubernetes is not the only container management
    tool out there. There is also Docker Swarm, the native tool provided within Docker.
    However, since Docker has announced native support for Kubernetes, it seems to
    be a natural choice. There are several important concepts and components regarding
    Kubernetes that we should know before we move on to any practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: Concepts and components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first term you will probably have to deal with when using Kubernetes is
    pod, which is a basic building block in Kubernetes. A pod represents a running
    process in the cluster. It can consist of one or more containers that are guaranteed
    to be co-located on the host machine and will share the same resources. One container
    per pod is the most common Kubernetes use case. Each pod has a unique IP address
    within the cluster but all containers deployed inside the same pod can communicate
    with others via `localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: Another common component is a service. A service logically groups a set of pods
    and defines a policy of access to it; it is sometimes called a microservice. By
    default, a service is exposed inside a cluster but it can also be exposed onto
    an external  IP address. We can expose a service using one of the four available
    behaviors: `ClusterIP`, `NodePort`, `LoadBalancer` , and `ExternalName`. The default option
    is `ClusterIP`. This exposes the service on a cluster-internal IP, which makes
    it reachable only from within the cluster. `NodePort` exposes the service on each
    Node's IP at a static port, and automatically create `ClusterIP` for exposing
    service inside a cluster. In turn, `LoadBalancer` exposes the service externally
    using a cloud provider’s load balancer, and `ExternalName` maps the service to
    the contents of the `externalName` field. We should also take a few moments to
    discuss Kubernetes's replication controller. This handles replication and scaling
    by running a specified number of copies of a pod across the cluster. It is also
    responsible for replacing pods if the underlying node fails. Every controller
    in Kubernetes is a separate process run by `kube-controller-manager`. You can
    also find node controller, endpoints controller, and service account and token
    controllers in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes uses an `etcd` key/value store as a backing store for all cluster
    data. Inside every node of the cluster is an agent called **kubelet**, which is
    responsible for ensuring that containers are running in a pod. Every command sent
    to Kubernetes by a user is processed by Kubernetes API exposed by `kubeapi-server`.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this is a really simplified explanation of Kubernetes's architecture.
    There are more components and tools available that have to be configured properly
    in order to run highly available Kubernetes clusters successfully. This is not
    a trivial task to perform, and it requires a significant amount of knowledge about
    this platform. Fortunately, there is a tool out there that makes it easy to run
    a Kubernetes cluster locally—Minikube.
  prefs: []
  type: TYPE_NORMAL
- en: Running Kubernetes locally via Minikube
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Minikube is a tool that makes it easy to run Kubernetes locally. It runs a single-node
    Kubernetes cluster inside a VM on the local machine. It is definitely the most
    suitable choice in development mode. Of course, Minikube does not support all
    of the features provided by Kubernetes; only the most important ones, including
    DNS, NodePorts, Config Map, Dashboard, and Ingress.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run Minikube on Windows, we need to have a virtualization tool installed.
    However, if you have already run Docker, you will have probably installed Oracle
    VM VirtualBox. In this case, you don''t have to do anything other than download
    and install the latest release of Minikube, which you can check at [https://github.com/kubernetes/minikube/releases](https://clicktime.symantec.com/a/1/eXr_fIrvCIRYzEHt0YvbtkptTqcVd9nJzBV28fxoaTY=?d=7tChM-hIl54SsiVoHKrovXbmLIi8ouu38bfWFa5LjYebKneJvW_c2_HMgDdoq431rSiEnNRRoWc7WI40qLP-zxO_svn7BtB5YkP7_3z6XE1bc9UDw_gg4B_LUQLmxfklfTjgbs0J-dnBHLc3GOsVYjvBMyOE-nmJR1SuKthIzdMfxP8oasaAGIamKBmwy-pKxDOZYKGzKE4iEAO1nFo15LHQ7enPYrMhvcEhb3LDIMsYYwnwVTe52q36t77MaAeAFdq7DgkU1BLlVMydfq9vglCYhLnhnOOzSDesZnjGR3spuBjVhNyCD3pcc73yC-ARPXPUpScKDxqUYA8pZg40QrbDOyzuC95KNm-9vIqcPXR6iDgu8QK_SscvFxnDi4A%3D&u=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fminikube%2Freleases) ,
    and `kubectl.exe` , as described at [https://storage.googleapis.com/kubernetes-release/release/stable.txt](https://storage.googleapis.com/kubernetes-release/release/stable.txt).
    Both files `minikube.exe` and `kubectl.exe` should be included in the `PATH` environment
    variable. In addition, Minikube provides its own installer, `minikube-installer.exe` ,
    which will automatically add `minikube.exe` to your path. You may then start Minikube
    from your command line by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command initializes a `kubectl` context called `minikube`. It
    contains the configuration that allows you to communicate with the Minikube cluster.
    You can now use `kubectl` commands in order to maintain your local cluster created
    by Minikube and deploy your containers there. An alternative solution to a command-line
    interface is Kubernetes dashboard. Kubernetes dashboard can be enabled for your
    node by calling `minikube` dashboard. You can create, update, or delete deployment
    using this dashboard, as well as list and view a configuration of all pods, services,
    ingresses, and replication controllers. It is possible to easily stop and remove
    a local cluster by invoking the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Deploying an application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every configuration existing on a Kubernetes cluster is represented by Kubernetes
    objects. These objects can be managed through the Kubernetes API and should be
    expressed in a YAML format. You may use that API directly, but will probably decide
    to leverage the `kubectl` command-line interface to make all the necessary calls
    for you. The description of a newly created object in Kubernetes has to provide
    specification that describes its desired state, as well as some basic information
    about the object. The following are some required fields in the YAML configuration
    file that should always be set:'
  prefs: []
  type: TYPE_NORMAL
- en: '`apiVersion`: This indicates the version of the Kubernetes API used to create
    an object. An API always requires the JSON format in a request but `kubectl` automatically
    converts YAML input into JSON.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kind`: This sets the kind of object to create. There are some predefined types
    available such as Deployment, Service, Ingress, or ConfigMap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: This allows you to identify the object by name, UID or, optional
    namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec`: This is the proper definition of an object. The precise format of a
    specification depends on an object''s kind and contains nested fields specific
    to that object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usually, when creating new objects on Kubernetes, its `kind` is deployment.
    In the `Deployment` YAML file, shown as follows, there are two important fields
    set. The first of, `replicas`, specifies the number of desired pods. In practice,
    this means that we run two instances of the containerized application. The second,
    `spec.template.spec.containers.image`, sets the name and version of the Docker
    image that will be launched inside a pod. The container will be exposed on port
    `8090`, on which `order-service` listens for HTTP connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming the preceding code is stored in the file `order-deployment.yaml`,
    we can now deploy our containerized application on Kubernetes using imperative
    management as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can perform the same action based on the declarative management
    approach, illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have to create the same deployment file for all the microservices and
    `discovery-service`. The subject of `discovery-service` is a very curious matter.
    We have the option to use built-in Kubernetes discovery based on pods and services,
    but our main goal here is to deploy and run Spring Cloud components on that platform.
    So, before deploying any microservices, we should first deploy, run, and expose
    Eureka on Kubernetes. The following is a deployment file of `discovery-service`
    that can also can be applied to Kubernetes by calling the `kubectl apply` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'If you create a Deployment, Kubernetes automatically creates pods for you.
    Their number is equal to the value set in the `replicas` field. A pod is not able
    to expose the API provided by the application deployed on the container, it just
    represents a running process on your cluster. To access the API provided by the
    microservices running inside pods, we have to define a service. Let''s remind
    ourselves what a service is. A service is an abstraction that defines a logical
    set of pods and a policy by which to access them. The set of pods targeted by
    a service is usually determined by a label selector. There are four types of service
    available in Kubernetes. The simplest and default one is `ClusterIP`, which exposes
    a service internally. If you would like to access a service from outside the cluster,
    you should define the type `NodePort`.  This option has been set out in the following
    example YAML file; now, all the microservices can communicate with Eureka using
    its Kubernetes service name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In fact, all of our microservices deployed on Minikube should be available outside
    the cluster, as we would like to access the API exposed by them. To do this, you
    need to provide the similar YAML configuration to that in the preceding example,
    changing only the service's name, labels and port.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is only one last component that should be present in our architecture:
    API Gateway. We could deploy a container with the Zuul proxy, however we need
    to introduce the popular Kubernetes object, Ingress. This component is responsible for
    managing external access to services that are typically exposed via HTTP. Ingress
    provides load balancing, SSL termination, and name-based virtual hosting. The
    Ingress configuration YAML file is shown as follows; note that all the services
    can be accessed on the same port, `80`, on different URL paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Maintaining a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Maintaining a Kubernetes cluster is rather complex. In this section, we will
    show you how to use some basic commands and the UI dashboard in order to view
    the object currently existing on the cluster. Let''s first list the elements that
    have been created for the purpose of running our example microservices-based system.
    First, we display a list of deployments by running the command `kubectl get deployments`,
    which should result in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e7394a7-2440-45a6-abdb-b4bb42eec586.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One deployment can create a number of pods. You can check the list of pods
    by calling the `kubectl get pods` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3603bc5a-2cef-41ac-a82e-3b2a96cfbeeb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The same list can be viewed using the UI dashboard. You can view these details
    by clicking on the selected row, or check out the container logs by clicking the
    icon available on the right-hand side of each row, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82fecacc-7077-472a-b216-14d941e30e55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The full list of available services can be displayed using the command `kubectl
    get services`. There are some interesting fields here, including one that indicates
    an IP address on which a service is available inside a cluster (CLUSTER-IP), and
    a pair of ports (PORT(S)) on which services are exposed internally and externally.
    We can also call the HTTP API exposed on `account-service` at the address `http://192.168.99.100:31099`,
    or the Eureka UI dashboard at the address`http://192.168.99.100:31931`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0307a557-39dc-4f7d-98af-f49d8c3c73a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to previous objects, services can also be displayed using the Kubernetes
    dashboard, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d5eb014-3315-437a-861c-69c44ff7a67b.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed a lot of topics not obviously related to Spring
    Cloud, but the tools explained in this chapter will allow you to take advantage
    of migrating to microservices-based architecture. When using Docker, Kubernetes,
    or tools for CI or CD, there is an obvious advantage to cloud-native development
    with Spring Cloud. Of course, all of the presented examples have been launched
    on the local machine, but you can refer to these to imagine how that process could
    be designed in a production environment across a cluster of remote machines.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we wanted to show you how simple and quick it can be to move
    from running Spring microservices manually on the local machine to a fully-automated
    process that builds the application from source code, creates and runs a Docker
    image with your application, and deploys it on a cluster consisting of multiple
    machines. It is not easy to describe all of the features provided by such complex
    tools as Docker, Kubernetes, or Jenkins in a single chapter. Instead, the main
    purpose here was to give you a look at the bigger picture of how to design and
    maintain a modern architecture based on concepts such as containerization, automated
    deploying, scaling, and a private, on-premise cloud.
  prefs: []
  type: TYPE_NORMAL
- en: We're now getting very close to the end of the book. We have already discussed
    most of the planned topics related to the Spring Cloud framework. In the next
    chapter, we will show you how to use two of the most popular cloud platforms available
    on the web, allowing you to continuously deliver Spring Cloud applications.
  prefs: []
  type: TYPE_NORMAL
