- en: Troubleshooting Guide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have come so far and I am sure you are enjoying each and every moment of
    this challenging and joyful learning journey. I will not say that this book ends
    after this chapter, but rather you are completing the first milestone. This milestone
    opens the doors for learning and implementing a new paradigm in the cloud with
    microservice-based design. I would like to reaffirm that integration testing is
    an important way to test the interaction between microservices and APIs. While
    working on your sample application **online table reservation system** (**OTRS**),
    I am sure you have faced many challenges, especially while debugging the application.
    Here, we will cover a few of the practices and tools that will help you to troubleshoot
    the deployed application, Docker containers, and host machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following three topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Logging and the ELK stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of correlation ID for service calls using Zipkin and Sleuth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependencies and versions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging and the ELK stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you imagine debugging any issue without seeing a log on the production system?
    Simply, no, as it would be difficult to go back in time. Therefore, we need logging.
    Logs also give us warning signals about the system if they are designed and coded
    that way. Logging and log analysis is an important step for troubleshooting any
    issue, and also for throughput, capacity, and monitoring the health of the system.
    Therefore, having a very good logging platform and strategy will enable effective
    debugging. Logging is one of the most important key components of software development
    in the initial days.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microservices are generally deployed using image containers such as Docker
    that provide the log with commands that help you to read logs of services deployed
    inside the containers. Docker and Docker Compose provide commands to stream the
    log output of running services within the container and in all containers respectively.
    Please refer to the following `logs` command of Docker and Docker Compose:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Docker logs command:** **Usage:** `docker logs [OPTIONS] <CONTAINER NAME>`  **Fetch
    the logs of a container:**'
  prefs: []
  type: TYPE_NORMAL
- en: '`**-f, --follow Follow log output**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**--help Print usage**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**--since="" Show logs since timestamp**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**-t, --timestamps Show timestamps**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**--tail="all" Number of lines to show from the end of the logs**`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Docker Compose logs command:** `**Usage: docker-compose logs [options] [SERVICE...]**`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Options:**'
  prefs: []
  type: TYPE_NORMAL
- en: '`**--no-color Produce monochrome output**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**-f, --follow Follow log output**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**-t, --timestamps Show timestamps**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**--tail Number of lines to show from the end of the logs for each container**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[SERVICES...] Service representing the container - you can give multiple**`'
  prefs: []
  type: TYPE_NORMAL
- en: These commands help you to explore the logs of microservices and other processes
    running inside the containers. As you can see, using the above commands would
    be a challenging task when you have a higher number of services. For example,
    if you have tens or hundreds of microservices, it would be very difficult to track
    each microservice log. Similarly, you can imagine, even without containers, how
    difficult it would be to monitor logs individually. Therefore, you can assume
    the difficulty of exploring and correlating the logs of tens to hundreds of containers.
    It is time-consuming and adds very little value.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a log aggregator and visualizing tools such as the ELK stack come
    to our rescue. It will be used for centralizing logging. We'll explore this in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Elasticsearch, Logstash, Kibana** (**ELK**) stack is a chain of tools
    that performs log aggregation, analysis, visualization, and monitoring. The ELK
    stack provides a complete logging platform that allows you to analyze, visualize,
    and monitor all of your logs, including all types of product logs and system logs.
    If you already know about the ELK stack, please skip to the next section. Here,
    we''ll provide a brief introduction to each tool in the ELK Stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87defb1f-0f40-4d90-8f8d-0be85aaf64c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'ELK overview (source: elastic.co)'
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elasticsearch is one of the most popular enterprise full text search engines.
    It is open source software. It is distributable and supports multi-tenancy. A
    single Elasticsearch server stores multiple indexes (each index represents a database),
    and a single query can search the data of multiple indexes. It is a distributed
    search engine and supports clustering.
  prefs: []
  type: TYPE_NORMAL
- en: It is readily scalable and can provide near-real-time searches with a latency
    of 1 second. It is developed in Java using Apache Lucene. Apache Lucene is also
    free and open source, and it provides the core of Elasticsearch, also known as
    the informational retrieval software library.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch APIs are extensive in nature and very elaborative. Elasticsearch
    provides a JSON-based schema, less storage, and represents data models in JSON.
    Elasticsearch APIs use JSON documents for HTTP requests and responses.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logstash is an open source data collection engine with real-time pipeline capabilities.
    In simple words, it collects, parses, processes, and stores the data. Since Logstash
    has data pipeline capabilities, it helps you to process any event data, such as
    logs, from a variety of systems. Logstash runs as an agent that collects the data,
    parses it, filters it, and sends the output to a designated app, such as Elasticsearch,
    or simple standard output on a console.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also has a very good plugin ecosystem (image sourced from [www.elastic.co](http://www.elastic.co)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8951d1d-10a5-4118-bf8d-6a6174fb9975.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Logstash ecosystem
  prefs: []
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kibana is an open source analytics and visualization web application. It is
    designed to work with Elasticsearch. You use Kibana to search, view, and interact
    with data stored in Elasticsearch indices.
  prefs: []
  type: TYPE_NORMAL
- en: It is a browser-based web application that lets you perform advanced data analysis
    and visualize your data in a variety of charts, tables, and maps. Moreover, it
    is a zero-configuration application. Therefore, it neither needs any coding nor
    additional infrastructure after installation.
  prefs: []
  type: TYPE_NORMAL
- en: ELK stack setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, these tools are installed individually and then configured to communicate
    with each other. The installation of these components is pretty straightforward.
    Download the installable artifact from the designated location and follow the
    installation steps, as shown in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The installation steps provided below are part of a basic setup which is required
    for setting up the ELK stack you want to run. Since this installation was done
    on my localhost machine, I have used the host localhost. It can be changed easily
    with any respective hostname that you want.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install Elasticsearch, we can use the Elasticsearch Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also install Elasticsearch by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the latest Elasticsearch distribution from [https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip it to the desired location in your system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure the latest Java version is installed and the `JAVA_HOME` environment
    variable is set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to Elasticsearch home and run `bin/elasticsearch` on Unix-based systems and
    `bin/elasticsearch.bat` on Windows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open any browser and hit `http://localhost:9200/`. On successful installation,
    it should provide you with a JSON object similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the GUI is not installed. You can install one by executing the
    following command from the `bin` directory; make sure the system is connected
    to the internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you are using the Elasticsearch image, then run the Docker image (later,
    we'll use `docker-compose` to run the ELK stack together).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, you can access the GUI interface with the URL `http://localhost:9200/_plugin/head/`.
    You can replace `localhost` and `9200` with your respective hostname and port
    number.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install Logstash, we can use the Logstash Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also install Logstash by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the latest Logstash distribution from [https://www.elastic.co/downloads/logstash](https://www.elastic.co/downloads/logstash).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip it to the desired location in your system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prepare a configuration file, as shown. It instructs Logstash to read input
    from given files and passes it to Elasticsearch (see the following `config` file;
    Elasticsearch is represented by localhost and the `9200` port). It is the simplest
    configuration file. To add filters and learn more about Logstash, you can explore
    the Logstash reference documentation available at [https://www.elastic.co/guide/en/logstash/current/index.html](https://www.elastic.co/guide/en/logstash/current/index.html):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, the OTRS `service` log and `edge-server` log are added as input.
    Similarly, you can also add log files of other microservices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Go to Logstash home and run `bin/logstash agent -f logstash.conf` on Unix-based
    systems and `bin/logstash.bat agent -f logstash.conf` on Windows. Here, Logstash
    is executed using the `agent` command. The Logstash agent collects data from the
    sources provided in the input field in the configuration file and sends the output
    to Elasticsearch. Here, we have not used the filters, because otherwise it may
    process the input data before providing it to Elasticsearch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, you can run Logstash using the downloaded Docker image (later, we'll
    use the `docker-compose` to run the ELK stack together).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install Kibana, we can use the Kibana Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also install the Kibana web application by performing the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the latest Kibana distribution from: [https://www.elastic.co/downloads/kibana](https://www.elastic.co/downloads/kibana).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip it to the desired location in your system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the configuration file `config/kibana.yml` from the Kibana home directory
    and point the `elasticsearch.url` to the previously configured Elasticsearch instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Go to Kibana home and run `bin/kibana agent -f logstash.conf` on Unix-based
    systems and `bin/kibana.bat agent -f logstash.conf` on Windows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are using the Kibana Docker image, then you can run the Docker image
    (later, we'll use docker-compose to run the ELK stack together).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, you can access the Kibana app from your browser using the URL `http://localhost:5601/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To learn more about Kibana, explore the Kibana reference documentation at [https://www.elastic.co/guide/en/kibana/current/getting-started.html](https://www.elastic.co/guide/en/kibana/current/getting-started.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we followed the preceding steps, you may have noticed that it requires some
    amount of effort. If you want to avoid a manual setup, you can Dockerize it. If
    you don't want to put effort into creating the Docker container of the ELK stack,
    you can choose one from Docker Hub. On Docker Hub, there are many ready-made ELK
    stack Docker images. You can try different ELK containers and choose the one that
    suits you the most. `willdurand/elk` is the most downloaded container and is easy
    to start, working well with Docker Compose.
  prefs: []
  type: TYPE_NORMAL
- en: Running the ELK stack using Docker Compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ELK images available on elastic.co''s own Docker repository have the XPack
    package enabled by default at the time of writing this section. In the future,
    it may be optional. Based on XPack availability in ELK images, you can modify
    the docker-compose file `docker-compose-elk.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you save the ELK Docker Compose file, you can run the ELK stack using
    the following command (the command is run from the directory that contains the
    Docker Compose file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the preceding command is as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c32104c2-3d7a-4822-a5f1-15d7e7be0b52.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the ELK stack using Docker Compose
  prefs: []
  type: TYPE_NORMAL
- en: 'If volume is not used, the environment pipeline does not work. For a Windows
    environment such as Windows 7, where normally volume is hard to configure, you
    can copy the pipeline CONF file inside the container and restart the Logstash
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Please restart the Logstash container after copying the pipeline CONF file
    `pipeline/logstash.conf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Pushing logs to the ELK stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are done making the ELK stack available for consumption. Now, Logstash just
    needs a log stream that can be indexed by Elasticsearch. Once the Elasticsearch
    index of logs is created, logs can be accessed and processed on the Kibana dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: To push the logs to Logstash, we need to make the following changes in our service
    code. We need to add logback and logstash-logback encoder dependencies in OTRS
    services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following dependencies in the `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We also need to configure the logback by adding `logback.xml` to `src/main/resources`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `logback.xml` file will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, the destination is `192.168.99.100:5001`, where Logstash is hosted; you
    can change it based on your configuration. For the encoder, the `net.logstash.logback.encoder.LogstashEncoder`
    class is used. The value of the `spring.application.name` property should be set
    to the service for which it is configured. Simiarly, a shutdown hook is added,
    so that once the service is stopped, all resources should be released and cleaned.
  prefs: []
  type: TYPE_NORMAL
- en: You want to start services after the ELK stack is available, so services can
    push the logs to Logstash.
  prefs: []
  type: TYPE_NORMAL
- en: Once the ELK stack and services are up, you can check the ELK stack to view
    the logs. You want to wait for a few minutes after starting the ELK stack and
    then access the following URLs (replace the IP based on your configuration).
  prefs: []
  type: TYPE_NORMAL
- en: 'To check whether Elasticsearch is up, access the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To check whether indexes have been created or not, access either of the following
    URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the Logstash index is done (you may have a few service endpoints to generate
    some logs), access Kibana:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Tips for ELK stack implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some useful tips for implementing the ELK stack:'
  prefs: []
  type: TYPE_NORMAL
- en: To avoid any data loss and handle the sudden spike of input load, using a broker
    such as Redis or RabbitMQ is recommended between Logstash and Elasticsearch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an odd number of nodes for Elasticsearch if you are using clustering to
    prevent the split-brain problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Elasticsearch, always use the appropriate field type for given data. This
    will allow you to perform different checks; for example, the `int` field type
    will allow you to perform `("http_status:<400")` or `("http_status:=200")`. Similarly,
    other field types also allow you to perform similar checks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of correlation ID for service calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you make a call to any REST endpoint and if any issue pops up, it is difficult
    to trace the issue and its root origin because each call is made to a server,
    and this call may call another, and so on and so forth. This makes it very difficult
    to figure out how one particular request was transformed and what it was called.
    Normally, an issue that is caused by one service can have domino effect on other
    services or can fail other service operation. It is very difficult to track and
    may require an enormous amount of effort. If it is monolithic, you know that you
    are looking in the right direction, but microservices make it difficult to understand
    what the source of the issue is and where you should get your data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how we can tackle this problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By using a correlation ID that is passed across all calls, it allows you to
    track each request and track the route easily. Each request will have its unique
    correlation ID. Therefore, when we debug any issue, the correlation ID is our
    starting point. We can follow it and, along the way, we can find out what went
    wrong.
  prefs: []
  type: TYPE_NORMAL
- en: The correlation ID requires some extra development effort, but it's effort well
    spent as it helps a lot in the long run. When a request travels between different
    microservices, you will be able to see all interactions and which service has
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: This is not something new or invented for microservices. This pattern is already
    being used by many popular products such as Microsoft SharePoint.
  prefs: []
  type: TYPE_NORMAL
- en: Use of Zipkin and Sleuth for tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the OTRS application, we'll make use of Zipkin and Sleuth for tracking.
    It provides trace IDs and span IDs and a nice UI to trace the requests. More importantly,
    you can find out the time taken by each request in Zipkin and it allows you to
    drill down to find out the request that makes maximum time for serving the request.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, you can see the time taken by the `findById` API
    call of the restaurant as well as the trace ID of the same request. It also shows
    the span ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2422251-7e3e-4319-947a-3d747347f8b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Total time taken and trace ID of restaurant `findById` API call
  prefs: []
  type: TYPE_NORMAL
- en: We'll stick to the following steps to configure the Zipkin and Sleuth in OTRS
    services.
  prefs: []
  type: TYPE_NORMAL
- en: 'You just need to add Sleuth and Sleuth-Zipkin dependencies to enable the tracking
    and request tracing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Access the Zipkin dashboard and find out the time taken by different requests.
    Replace the port if the default port is changed. Please make sure that services
    are up before making use of Zipkin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if the ELK stack is configured and up, then you can use this trace ID
    to find the appropriate logs in Kibana, as shown in following screenshot. The
    X-B3-TraceId field is available in Kibana, which is used to filter the logs based
    on trace ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04a4d229-71fa-4d6d-97b5-0289e3e650cd.png)Kibana dashboard - search
    based on request trace ID'
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies and versions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two common problems that we face in product development are cyclic dependencies
    and API versions. We'll discuss them in terms of microservice-based architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Cyclic dependencies and their impact
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, monolithic architecture has a typical layer model, whereas microservices
    carry the graph model. Therefore, microservices may have cyclic dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is necessary to keep a dependency check on microservice relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us have a look at the following two cases:'
  prefs: []
  type: TYPE_NORMAL
- en: If you have a cycle of dependencies between your microservices, you are vulnerable
    to distributed stack overflow errors when a certain transaction might be stuck
    in a loop. For example, when a restaurant table is being reserved by a person.
    In this case, the restaurant needs to know the person (`findBookedUser`), and
    the person needs to know the restaurant at a given time (`findBookedRestaurant`).
    If it is not designed well, these services may call each other in a loop. The
    result may be a stack overflow generated by JVM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If two services share a dependency and you update that other service's API in
    a way that could affect them, you'll need to update all three at once. This brings
    up questions such as, which should you update first? In addition, how do you make
    this a safe transition?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing dependencies while designing the system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Therefore, it is important while designing the microservices to establish the
    proper relationship between different services internally to avoid any cyclic
    dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: It is a design issue and must be addressed, even if it requires a refactoring
    of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining different versions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you have more services, it means different release cycles for each of them,
    which adds to this complexity by introducing different versions of services, in
    that there will be different versions of the same REST services. Reproducing the
    solution to a problem will prove to be very difficult when it has gone in one
    version and returns in a newer one.
  prefs: []
  type: TYPE_NORMAL
- en: Let's explore more
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The versioning of APIs is important because, over time, APIs change. Your knowledge
    and experience improves with time, and that leads to changes in APIs. Changing
    APIs may break existing client integrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, there are various ways to manage the API versions. One of these
    is using the version in the path that we have used in this book; some also use
    the HTTP header. The HTTP header could be a custom request header or you could
    use `Accept Header` for representing the calling API version. For more information
    on how versions are handled using HTTP headers, please refer to *RESTful Java
    Patterns and Best Practices* by Bhakti Mehta, Packt Publishing: [https://www.packtpub.com/application-development/restful-java-patterns-and-best-practices](https://www.packtpub.com/application-development/restful-java-patterns-and-best-practices).'
  prefs: []
  type: TYPE_NORMAL
- en: It is very important while troubleshooting any issue that your microservices
    are implemented to produce the version numbers in logs. In addition, ideally,
    you should avoid any instance where you have too many versions of any microservice.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This following links will have more information:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Elasticsearch: [https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logstash: [https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kibana: [https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`willdurand/elk`: ELK Docker image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering Elasticsearch - Second* *Edition*: [https://www.packtpub.com/web-development/mastering-elasticsearch-second-edition](https://www.packtpub.com/web-development/mastering-elasticsearch-second-edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored the ELK stack overview and installation. In
    the ELK stack, Elasticsearch is used for storing the logs and service queries
    from Kibana. Logstash is an agent that runs on each server that you wish to collect
    logs from. Logstash reads the logs, filters/transforms them, and provides them
    to Elasticsearch. Kibana reads/queries the data from Elasticsearch and presents
    it in tabular or graphical visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: We also understand the utility of having the correlation ID while debugging
    issues. At the end of this chapter, we also discovered the shortcomings of a few
    microservice designs. It was a challenging task to cover all of the topics relating
    to microservices in this book, so I tried to include as much relevant information
    as possible with precise sections with references, which allow you to explore
    more. Now, I would like to let you start implementing the concepts we have learned
    in this chapter in your workplace or in your personal projects. This will not
    only give you hands-on experience, but may also allow you to master microservices.
    In addition, you will also be able to participate in local meetups and conferences.
  prefs: []
  type: TYPE_NORMAL
