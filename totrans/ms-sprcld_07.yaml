- en: Advanced Load Balancing and Circuit Breakers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will continue the subject discussed in the previous chapter,
    inter-service communication. We will extend it to more advanced samples of load
    balancing, timeouts, and circuit breaking.
  prefs: []
  type: TYPE_NORMAL
- en: Spring Cloud provides features that make implementation of communication between
    microservices nice and simple. However, we must not forget that the major difficulties
    we would face with such communication concern the processing time of the systems
    involved. If you have many microservices in your system, one of the first issues
    you need to deal with is the problem of latency. In this chapter, I would like
    to discuss a few Spring Cloud features that help us to avoid latency problems
    that are caused by many hops between services when processing a single input request,
    slow responses from several services, or a temporary unavailability of services.
    There are several strategies for dealing with partial failures. These include
    setting network timeouts, limiting the number of waiting requests, implementing
    different load balancing methods, or setting up a circuit breaker pattern and
    fallback implementation.
  prefs: []
  type: TYPE_NORMAL
- en: We will also talk about Ribbon and Feign clients once again, this time focusing
    on their more advanced configuration features. An entirely new library that will
    be introduced here is Netflix Hystrix. This library implements the circuit breaker
    pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we will cover in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Different load balancing algorithms with Ribbon clients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling a circuit breaker for the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing Hystrix with configuration properties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring interservice communication with the Hystrix dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Hystrix together with Feign clients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spring Cloud Netflix provides different load balancing algorithms in order
    to provide different benefits to the user. Your choice of supported method depends
    on your needs. In the Netflix OSS nomenclature, this algorithm is called a **rule**.
    The custom rule class should have implemented an `IRule` base interface. The following
    implementations are available by default inside Spring Cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RoundRobinRule`: This rule simply chooses servers using the well-known round
    robin algorithm, where incoming requests are distributed across all instances
    sequentially. It is often used as the default rule or fallbacks for more advanced
    rules, such as `ClientConfigEnabledRoundRobinRule` and `ZoneAvoidanceRule`. `ZoneAvoidanceRule`
    is the default rule for Ribbon clients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AvailabilityFilteringRule`: This rule will skip servers that are marked as
    circuit tripped or with a high number of concurrent connections. It also uses
    `RoundRobinRule` as a base class. By default, an instance is circuit tripped if
    an HTTP client fails to establish a connection with it three times in a row. This
    approach may be customized with the `niws.loadbalancer.<clientName>.connectionFailureCountThreshold` property.
    Once an instance is circuit tripped, it will remain in this state for the next
    30 seconds before the next retry. This property may also be overridden in the
    configuration settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WeightedResponseTimeRule`: With this implementation, a traffic volume forwarder
    to the instance is inversely proportional to the instance''s average response
    time. In other words, the longer the response time, the less weight it will get.
    In these circumstances, a load balancing client will record the traffic and response
    time of every instance of the service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BestAvailableRule`: According to the description from the class documentation,
    this rule skips servers with *tripped* circuit breakers and picks the server with
    the lowest concurrent requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tripped circuit breaker is a term taken from electrical engineering, and means
    that there's no current flowing through a circuit. In IT terminology, it refers
    to the situation where too many consecutive requests that are sent to a service
    fail, and therefore any further attempts to invoke the remote service will be
    interrupted immediately by the software on the client side in order to relieve
    the server-side application.
  prefs: []
  type: TYPE_NORMAL
- en: The WeightedResponseTime rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we have usually tested our services manually by calling them from
    a web browser or a REST client. The current changes do not allow such an approach
    because we need to set fake delays for the services, as well as generate many
    HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Hoverfly for testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, I would like to introduce an interesting framework that may
    be a perfect solution for these kinds of tests. I am talking about Hoverfly, a
    lightweight service virtualization tool that is used to stub or simulate HTTP
    services. It is originally written in Go, but also gives you an expressive API
    for managing Hoverfly in Java. Hoverfly Java, maintained by SpectoLabs, provides
    classes that abstract away the binary and API calls, a DSL for creating simulations,
    and an integration with the JUnit test framework. This framework has a feature
    that I really like. You may easily add a delay to every simulated service by calling
    one method in your DSL definition. To enable Hoverfly for your project, you have
    to include the following dependency in your Maven `pom.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Testing the rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sample we are discussing here is available on GitHub. To access it, you
    have to switch to  `weighted_lb` branch ([https://github.com/piomin/sample-spring-cloud-comm/tree/weighted_lb](https://github.com/piomin/sample-spring-cloud-comm/tree/weighted_lb)).
    Our JUnit test class, called `CustomerControllerTest`, is available under the `src/test/java`
    directory. To enable Hoverfly for the test, we should define the JUnit `@ClassRule`.
    The `HoverflyRule` class provides an API that allows us to simulate many services
    with different addresses, characteristics, and responses. In the following source
    code fragment, you may see that two instances of our sample microservice `account-service`
    have been declared inside `@ClassRule`.  As you probably remember, that service
    has been invoked by `customer-service` and `order-service`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a test class from the `customer-service` module. It simulates
    the `GET /customer/*` method with a predefined response for two instances of `account-service` available
    on ports `8091` and `9091`. The first of them has been delayed by `200` milliseconds,
    while the second is delayed by `50` milliseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Before running the test, we should also modify the `ribbon.listOfServers` configuration
    file by changing it to `listOfServers: account-service:8091, account-service:9091`.
    We should only make such a modification when working with Hoverfly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a `test` method that invokes the `GET /withAccounts/ {id}` endpoint
    exposed by `customer-service` a thousand times. This, in turn, invokes the `GET
    customer/{customerId}` endpoint from `account-service`, with a list of accounts
    owned by the customer. Every request is load balanced between two instances of
    `account-service` using `WeightedResponseTimeRule`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The method of working with a weighted response rule implementation is really
    interesting. Just after starting the test, the incoming requests are load balanced
    at a ratio of 50:50 between two instances of `account-service`. But, after some
    time, most of them are forwarded to the instance with the lesser delay.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, 731 requests were processed by the instance available on port  `9091` and
    269 by the instance at port `8091` for a JUnit test launched on my local machine. However,
    at the end of the test, the proportion looked a bit different and was weighted
    in favor of the instance with the lesser delay, where incoming traffic is divided
    4:1 between the two instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will change our test case a little by adding a third instance of `account-service`
    with a big delay of around 10 seconds. This modification aims to simulate a timeout
    in HTTP communication. Here''s the fragment from the JUnit `@ClassRule` definition
    with the newest service instance listening on port `10091`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We should accordingly perform a change in the Ribbon configuration to enable
    load balancing to the newest instance of `account-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The last thing that has to be changed, but which is left as it is in the previous
    test case, is the `RestTemplate` bean declaration. In this instance, I have set
    both the read and the connect timeout to one second because the third instance
    of `account-service` launched during the test is delayed by 10 seconds. Every
    request sent there would be terminated by the timeout after one second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you run the same test as before, the result would not be satisfactory. The
    distribution between all declared instances will be 420, processed by the instance
    listening on port `8091` (with a delay of `200` milliseconds), 468, processed
    by the instance listening on port `9091` (with a delay of `50` milliseconds),
    and 112 sent to the third instance, terminated by the timeout. Why am I quoting
    all these statistics? We may change a default load balancing rule from `WeightedResponseTimeRule`
    to `AvailabilityFilteringRule` and rerun the test. If we do this, 496 requests
    will be sent to both the first and second instance, while only 8 will be sent
    to the third instance, with a one second timeout. Interestingly, if you set `BestAvailableRule`
    as the default rule, all requests would be sent to the first instance.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have read through this example, you can easily see the differences
    between all available load balancing rules for the Ribbon client.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the Ribbon client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Several configuration settings of the Ribbon client may be overridden with
    Spring bean declarations. As with Feign, it should be declared in the client annotation
    field named configuration, for example,`@RibbonClient(name = "account-service",
    configuration = RibbonConfiguration.class)`. The following features may be customized
    with this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '`IClientConfig`: The default implementation of this is `DefaultClientConfigImpl`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRule`: This component is used to determine which service instance should
    be selected from a list. The `ZoneAvoidanceRule` implementation class is auto-configured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IPing`: This is a component that runs in the background. It is responsible
    for ensuring that the instances of service are running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ServerList<Server>`: This can be static or dynamic. If it is dynamic (as used
    by `DynamicServerListLoadBalancer`), a background thread will refresh and filter
    the list at a predefined interval. By default, Ribbon uses a static list of servers
    taken from configuration file. It is implemented by  `ConfigurationBasedServerList`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ServerListFilter<Server>`: `ServerListFilter` is a component used by `DynamicServerListLoadBalancer`
    to filter the servers returned from a `ServerList` implementation. There are two
    implementations of that interface—auto-configured `ZonePreferenceServerListFilter`
    and `ServerListSubsetFilter`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ILoadBalancer`: This is responsible for performing load balancing between
    available instances of a service on the client side. By default, Ribbon uses `ZoneAwareLoadBalancer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ServerListUpdater`: This is responsible for updating the list of available
    instances of a given application. By default, Ribbon uses `PollingServerListUpdater`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at an example configuration class that defines the default implementation
    of the `IRule` and `IPing` components. Such a configuration may be defined for
    a single Ribbon client, as well as for all Ribbon clients available in the application
    classpath, by providing the `@RibbonClients(defaultConfiguration = RibbonConfiguration.class)` annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Even if you don''t have any experience with Spring, you may probably have guessed
    (based on the previous samples) that the configuration can also be customized
    using the `properties` file. In that case, Spring Cloud Netflix is compatible
    with the properties described in the Ribbon documentation provided by Netflix.
    The following classes are the supported properties, and they should be prefixed
    by `<clientName>.ribbon`, or, if they apply to all clients, by `ribbon`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NFLoadBalancerClassName`: `ILoadBalancer` default implementation class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NFLoadBalancerRuleClassName`: `IRule` default implementation class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NFLoadBalancerPingClassName`: `IPing` default implementation class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NIWSServerListClassName`: `ServerList` default implementation class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NIWSServerListFilterClassName`: `ServerListFilter` default implementation
    class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s a similar sample to the preceding `@Configuration` class that overrides
    the `IRule` and `IPing` default implementations used by the Spring Cloud application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The circuit breaker pattern with Hystrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already discussed the different implementations of load balancer algorithms
    in Spring Cloud Netflix. Some of them are based on monitoring the instance response
    time or the number of failures. In these cases, a load balancer makes decisions
    about which instance should be invoked based on these statistics. The circuit
    breaker pattern should be treated as an extension of that solution. The main idea
    behind a circuit breaker is very simple. A protected function call is wrapped
    in a circuit breaker object, which is responsible for monitoring a number of failure
    calls. If the failures reach a threshold, the circuit is opened, and all further
    calls will be failed automatically. Usually, it is also desirable to have some
    kind of monitor alert if a circuit breaker trips. Some crucial benefits derived
    from the usage of the circuit breaker pattern in your applications are the ability
    to continue operating when a related service fails, the prevention of a cascaded
    failure, and giving a failing service time to recover.
  prefs: []
  type: TYPE_NORMAL
- en: Building an application with Hystrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Netflix provides an implementation of the circuit breaker pattern in their
    library called **Hystrix**. That library has also been included as a default implementation
    of the circuit breaker for Spring Cloud. Hystrix has some other interesting features,
    and should also be treated as a comprehensive tool for dealing with latency and
    fault tolerance for distributed systems. What is important is that if the circuit
    breaker is opened, Hystrix redirects all calls to the specified fallback method.
    The fallback method is designed to provide a generic response without any dependency
    on a network, usually read from an in-memory cache or just implemented as static
    logic. If it becomes necessary to perform a network call, it is recommended that
    you implement it using another `HystrixCommand` or `HystrixObservableCommand`.
    To include Hystrix in your project, you should use the `spring-cloud-starter-netflix-hystrix`
    or `spring-cloud-starter-hystrix` starter for Spring Cloud Netflix versions older
    than 1.4.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Implementing Hystrix's commands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spring Cloud Netflix Hystrix looks for a method that is annotated with the
    `@HystrixCommand` annotation, and then wraps it in a proxy object connected to
    a circuit breaker. Thanks to this, Hystrix is able to monitor all calls of such
    a method. This annotation currently works only for a class marked with `@Component`
    or `@Service`. That''s important information for us, because we have implemented
    the logic related to other services calling in all the previous samples inside
    the REST controller class, which is marked with the `@RestController` annotation.
    So, in the `customer-service` application, all that logic has been moved to the
    newly created `CustomerService` class, which is then injected into the controller
    bean. The method responsible for communication with `account-service` has been
    annotated with `@HystrixCommand`. I have also implemented a fallback method, the
    name of which passes into the `fallbackMethod` annotation''s field. This method only returns
    an empty list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t forget to mark your main class with `@EnableHystrix`, which is needed
    to tell Spring Cloud that it should use circuit breakers for the application.
    We may also optionally annotate a class with `@EnableCircuitBreaker`, which does
    the same. For test purposes, the `account-service.ribbon.listOfServers` property should
    have included the network addresses of two instances of the `localhost:8091, localhost:9091` service.
    Although we have declared two instances of `account-service` for the Ribbon client,
    we will start the only one that is available on the `8091` port. If you call the `customer-service`
    method `GET http://localhost:8092/withAccounts/{id}`, Ribbon will try to load
    balance every incoming request between those two declared instances, that is,
    once you receive the response containing a list of accounts and the second time
    you receive an empty account list, or vice versa. This is illustrated by the following
    fragment of the application logs. This is illustrated by the following fragment
    of application''s logs. To access the sample application''s source code, you should
    switch to the `hystrix_basic` branch ([https://github.com/piomin/sample-spring-cloud-comm/tree/hystrix_basic](https://github.com/piomin/sample-spring-cloud-comm/tree/hystrix_basic))
    in the same GitHub repository as the samples from the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Implementing fallback with cached data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The fallback implementation presented in the previous example is very simple.
    Returning an empty list does not make much sense for an application running in
    production. It makes more sense to use the fallback method in your application
    when you read data from a cache in case of a request failure, for example. Such
    a cache may be implemented inside the client application or with the use of third-party
    tools, such as Redis, Hazelcast, or EhCache. The simplest implementation is available
    within the Spring Framework, and can be used after including the `spring-boot-starter-cache` artifact with your
    dependencies. To enable caching for the Spring Boot application, you should annotate
    the main or configuration class with `@EnableCaching` and provide the `CacheManager`
    bean in the following context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can mark the method wrapped with the circuit breaker using the `@CachePut`
    annotation. This will add the result returning from the calling method to the
    cache map. In that case, our map is named `accounts`. Finally, you may read the
    data inside your fallback method implementation by invoking the `CacheManager`
    bean directly. If you retry the same request a couple of times, you will see that
    the empty list of accounts is no longer returned as a response. Instead, the service
    always returns data that is cached during the first successful call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The tripping circuit breaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let me suggest an exercise for you to do. Until now, you have learned how to
    enable and implement circuit breakers in your application using Hystrix, in conjunction
    with Spring Cloud, and how to use a fallback method to take data from the cache.
    But you still have not used a tripped circuit breaker to prevent the failure instance
    from being invoked by a load balancer. Now, I would like to configure Hystrix
    to open the circuit after three failed call attempts if the failure percentage
    is greater than `30` percent and prevent the API method from being called for
    the next 5 seconds. The measurement time window is around `10` seconds. To meet
    these requirements, we have to override several default Hystrix configuration
    settings. It may be performed using the `@HystrixProperty` annotation inside `@HystrixCommand`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the current implementation of the method responsible for getting the
    account list from `customer-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The full list of Hystrix''s configuration properties is available on Netflix''s
    GitHub site at[ https://github.com/Netflix/Hystrix/wiki/Configuration](https://github.com/Netflix/Hystrix/wiki/Configuration).
    I won''t discuss all of them, only the most important properties for communication
    between microservices. Here''s the list of the properties used in our sample,
    along with their descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`execution.isolation.thread.timeoutInMilliseconds`: This property sets the
    time in milliseconds, after which a read or connect timeout will occur and the
    client will walk away from the command execution. Hystrix marks such a method
    call as a failure, and performs fallback logic. That timeout may be completely
    turned off by setting the `command.timeout.enabled` property to `false`. The default
    is 1,000 milliseconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`circuitBreaker.requestVolumeThreshold`: This property sets the minimum number
    of requests in a rolling window that will trip the circuit. The default value
    is 20\. In our sample, this property is set to `10`, which means that the first
    nine will not trip the circuit, even if all of them fail. I set that value because
    we have assumed that the circuit should be opened if `30` percent of incoming
    requests fail, but the minimum number of incoming requests is three.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`circuitBreaker.errorThresholdPercentage`: This property sets the minimum error
    percentage. Exceeding this percentage results in opening the circuit, and the
    system starts short-circuiting requests to fallback logic. The default value is
    50\. I set it to `30` because, in our sample, I want `30` percent of failed requests
    should open the circuit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`circuitBreaker.sleepWindowInMilliseconds`: This property sets a period of
    time between tripping the circuit and allowing attempts taken in order to determine
    whether the circuit should be closed again. During this time, all incoming requests
    are rejected. The default value is `5,000`. Because we would like to wait `10`
    seconds before the first call is retired after the circuit has been opened, I
    set it to `10,000`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metrics.rollingStats.timeInMilliseconds`: This property sets the duration
    of the statistical rolling window in milliseconds. This is how long Hystrix keeps
    metrics for the circuit breaker to use, and for publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these settings, we may run the same JUnit test as for the previous example.
    We launch two stubs of `account-service` using `HoverflyRule`. The first of them
    would be delayed by 200 milliseconds, while a second one that is delayed by 2,000
    milliseconds is greater than the timeout set for `@HystrixCommand` with the `execution.isolation.thread.timeoutInMilliseconds` property.
    After running JUnit `CustomerControllerTest`, take a look at the printed logs.
    I have inserted the logs taken from the test launched on my machine. The first
    request from `customer-service` is load balanced to the first instance, delayed
    by 200 ms `(1)`. Every request sent to the instance available on `9091` finishes
    with a timeout after one second. After sending 10 requests, the first failure
    causes a trip of the circuit `(2)`. Then, for the next 10 seconds, every single
    request is handled by a fallback method, which returns cached data `(3)`, `(4)`.
    After 10 seconds, the client tries to call an instance of `account-service` again
    and succeeds `(5)` because it hits on the instance delayed by 200 ms. That success
    results in the closure of the circuit. Unfortunately, the second instance of `account-service`
    still responds slowly, so the scenario happens all over again until the JUnit
    test finishes `(6)` and `(7)`. This detailed description shows you exactly how
    a circuit breaker with Hystrix works for Spring Cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Monitoring latency and fault tolerance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I have already mentioned, Hystrix is not only a simple tool implementing
    a circuit breaker pattern. It is a solution that deals with latency and fault
    tolerance in distributed systems. One interesting feature provided by Hystrix
    is the ability to expose the most important metrics related to interservice communication
    and display them using a UI dashboard. This function is available for clients
    wrapped with the Hystrix command.
  prefs: []
  type: TYPE_NORMAL
- en: In some previous samples, we have analyzed only a part of our system to simulate
    a delay in communication between `customer-service` and `account-service`. That's
    a really good approach when testing advanced load balancing algorithms or different
    circuit breaker configuration settings, but now we will go back to analyzing the
    whole of our sample system setup as a set of standalone Spring Boot applications.
    This allows us to observe how Spring Cloud, in conjunction with Netflix OSS tools,
    helps us to monitor and react to latency issues and failures in communication
    between our microservices. The sample system simulates a failure in a simple way.
    It has a static configuration with the network addresses of two instances, `account-service`,
    and `product-service`, but only one of them for each service is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to refresh your memory, the architecture of our sample system, taking
    into consideration assumptions about failure, is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce848d7e-8834-48f2-885f-273126e8aa5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This time, we''ll begin a bit differently, with a test. Here''s the fragment
    of the test method, which is being invoked in a loop. First, it calls the `POST
    http://localhost:8090/` endpoint from `order-service`, sending an `Order` object,
    and it receives a response with the `id`, `status`, and `price` set. Within that
    request, which has been labeled in the preceding diagram as `(1)`, `order-service`
    communicates with `product-service `and `customer-service` and, in addition, `customer-service`
    calls the endpoint from `account-service`. If the order has been accepted, the
    test client calls the `PUT http://localhost:8090/{id}` method with the order''s `id`
    to accept it and withdraw funds from the account. On the server side, there is
    only one interservice communication in that case, which is labeled `(2)` in the
    preceding diagram. Before running this test, you have to launch all microservices
    that are a part of our system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Exposing Hystrix's metrics stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each microservice that uses Hystrix in communication with other microservices
    may expose metrics of every integration wrapped with the Hystrix command. To enable
    such a metrics stream, you should include a dependency on `spring-boot-starter-actuator`.
    This will expose the `/hystrix.stream` object as a management endpoint. It is
    also necessary to include `spring-cloud-starter-hystrix`, which has already been
    added to our sample application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'A generated stream is exposed as further JSON entries containing metrics characterizing
    a single call within a method. Here''s a single entry for a call within the `GET
    /withAccounts/{id}` method from `customer-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Hystrix dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hystrix dashboard visualizes the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Health and traffic volume is displayed as a circle that is changing its color
    and size together with the changes in incoming statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The error percentage over the last 10 seconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The request rate over the last two minutes by number, displaying the results
    on a graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The circuit breaker status (open/closed)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of service hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latency percentiles over the last minute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The service's thread pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an application with the dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Hystrix dashboard is integrated with Spring Cloud. The best approach when
    implementing the dashboard inside a system is to separate out an independent Spring
    Boot application with the dashboard. To include the Hystrix dashboard in your
    project, use the `spring-cloud-starter-hystrix-netflix-dashboard` starter or `spring-cloud-starter-hystrix-dashboard`
    for Spring Cloud Netflix versions older than 1.4.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The application''s main class should be annotated with `@EnableHystrixDashboard`.
    After launching it, the Hystrix dashboard is available under the `/hystrix` context
    path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'I configured port `9000` as the default for the Hystrix application in our
    sample system, which is implemented in the `hystrix-dashboard` module. So, if
    you call the `http://localhost:9000/hystrix` address in a web browser after launching
    `hystrix-dashboard`, it will display the page as shown in the following screenshot.
    There, you should provide the Hystrix stream endpoint''s address, and, optionally,
    a title. If you would like to display metrics for all the endpoints that are called
    from `order-service`, type the address `http://localhost:8090/hystrix.stream`
    and then click the Monitor Stream button :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0aa6b722-29ff-47e5-a8f4-34ea62bc2944.png)'
  prefs: []
  type: TYPE_IMG
- en: Monitoring metrics on the dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at calling the `GET /withAccounts/{id}` method from
    `customer-service`. It is wrapped with `@HystrixCommand`. It is displayed on the Hystrix
    dashboard under the title `customer-service.findWithAccounts`, taken from a `commandKey` attribute.
    In addition, the UI dashboard also shows information about the thread pools that
    are assigned to every Spring Bean that provides an implementation of methods wrapped
    with Hystrix''s command. In this case, it is `CustomerService`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the screen from the Hystrix dashboard just after the start of a JUnit
    test. We monitor the state of all three methods wrapped with `@HystrixCommand`.
    The circuit has been opened for the `findByIds` method from `product-service`,
    as expected. After a few seconds, the circuit has also been opened for the `withdraw`
    method from `account-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe9084d4-8482-4bd7-aad2-470cd1af41ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After a few moments, the situation will be stabilized. All the circuits remain
    closed because only a small percentage of traffic is sent to the inactive instances
    of applications. This shows the power of Spring Cloud with Hystrix and Ribbon.
    The system was able to automatically reconfigure itself in order to redirect most
    of the incoming requests to the working instances based on the metrics generated
    by the load balancers and circuit breakers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a1a4f33-d388-46a9-a060-05d87303eb6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Aggregating Hystrix's streams with Turbine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have probably noticed that we were only able to look at an individual instance
    of the service in the Hystrix dashboard. There were no metrics from communication
    between `customer-service` and `account-service` when we were displaying the state
    of commands for `order-service`, and vice versa. We might also imagine that there
    is more than one instance of `order-service` running, which makes it necessary
    to switch regularly between different instances or services in the Hystrix dashboard.
    Fortunately, there is an application called **Turbine** that aggregates all of
    the relevant `/hystrix.stream` endpoints into a combined `/turbine.stream` and
    makes it possible for us to monitor the overall health of the whole system.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Turbine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before making any changes to enable Turbine for our application, we should
    start by enabling service discovery, which is required here. Switch to the `hystrix_with_turbine` branch to
    access the version of our sample system that supports service discovery with Eureka
    and aggregates Hystrix''s streams using Turbine. To enable Turbine for the project
    exposing the UI dashboard, just include `spring-cloud-starter-turbine` in the
    dependencies and annotate the main application class with `@EnableTurbine`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `turbine.appConfig` configuration property is a list of Eureka service
    names that Turbine will use to look up instances. The Turbine stream is then available
    in the Hystrix dashboard under the URL `http://localhost:9000/turbine.stream`.
    The address is also determined by a value of the `turbine.aggregator.clusterConfig`
    property, `http://localhost:9000/turbine.stream?cluster=<clusterName>`. The cluster
    parameter can be omitted if the name is `default`. Here''s the Turbine configuration
    that combines all of Hystrix''s visualization metrics in a single UI dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, all of Hystrix''s metrics for the whole sample system are displayed in
    a single dashboard site. All we need to display them is to monitor the statistics
    stream, available under `http://localhost:9000/turbine.stream`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1426d797-14f8-4198-8bc0-0090718a565c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, we can configure a cluster per service by providing a list of
    services with the `turbine.aggregator.clusterConfig` property. In that case, you
    may switch between clusters by providing the service name `cluster` with the `http://localhost:9000/turbine.stream?cluster=ORDER-SERVICE` parameter.
    The cluster name must be provided in uppercase because values returned by the
    Eureka server are in uppercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, Turbine is looking for the `/hystrix.stream` endpoint on a registered
    instance under its `homePageUrl` address in Eureka. Then it appends `/hystrix.stream`
    to that URL. Our sample application `order-service` is launched under port `8090`,
    so we should also override the default management port to `8090`. The current
    configuration of `order-service` is shown in the following code fragment. Alternatively,
    you may also change that port with the `eureka.instance.metadata-map.management.port`
    property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Enabling Turbine with streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The classic Turbine model of pulling metrics from all the distributed Hystrix
    commands is not always a good choice. An operation such as collecting metrics
    from HTTP endpoints may also be realized asynchronously with a message broker.
    To enable Turbine with streaming, we should include the following dependencies
    with the project and then annotate the main application with `@EnableTurbineStream`.
    The following sample uses RabbitMQ as a default message broker, but you may use
    Apache Kafka by including `spring-cloud-starter-stream-kafka`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The dependencies visible in the preceding code should be included on the server
    side. For client applications, these are `order-service` and `customer-service`,
    and we need to add the `spring-cloud-netflix-hystrix-stream` library. If you have
    run your message broker locally, it should have worked successfully on auto-configured
    settings. You may also run RabbitMQ using a Docker container, as we did in the
    example of the Spring Cloud Config with AMQP bus described in [Chapter 5](37142825-02d0-48a0-99df-1a1a88a1bbd4.xhtml),
    *Distributed Configuration with Spring Cloud Config*. Then you should override
    the following properties in `application.yml` for both the client-side and server-side
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: If you log in to the RabbitMQ management console, available under `http://192.168.99.100:15672`, you
    will see that the new exchange with the name `springCloudHystrixStream` has been
    created after our sample application's startup. Now, the only thing left to do
    is to run the same JUnit test as we did for the sample that illustrated the classic
    Turbine approach, described in the previous section. All metrics are sent through
    the message broker and may be observed under the `http://localhost:9000` endpoint.
    If you would like to try it by yourself, switch to the `hystrix_with_turbine_stream` branch (see [https://github.com/piomin/sample-spring-cloud-comm/tree/hystrix_with_turbine_stream](https://github.com/piomin/sample-spring-cloud-comm/tree/hystrix_with_turbine_stream) for
    more information).
  prefs: []
  type: TYPE_NORMAL
- en: Failures and the circuit breaker pattern with Feign
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Feign client is, by default, integrated with Ribbon and Hystrix. This means
    that, if you wish, you can apply different approaches to deal with latency and
    timeouts in your system when using that library. The first of these approaches
    is a connection retry mechanism provided by the Ribbon client. The second is a
    circuit breaker pattern and a fallback implementation available under the Hystrix
    project, which has already been discussed in the previous sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Retrying the connection with Ribbon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hystrix is enabled by default for the application when using a Feign library.
    This means that you should disable it in the configuration settings if you do
    not want to use it. For the purpose of testing a retry mechanism with Ribbon,
    I suggest that you disable Hystrix. In order to enable connection retrying for
    Feign, you only have to set two configuration properties—`MaxAutoRetries` and `MaxAutoRetriesNextServer`.
    The important settings, in this case, are also `ReadTimeout` and `ConnectTimeout`.
    All of them may be overridden in the `application.yml` file. Here''s the list
    of the most important Ribbon settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MaxAutoRetries`: This is the maximum number of retries on the same server
    or service instances. The first try is excluded from this count.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MaxAutoRetriesNextServer`: This is the maximum number of next servers or service
    instances to retry, excluding the first server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OkToRetryOnAllOperations`: This states that all operations can be retried
    for this client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ConnectTimeout`: This is the maximum time waiting to establish a connection
    to a server or service instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReadTimeout`: This is the maximum time waiting for a response from the server
    after establishing a connection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s assume that we have two instances of a target service. The connection
    to the first has been established, but it responds too slowly and a timeout occurs.
    The client performs one retry to that instance in accordance with the `MaxAutoRetries=1` property.
    If it has still not been successful, it tries to connect with a second available
    instance of that service. This action is repeated twice in the case of a failure,
    according to what has been set in the `MaxAutoRetriesNextServer=2` property. If
    the described mechanism is ultimately *not successful*, the timeout is returned
    to the external client. In that case, it may happen even after more than four
    seconds. Take a look at the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This solution is a standard retry mechanism implemented for a microservices-based
    environment. We may also look at some other scenarios related to the different
    configuration settings of Ribbon's timeouts and retries. There is no reason why
    we shouldn't use that mechanism together with Hystrix's circuit breaker. However,
    we have to remember that `ribbon.ReadTimeout` should be lower than the value of
    Hystrix's `execution.isolation.thread.timeoutInMilliseconds` property.
  prefs: []
  type: TYPE_NORMAL
- en: I suggest that you test the configuration settings that we just described as
    an exercise. You may use a previously introduced Hoverfly JUnit rule for simulating
    the delays and stubs of a service's instances.
  prefs: []
  type: TYPE_NORMAL
- en: Hystrix's support for Feign
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To begin with, I would like to reiterate that Hystrix is enabled by default
    for the application when using a Feign library, but only for the older versions
    of Spring Cloud. According to the documentation for the newest version of Spring
    Cloud, we should set the `feign.hystrix.enabled` property to `true`, which forces
    Feign to wrap all methods with a circuit breaker.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to the Spring Cloud Dalston release, if Hystrix was on the classpath,
    Feign would have wrapped all methods in a circuit breaker by default. This default
    behavior was changed in Spring Cloud Dalston in favor of an opt-in approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using Hystrix together with a Feign client, the simplest way to provide
    configuration properties previously set with `@HystrixProperty` inside `@HystrixCommand`
    is through the `application.yml` file. Here''s the equivalent configuration of
    the samples presented before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Feign supports the notation of a fallback. To enable fallbacks for a given
    `@FeignClient`, we should set the `fallback` attribute with the class name that
    provides a fallback implementation. The implementation class should be defined
    as a Spring Bean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Fallback implementation is based on a cache, and implements the interface annotated
    with `@FeignClient`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Optionally, we may implement a `FallbackFactory` class. That approach has one
    big advantage, it gives you access to the cause that made the fallback trigger. To
    declare a `FallbackFactory` class for Feign, just use the `fallbackFactory` attribute
    inside `@FeignClient`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The custom `FallbackFactory` class needs to implement a `FallbackFactory` interface, which
    declares the one `T create(Throwable cause)` method that has to be overridden:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may not be aware of the configuration settings or tools described in this
    chapter if you have already been using auto-configured clients for inter-service
    communication. However, I think that it is worth having some knowledge about a
    few of the advanced mechanisms, even if they can run in the background and/or
    out of the box. In this chapter, I have tried to give you a closer view on topics,
    such as load balancers, retries, fallbacks, or circuit breakers by demonstrating
    how they work using simple examples. After reading this chapter, you should be
    able to customize Ribbon, Hystrix, or Feign clients to suit your needs related
    to communication between microservices, both on a small and large scale. You should
    also understand the when and why of using them in your system. With this chapter,
    we are closing the discussion about the core elements inside microservices-based
    architecture. Now, we have got one more important component to look at that is
    outside the system by quite a bit, the gateway. This hides the system complexity
    from an external client.
  prefs: []
  type: TYPE_NORMAL
