- en: Understanding Distributed Tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to use distributed tracing to better understand
    how our microservices cooperate; for example, fulfilling a request sent to the
    external API. Being able to utilize distributed tracing is essential for being
    able to manage a system landscape of cooperating microservices. As already described
    in [Chapter 8](9878a36a-5760-41a4-a132-1a2387b61037.xhtml), *Introduction to Spring
    Cloud*, in reference to the *Spring Cloud Sleuth and Zipkin for distributed tracing* section,
    Spring Cloud Sleuth will be used to collect trace information, and Zipkin will
    be used for the storage and visualization of said trace information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing distributed tracing with Spring Cloud Sleuth and Zipkin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to add distributed tracing to the source code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to perform distributed tracing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will learn how to visualize trace information using Zipkin in relation to
    the following:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Successful and unsuccessful API requests
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronous and asynchronous processing of API requests
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use both RabbitMQ and Kafka to send trace events from our microservices
    to the Zipkin server
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All commands described in this book are run on a MacBook Pro using macOS Mojave
    but should be straightforward to modify so that they can be run on another platform
    such as Linux or Windows.
  prefs: []
  type: TYPE_NORMAL
- en: No new tools need to be installed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The source code for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter14](https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter14).
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to run the commands as described in the book, download the source
    code to a folder and set up an environment variable, `$BOOK_HOME`, that points
    to that folder. Some sample commands are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The Java source code is written for Java 8 and tested on Java 12\. This chapter
    uses Spring Cloud 2.1.0, SR1 (also known as the **Greenwich** release), Spring
    Boot 2.1.4, and Spring 5.1.6, that is, the latest available version of the Spring
    components at the time of writing this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The base Docker image, `openjdk:12.0.2`, is used in all Dockerfiles.
  prefs: []
  type: TYPE_NORMAL
- en: All source code examples in this chapter come from the source code in `$BOOK_HOME/Chapter14` but
    are, in several cases, edited to remove non-relevant parts of the source code,
    such as comments and import and log statements.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to see the changes applied to the source code in this chapter, that
    is, see what it took to add distributed tracing using Spring Cloud Sleuth and
    Zipkin, you can compare it with the source code for [Chapter 13](23795d34-4068-4961-842d-989cde26b642.xhtml),
    *Improving Resilience Using Resilience4j*. You can use your favorite `diff` tool
    and compare the two folders – `$BOOK_HOME/Chapter13` and `$BOOK_HOME/Chapter14`.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing distributed tracing with Spring Cloud Sleuth and Zipkin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To recapitulate from [Chapter 8](9878a36a-5760-41a4-a132-1a2387b61037.xhtml), *Introduction
    to Spring Cloud*, in reference to the *Spring Cloud Sleuth and Zipkin for distributed
    tracing* section, the tracing information from a complete workflow is called a
    **trace** or a **trace** **tree**, and sub-parts of the tree, for example, the
    basic units of work, are called a **span**. Spans can consist of sub spans forming
    the trace tree. The Zipkin UI can visualize a trace tree and its spans as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c486eb1c-6ecf-4f66-9783-fdecfe33f7dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Spring Cloud Sleuth can send trace information to Zipkin either synchronously
    over HTTP, or asynchronously using a message broker such as RabbitMQ or Kafka.
    To avoid creating runtime dependencies on the Zipkin server from the microservices,
    it is preferable to send trace information to Zipkin asynchronously using either
    RabbitMQ or Kafka. This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0006d41f-9f89-4f39-a1ca-8d3971451602.png)'
  prefs: []
  type: TYPE_IMG
- en: Zipkin comes with native support for storing trace information either in memory,
    or in Apache Cassandra, Elasticsearch, or MySQL. Added to this, a number of extensions
    are available. For details, refer to [https://zipkin.apache.org/pages/extensions_choices.html](https://zipkin.apache.org/pages/extensions_choices.html)[.](https://zipkin.apache.org/pages/extensions_choices.html) In
    this chapter, we will store the trace information in memory.
  prefs: []
  type: TYPE_NORMAL
- en: Adding distributed tracing to the source code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to update the source code to enable distributed
    tracing using Spring Cloud Sleuth and Zipkin. This can be done through the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Add dependencies to the build files to bring in Spring Cloud Sleuth and the
    capability of sending trace information to Zipkin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add dependencies to RabbitMQ and Kafka for the projects that haven't used them
    before, that is, the Spring Cloud projects `authorization-server`, `eureka-server`, and
    `gateway`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the microservices to send trace information to Zipkin using either
    RabbitMQ or Kafka.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a Zipkin server to the Docker compose files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the `kafka` Spring profile in `docker-compose-kafka.yml` to the Spring Cloud
    projects `authorization-server`, `eureka-server`, and `gateway`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding the Zipkin server will be effected using a Docker image from Docker Hub
    that has been published by the Zipkin project. Refer to [https://hub.docker.com/r/openzipkin/zipkin](https://hub.docker.com/r/openzipkin/zipkin)
    for details.
  prefs: []
  type: TYPE_NORMAL
- en: Zipkin is itself a Spring Boot application and is, at the time of writing, undergoing
    incubation at the **Apache Software Foundation** (**ASF**). Refer to [https://zipkin.apache.org/](https://zipkin.apache.org/)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Adding dependencies to build files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be able to utilize Spring Cloud Sleuth and the ability to send trace information
    to Zipkin, we need to add a couple of dependencies to the Gradle project build
    files, `build.gradle`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is accomplished by adding the following two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For the Gradle projects that haven''t used RabbitMQ and Kafka before, that
    is, the Spring Cloud projects `authorization-server`, `eureka-server`, and `gateway`,
    the following dependencies have to be added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Adding configuration for Spring Cloud Sleuth and Zipkin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Configuration for using Spring Cloud Sleuth and Zipkin is added to the common
    configuration file, `config-repo/application.yml`. In the default profile, it
    is specified that trace information shall be sent to Zipkin using RabbitMQ:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, Spring Cloud Sleuth only sends 10% of the traces to Zipkin. To
    ensure that all traces are sent to Zipkin, the following property is added in
    the default profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When sending traces to Zipkin using Kafka, the Spring profile `kafka` will
    be used. In earlier chapters, the `kafka` Spring profile was defined in the configuration
    files specific to the composite and core microservices. In this chapter, where
    the Spring Cloud services will also use Kafka to send trace information to Zipkin,
    the `kafka` Spring profile is moved to the common configuration file, `config-repo/application.yml`.
    The following two properties have also been added to the `kafka` Spring profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '`spring.zipkin.sender.type: kafka` tells Spring Cloud Sleuth to send trace
    information to Zipkin using Kafka.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spring.kafka.bootstrap-servers: kafka:9092` specifies where to find the Kafka
    server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All in all, the `kafka` Spring profile appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Adding Zipkin to the Docker Compose files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we mentioned previously, the Zipkin server is added to the Docker Compose
    files using an already existing Docker image, `openzipkin/zipkin`, published by
    the Zipkin project. In `docker-compose.yml` and `docker-compose-partitions.yml`,
    where RabbitMQ is used, the definition of the Zipkin server appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding source code:'
  prefs: []
  type: TYPE_NORMAL
- en: The version of the Docker image, `openzipkin/zipkin`, is specified to be version
    `2.12.19`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `RABBIT_ADDRESSES=rabbitmq` environment variable is used to specify that
    Zipkin shall receive trace information using RabbitMQ and that Zipkin shall connect
    to RabbitMQ using the hostname `rabbitmq`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `STORAGE_TYPE=mem` environment variable is used to specify that Zipkin shall keep
    all trace information in memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory limit for Zipkin is increased to 512 MB, compared to 350 MB for all
    other containers. The reason for this is that since Zipkin is configured to keep
    all trace information in memory, it will consume more memory than the other containers
    after a while.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zipkin exposes the HTTP port `9411` for web browsers to access its web user
    interface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker will wait to start up the Zipkin server until the RabbitMQ service reports
    being healthy to Docker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While this is OK to store the trace information in Zipkin in memory for development
    and test activities, Zipkin should be configured to store trace information in
    a database such as Apache Cassandra, Elasticsearch, or MySQL in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `docker-compose-kafka.yml`, where Kafka is used, the definition of the Zipkin
    server appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain for the preceding source code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The configuration for using Zipkin together with Kafka is similar to the configuration
    when using Zipkin with RabbitMQ previously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main difference it the use of the `KAFKA_BOOTSTRAP_SERVERS=kafka:9092` environment
    variable, which is used to specify that Zipkin shall use Kafka to receive trace
    information and that Zipkin shall connect to Kafka using the hostname `kafka` and
    the port `9092`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In `docker-compose-kafka.yml`, the `kafka` Spring profile is added to the Spring
    Cloud services `eureka`, `gateway`, and `auth-server`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: That's what it takes to add distributed tracing using Spring Cloud Sleuth and
    Zipkin, so let's try it out in the next section!
  prefs: []
  type: TYPE_NORMAL
- en: Trying out distributed tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the necessary changes to the source code in place, we can try out distributed
    tracing! We will do this by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Build, start, and verify the system landscape with RabbitMQ as the queue manager.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send a successful API request and see what trace information we can find in
    Zipkin related to this API request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send an unsuccessful API request and see what the trace information in Zipkin
    looks like.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send a successful API request that triggers asynchronous processing and see
    how its trace information is represented in Zipkin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Investigate how we can monitor trace information that's passed to Zipkin in
    RabbitMQ.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Switch the queue manager to Kafka and repeat the preceding steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will discuss these steps in detail in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Starting up the system landscape with RabbitMQ as the queue manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start up the system landscape. Build the Docker images with the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the system landscape in Docker and run the usual tests with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can call the API, we need an access token. Run the following commands
    to acquire an access token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Sending a successful API request
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we are ready to send a normal request to the API. Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Expect the command to returns the HTTP status code for success, 200.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now launch the Zipkin UI to look into what trace information has been
    sent to Zipkin:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the following URL in your web browser: `http://localhost:9411/zipkin/.`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To find the trace information for our request, implement the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Service Name: gateway.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set Sort order: to Newest First.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Find Traces button.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The response from finding traces should look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee509be8-dcc5-41bb-b758-361379f8fe94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The trace information from our preceding API request is the first one in the
    list. Click on it to see details pertaining to the trace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fa726b9-d1b5-424a-ade5-c5324f925c2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the detailed trace information view, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The request was received by the gateway service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It delegated the processing of the request to the product-composite service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The product-composite service, in turn, sent three parallel requests to the
    core services: product, recommendation, and review.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the product-composite service received the response from all three core
    services, it created a composite response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The composite response was sent back to the caller through the gateway service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When using Safari, I have noticed that the trace tree isn't always rendered
    correctly. Switching to either Chrome or Firefox resolved the issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we click on the first span, gateway, we can see even more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6f693ce-7cea-43ea-ba6c-8faa8db74269.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see the actual request we sent: product-composite/2\. This is very
    valuable when analyzing traces that, for example, take a long time to complete!
  prefs: []
  type: TYPE_NORMAL
- en: Sending an unsuccessful API request
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see what the trace information looks like if we make an unsuccessful
    API request; for example, searching for a product that does not exist:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Send an API request for product ID `12345` and verify that it returns the HTTP
    status code for Not Found, 404:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the Zipkin UI, go back to the search page (use the back button in the web
    browser) and click on the Find Traces button. You should see the failed request
    at the top of the returned list, in red:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3b7bf196-16fe-4277-8e51-3f393696278f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the top trace marked in red:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4ca53dfa-35ad-4c3c-b50f-8e3a3c357f61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the detailed trace view, we can see by the color-coding that the request
    went wrong when product-composite called the product service. Click on the product
    span to see details of what went wrong:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b33855c9-3b3e-4aaf-81ed-8a080c05dec7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see what request caused the error, product/12345, as well as the
    error code and the reason returned: 404 Not Found. This is very useful when analyzing
    the root cause of a failure!'
  prefs: []
  type: TYPE_NORMAL
- en: Sending an API request that triggers asynchronous processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The third type of request that is interesting to see how it is represented in
    the Zipkin UI is a request where parts of its processing are done asynchronously.
    Let's try a delete request, where the delete process in the core services is done
    asynchronously. The `product-composite` service sends a delete event to each of
    the three core services over the message broker and each core service picks up
    the delete event and processes it asynchronously. Thanks to Spring Cloud Sleuth,
    trace information is added to the events that are sent to the message broker,
    resulting in a coherent view of the total processing of the delete request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to delete the product with a product ID of `12345` and verify
    that it returns the HTTP status code for success, 200:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Remember that the delete operation is idempotent, that is, it will succeed even
    if the product doesn't exist!
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Zipkin UI, go back to the search page (use the back button in the web
    browser) and click on the Find Traces button. You should see the trace from the
    delete request at the top of the returned list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8304d85-6234-4bf4-9470-e18b2f92a68a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the first trace to see its trace information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b06714c3-aec0-4166-a066-2001d3c14c3d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see the trace information for processing the delete request:'
  prefs: []
  type: TYPE_NORMAL
- en: The request was received by the gateway service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It delegated the processing of the request to the product-composite service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The product-composite service, in turn, published three events on the message
    broker (RabbitMQ, in this case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The product-composite service is now done and returns a success HTTP status
    code, 200, through the gateway service back to the caller.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The core services, product, recommendation, and review, receive the delete events
    and start to process them asynchronously, that is, independent of one another.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To see more detailed information, click on the product span:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fd269c2-3bc7-4c83-91cd-6a16c6010757.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that the product service was triggered by an event coming in
    to its input channel, which was sent from the message broker.
  prefs: []
  type: TYPE_NORMAL
- en: The Zipkin UI contains much more functionality for finding traces of interest!
  prefs: []
  type: TYPE_NORMAL
- en: To get more accustomed to the Zipkin UI, try out the `Annotation Query` parameter;
    for example, search for a specific request using `http.path=/product-composite/214`
    or `error=401` to find requests that failed due to authorization failures. Watch
    out for the `Limit` parameter, which is set to `10` by default; this can hide
    results of interest if not raised. Also, ensure that the `Lookback` parameter
    doesn't remove traces of interest!
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring trace information passed to Zipkin in RabbitMQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To monitor trace information that''s sent to Zipkin over RabbitMQ, we can use
    the RabbitMQ management web UI. Open the following URL in your web browser: `http://localhost:15672/#/queues/%2F/zipkin`.
    If required, log in using the username `guest` and the password `guest`. Expect
    a web page that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ebbf762-280a-457c-b52e-7344c666c249.png)'
  prefs: []
  type: TYPE_IMG
- en: In the graph named `Message Rates`, we can see that trace messages are sent
    to Zipkin, currently at an average rate of 1.2 messages per second.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wrap up the tests of distributed tracing using RabbitMQ by bringing down the
    system landscape. Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Using Kafka as a message broker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's also verify that we can send trace information to Zipkin using Kafka instead
    of RabbitMQ!
  prefs: []
  type: TYPE_NORMAL
- en: 'Start up the system landscape using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Repeat the commands we performed in the previous sections, where we used RabbitMQ,
    and verify that you can see the same trace information in the Zipkin UI when using
    Kafka!
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka doesn''t come with a management web UI like RabbitMQ. Therefore, we need
    to run a few Kafka commands to be able to verify that the trace events actually
    were passed to the Zipkin server using Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: For a recap on how to run Kafka commands when running Kafka as a Docker container,
    refer to the *Using Kafka with two partitions per topic* section in [Chapter 7](436fb8c1-0c4d-410c-a3ec-da251aba4ca1.xhtml),
    *Developing Reactive Microservices.*
  prefs: []
  type: TYPE_NORMAL
- en: 'First, list the available topics in Kafka:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Expect to find a topic named `zipkin`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8ce689f9-3d7a-49de-b487-817b43554f35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, ask for trace events that were sent to the `zipkin` topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Expect a lot of events similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8fcdde07-be13-497a-8b95-1b56e4fe3a95.png)'
  prefs: []
  type: TYPE_IMG
- en: The details of a trace event are not important. The Zipkin server sorts that
    out for us and makes the information presentable in the Zipkin UI. The important
    point here is that we can see that the trace events actually were sent to the
    Zipkin server using Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, bring down the system landscape and unset the `COMPOSE_FILE` environment
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: That concludes this chapter on distributed tracing!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to use distributed tracing to understand
    how our microservices cooperate. We have learned how to use Spring Cloud Sleuth
    to collect trace information, and how to use Zipkin to store and visualize the
    trace information.
  prefs: []
  type: TYPE_NORMAL
- en: To promote the decoupling of runtime components, we have learned how to configure
    microservices to send trace information to the Zipkin server asynchronously while
    using RabbitMQ and Kafka as message brokers. We have seen how adding Spring Cloud
    Sleuth to microservices is effected by adding a couple of dependencies to the
    build files and setting up a few configuration parameters. We have also seen how
    the Zipkin UI makes it very easy to identify what part of a complex workflow caused
    either an unexpectedly long response time or an error. Both synchronous and asynchronous
    workflows can be visualized by Zipkin UI.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about container orchestrators, specifically
    Kubernetes. We will learn how to use Kubernetes to deploy and manage microservices,
    while also improving important runtime characteristics such as scalability, high
    availability, and resilience.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What configuration parameter is used to control how trace information is sent
    to Zipkin?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of the `spring.sleuth.sampler.probability` configuration
    parameter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you identify the longest-running request after executing the `test-em-all.bash` test
    script?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we find requests that have been interrupted by the timeout introduced
    in [Chapter 13](23795d34-4068-4961-842d-989cde26b642.xhtml), *Improving Resilience
    Using Resilience4j*?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does the trace look like for an API request when the circuit breaker introduced
    in [Chapter 13](23795d34-4068-4961-842d-989cde26b642.xhtml), *Improving Resilience
    Using Resilience4j*, is open?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we locate APIs that failed on the caller not being authorized to perform
    the request?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
