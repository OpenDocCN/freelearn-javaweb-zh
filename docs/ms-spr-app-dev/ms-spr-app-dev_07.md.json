["```java\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans \n\n   xsi:schemaLocation=\"\n    http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n    http://www.springframework.org/schema/hadoop http://www.springframework.org/schema/hadoop/spring-hadoop.xsd\">\n\n   <bean id ... >\n\n   4<hdp:configuration ...>\n</beans>\n```", "```java\nHdfsResourceLoader(Configuration config)\nHdfsResourceLoader(Configuration config) \nHdfsResourceLoader(Configuration config, URI uri) \nHdfsResourceLoader(Configuration config, URI uri, String user) HdfsResourceLoader(FileSystem fs)\n```", "```java\n<hdp:configuration>\n  fs.default.name=webhdfs://localhost\n  ...\n</hdp:configuration>\n```", "```java\n<!-- manually creates the default SHDP file-system named 'hadoopFs' -->\n<hdp:file-system uri=\"webhdfs://localhost\"/>\n\n<!-- creates a different FileSystem instance --> \n<hdp:file-system id=\"old-cluster\" uri=\"hftp://old-cluster/\"/>\n```", "```java\n<beans  ...> \n<hdp:configuration .../>\n\n<hdp:script id=\"inlined-js\" language=\"javascript\" run-at-startup=\"true\">\n  importPackage(java.util);\n  name = UUID.randomUUID().toString()\n  scriptName = \"src/test/resources/test.properties\"\n  // fs - FileSystem instance based on 'hadoopConfiguration' bean\n  // call FileSystem#copyFromLocal(Path, Path)  \n  fs.copyFromLocalFile(scriptName, name)\n  // return the file length \n  fs.getLength(name)\n</hdp:script>\n\n</beans>\n```", "```java\nimport org.apache.hadoop.conf.Configuration;\n\nimport org.apache.hadoop.hbase.HBaseConfiguration;\n\nimport org.apache.hadoop.hbase.HColumnDescriptor;\n\nimport org.apache.hadoop.hbase.HTableDescriptor;\n\nimport org.apache.hadoop.hbase.client.HBaseAdmin;\n\npublic class HbaseTableCreation\n{\n  public static void main(String[] args) throws IOException {\n    HBaseConfiguration hc = new HBaseConfiguration(new Configuration());\n\n    HTableDescriptor ht = new HTableDescriptor(\"EmployeeTable\"); \n\n    ht.addFamily( new HColumnDescriptor(\"Id\"));\n\n    ht.addFamily( new HColumnDescriptor(\"Name\"));\n\n    System.out.println( \"connecting\" );\n\n    HBaseAdmin hba = new HBaseAdmin( hc );\n\n    System.out.println( \"Creating Table EmployeeTable\" );\n\n    hba.createTable( ht );\n\n    System.out.println(\"Done....EmployeeTable..\");\n  }\n}\n```", "```java\nHbaseTemplate() \nHbaseTemplate(Configuration configuration)\n```", "```java\n// default HBase configuration\n<hdp:hbase-configuration/>\n\n// wire hbase configuration (using default name 'hbaseConfiguration') into the template \n<bean id=\"htemplate\" class=\"org.springframework.data.hadoop.hbase.HbaseTemplate\" p:configuration-ref=\"hbaseConfiguration\"/>\n```", "```java\n// writing to 'EmployeeTable'\ntemplate.execute(\"EmployeeTable\", new TableCallback<Object>() {\n  @Override\n  public Object doInTable(HTable table) throws Throwable {\n    Put p = new Put(Bytes.toBytes(\"Name\"));\n    p.add(Bytes.toBytes(\"Name\"), Bytes.toBytes(\"SomeQualifier\"), Bytes.toBytes(\"Anjana\"));\n    table.put(p);\n    return null;\n  }\n});\n\n// read each row from 'EmployeeTable'\nList<String> rows = template.find(\"EmployeeTable\", \"Name\", new RowMapper<String>() {\n  @Override\n  public String mapRow(Result result, int rowNum) throws Exception {\n    return result.toString();\n  }\n}));\n```", "```java\n    <!-- default bean id is 'hbaseConfiguration' that uses the existing 'hadoopCconfiguration' object ->\n    <hdp:hbase-configuration configuration-ref=\"hadoopCconfiguration\" />\n    ```", "```java\n    <!-- delete associated connections but do not stop the proxies -->\n    <hdp:hbase-configuration stop-proxy=\"false\" delete-connection=\"true\">\n      toooo=baaaa\n      property=value\n    </hdp:hbase-configuration>\n    ```", "```java\n    <!-- specify ZooKeeper host/port -->\n    <hdp:hbase-configuration zk-quorum=\"${hbase.host}\" zk-port=\"${hbase.port}\">\n    ```", "```java\n    <hdp:hbase-configuration properties-ref=\"some-props-bean\" properties-location=\"classpath:/conf/testing/hbase.properties\"/>\n    ```", "```java\n<hdp:configuration />\n```", "```java\n<hdp:configuration resources=\"classpath:/custom-site.xml, classpath:/hq-site.xml\">\n```", "```java\n<hdp:configuration>\n        fs.default.name=hdfs://localhost:9000\n        hadoop.tmp.dir=/tmp/hadoop\n        electric=sea\n     </hdp:configuration>\n```", "```java\n<hdp:configuration>\n        fs.default.name=${hd.fs}\n        hadoop.tmp.dir=file://${java.io.tmpdir}\n        hangar=${number:18}\n     </hdp:configuration>\n          <context:property-placeholder location=\"classpath:hadoop.properties\" />\n```", "```java\n< !-- Spring Data Apache Hadoop -- >\n< dependency >\n    < groupId > org.springframework.data </ groupId >\n    < artifactId  > spring-data-hadoop </ artifactId >\n    < version > 1.0.0.RELEASE </ version >\n< /dependency >\n< !-- Apache Hadoop Core \u2013- >\n< dependency >\n    < groupId > org.apache.hadoop </ groupId >\n    < artifactId > hadoop-core </ artifactId >\n    < version > 1.0.3 </version >\n</dependency>\n```", "```java\npublic class CustomWordMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n  private Text myword = new Text();\n\n  @Override\n  protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n    String line = value.toString();\n    StringTokenizer lineTokenz = new StringTokenizer(line);\n    while (lineTokenz.hasMoreTokens()) {\n      String cleaned_data = removeNonLettersNonNumbers(lineTokenz.nextToken());\n        myword.set(cleaned_data);\n        context.write(myword, new IntWritable(1));\n    }\n  }\n\n  /**\n  * Replace all Unicode characters that are neither numbers nor letters with an empty string.\n  * @param original, It is the original string\n  * @return a string object that contains only letters and numbers\n  */\n  private String removeNonLettersNonNumbers (String original) {\n    return original.replaceAll(\"[^\\\\p{L}\\\\p{N}]\", \"\");\n  }\n}\n```", "```java\n    import org.apache.hadoop.io.IntWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Reducer;\n\n    public class CustomWordReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n        protected static final String MY_TARGET_TEXT = \"SPRING\";\n\n    @Override\n     protected void reduce(Text keyTxt, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n            if (containsTargetWord(keyTxt)) {\n                int wCount = 0;\n                for (IntWritable value: values) {\n                   wCount += value.get();\n                }\n                context.write(key, new IntWritable(wCount));\n            }\n        }\n        private boolean containsTargetWord(Text keyTxt) {\n            return keyTxt.toString().equals(MY_TARGET_TEXT);\n        }\n    }\n    ```", "```java\n    fs.default.name=hdfs://localhost:9000\n    mapred.job.tracker=localhost:9001\n    input.path=/path/to/input/file/\n    output.path=/path/to/output/file\n\n    ```", "```java\n    <context:property-placeholder location=\"classpath:application.properties\" />\n    ```", "```java\n    <hdp:configuration>\n      fs.default.name=${fs.default.name}\n      mapred.job.tracker=${mapred.job.tracker}\n    </hdp:configuration>\n    ```", "```java\n    <hdp:job id=\"wordCountJobId\"\n    input-path=\"${input.path}\"\n    output-path=\"${output.path}\"\n    jar-by-class=\"net.qs.spring.data.apachehadoop.Main\"\n    mapper=\"com.packt.spring.data.apachehadoop.CustomWordMapper\"\n    reducer=\"com.packt.spring.data.apachehadoop.CustomWordReducer\"/>\n    ```", "```java\n    <hdp:job-runner id=\"wordCountJobRunner\" job-ref=\"wordCountJobId\" run-at-startup=\"true\"/>\n    ```", "```java\nimport org.springframework.context.ApplicationContext;\nimportorg.springframework.context.support.ClassPathXmlApplicationContext;\n\npublic class Main {\n  public static void main(String[] arguments) {\n    ApplicationContext ctx = new ClassPathXmlApplicationContext(\"application-context.xml\");\n  }\n}\n```", "```java\nSPRING IS A SEASON. SPRING IS A FRAMEWORK IN JAVA. ITS SPRING IN INDIA. SPRING IS GREEEN. SPRING SPRING EVERY WHERE\n```", "```java\nhadoop dfs -put myinput.txt /input/myinput.txt\nhadoop dfs -ls /input\n\n```", "```java\n    <configuration>\n      <property>\n      <name>fs.default.name</name>\n      <value>hdfs://localhost:9000</value>\n      </property>\n    </configuration>\n    ```", "```java\n    <configuration>\n      <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n      </property>\n    </configuration>\n    ```", "```java\n    <configuration>\n      <property>\n        <name>mapred.job.tracker</name>\n        <value>localhost:9001</value>\n      </property>\n    </configuration>\n    ```", "```java\n<!-- Spring Data Apache Hadoop -->\n<dependency>\n  <groupId>org.springframework.data</groupId>\n  <artifactId>spring-data-hadoop</artifactId>\n  <version>1.0.0.RC2</version>\n</dependency>\n<!-- Apache Hadoop Core -->\n<dependency>\n  <groupId>org.apache.hadoop</groupId>\n  <artifactId>hadoop-core</artifactId>\n  <version>1.0.3</version>\n</dependency>\n<!-- Apache Hadoop Streaming -->\n<dependency>\n  <groupId>org.apache.hadoop</groupId>\n  <artifactId>hadoop-streaming</artifactId>\n  <version>1.0.3</version>\n</dependency>\n```", "```java\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\nimport sys\nimport unicodedata\n\n# Removes punctuation characters from the string\ndef strip_punctuation(word):\n return ''.join(x for x in word if unicodedata.category(x) != 'Po')\n\n#Process input one line at the time\nfor line in sys.stdin:\n #Converts the line to Unicode\n line = unicode(line, \"utf-8\")\n #Splits the line to individual words\n words = line.split()\n #Processes each word one by one\n for word in words:\n #Removes punctuation characters\n word = strip_punctuation(word)\n #Prints the output\n print (\"%s\\t%s\" % (word, 1)).encode(\"utf-8\")\n\n```", "```java\n    #!/usr/bin/python\n    # -*- coding: utf-8 -*-s\n    import sys\n    wordCount = 0\n    #Process input one line at the time\n    for line in sys.stdin:\n     #Converts the line to Unicode\n     line = unicode(line, \"utf-8\")\n     #Gets key and value from the current line\n     (key, value) = line.split(\"\\t\")\n     if key == \"Amily\":\n     #Increase word count by one\n     wordCount = int(wordCount + 1);\n    #Prints the output\n    print (\"Watson\\t%s\" % wordCount).encode(\"utf-8\")\n\n    ```", "```java\n    #Configures the default file system of Apache Hadoop\n    fs.default.name=hdfs://localhost:9000\n\n    #The path to the directory that contains our input files\n    input.path=/input/\n\n    #The path to the directory in which the output is written\n    output.path=/output/\n\n    #Configure the path of the mapper script\n    mapper.script.path=pythonmapper.py\n\n    #Configure the path of the reducer script\n    reducer.script.path=pythonreducer.py\n\n    ```", "```java\n    <context:property-placeholder location=\"classpath:application.properties\" />\n    <hdp:configuration>\n      fs.default.name=${fs.default.name}\n    </hdp:configuration>\n    ```", "```java\n    <hdp:configuration>\n      fs.default.name=${fs.default.name}\n    </hdp:configuration>\n    <hdp:streaming id=\"streamingJob\"\n      input-path=\"${input.path}\"\n      output-path=\"${output.path}\"\n      mapper=\"${mapper.script.path}\"\n      reducer=\"${reducer.script.path}\"/>\n    <hdp:job-runner id=\"streamingJobRunner\" job-ref=\"streamingJob\" run-at-startup=\"true\"/>\n    ```", "```java\n    import org.springframework.context.ApplicationContext;\n    import org.springframework.context.support.ClassPathXmlApplicationContext;\n\n    public class Main {\n      public static void main(String[] arguments) {\n        ApplicationContext ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\");\n      }\n    }\n    ```", "```java\n    hadoop dfs -put MILLSANDBOON.txt /input/ MILLSANDBOON.txt\n\n    ```", "```java\n    hadoop dfs -rmr /output\n    hadoop dfs -cat /output/part-00000\n\n    ```"]